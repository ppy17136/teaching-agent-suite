# -*- coding: utf-8 -*-
"""
æ•™å­¦æ™ºèƒ½ä½“å¹³å°ï¼ˆå•æ–‡ä»¶ç‰ˆ app.pyï¼‰- æ•´åˆPDFå…¨é‡æŠ½å–ç‰ˆ
æ•´åˆäº†ä¼˜åŒ–åçš„PDFæŠ½å–åŠŸèƒ½ï¼Œå…·å¤‡ï¼š
1) åŸ¹å…»æ–¹æ¡ˆPDFå…¨é‡æŠ½å–ï¼ˆæ–‡æœ¬+è¡¨æ ¼+ç»“æ„åŒ–è§£æï¼‰
2) è¯†åˆ«æ¸…å•å¯ç¼–è¾‘ç¡®è®¤åå†ä¿å­˜
3) è¡¨æ ¼ä»¥data_editorå½¢å¼å±•ç¤ºä¾¿äºä¿®æ­£
4) ä¿ç•™åŸæœ‰çš„ä¾èµ–è¿½æº¯å’Œç‰ˆæœ¬ç®¡ç†
"""

import os
import io
import re
import json
import time
import base64
import hashlib
import sqlite3
import zipfile
import threading
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple
import pandas as pd
import streamlit as st
import requests
import numpy as np
from PIL import Image, ImageOps

# -------- å¯é€‰è§£æä¾èµ– --------
try:
    import pdfplumber
except Exception:
    pdfplumber = None

try:
    from docx import Document
except Exception:
    Document = None

# ---------------------------
# åŸºç¡€é…ç½®
# ---------------------------
st.set_page_config(page_title="æ•™å­¦æ™ºèƒ½ä½“å¹³å°", layout="wide")

BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
DEFAULT_TEXT_MODEL = "qwen-max"
DEFAULT_VL_MODEL = "qwen-vl-plus"

DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)
DB_PATH = os.path.join(DATA_DIR, "app.db")

_DB_LOCK = threading.Lock()

# ---------------------------
# UI ç¾åŒ–ï¼ˆCSSï¼‰
# ---------------------------
def inject_css():
    st.markdown(
        """
<style>
.main .block-container {
    padding-top: 1.0rem;
    padding-bottom: 2rem;
    max-width: 100% !important;
    padding-left: 2rem;
    padding-right: 2rem;
}
h1, h2, h3 { letter-spacing: .2px; }
code { font-size: 0.9em; }

.topbar{
    padding: 18px 18px;
    border-radius: 18px;
    background: linear-gradient(90deg, #0ea5e9 0%, #6366f1 55%, #8b5cf6 100%);
    color: white;
    box-shadow: 0 8px 24px rgba(0,0,0,.12);
}
.topbar .title{ font-size: 30px; font-weight: 800; }
.topbar .sub{ opacity: .9; margin-top: 6px; font-size: 14px; }

.card{
    border: 1px solid rgba(0,0,0,.08);
    border-radius: 18px;
    padding: 16px 16px;
    background: rgba(255,255,255,.6);
    box-shadow: 0 6px 16px rgba(0,0,0,.06);
}
.badge{
    display:inline-block; padding: 2px 10px; border-radius: 999px;
    font-size: 12px; border: 1px solid rgba(0,0,0,.12); margin-right: 6px;
}
.badge.ok { background:#ecfdf5; color:#065f46; border-color:#a7f3d0; }
.badge.warn { background:#fffbeb; color:#92400e; border-color:#fde68a; }
.badge.bad { background:#fef2f2; color:#991b1b; border-color:#fecaca; }

.depbar{ display:flex; gap:8px; flex-wrap: wrap; padding: 10px 0; }
.depitem{
    padding: 8px 10px; border-radius: 14px; border: 1px solid rgba(0,0,0,.10);
    background: rgba(255,255,255,.7); font-size: 13px;
}
.depitem b{ margin-right:6px; }

.docbox{
    border: 1px solid rgba(0,0,0,.10);
    border-radius: 18px;
    padding: 14px 16px;
    background: rgba(255,255,255,.75);
    line-height: 1.55;
    white-space: normal;
}
section[data-testid="stSidebar"] .stMarkdown h2{ font-size: 18px; font-weight: 800; }
div[data-testid="stDataFrame"] { border-radius: 14px; overflow:hidden; }
</style>
""",
        unsafe_allow_html=True,
    )

inject_css()

# ---------------------------
# æ•°æ®åº“å±‚
# ---------------------------
def db() -> sqlite3.Connection:
    os.makedirs(DATA_DIR, exist_ok=True)
    conn = sqlite3.connect(DB_PATH, check_same_thread=False, timeout=30)
    conn.execute("PRAGMA foreign_keys=ON;")
    conn.execute("PRAGMA busy_timeout=5000;")
    try:
        conn.execute("PRAGMA journal_mode=WAL;")
    except Exception:
        conn.execute("PRAGMA journal_mode=DELETE;")
    return conn

def init_db():
    with _DB_LOCK:
        conn = db()
        conn.execute(
            """
CREATE TABLE IF NOT EXISTS projects(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL,
    meta_json TEXT DEFAULT '{}',
    created_at INTEGER NOT NULL
);
"""
        )
        conn.execute(
            """
CREATE TABLE IF NOT EXISTS artifacts(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    project_id INTEGER NOT NULL,
    type TEXT NOT NULL,
    title TEXT NOT NULL,
    content_md TEXT NOT NULL,
    content_json TEXT NOT NULL DEFAULT '{}',
    hash TEXT NOT NULL,
    created_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL,
    FOREIGN KEY(project_id) REFERENCES projects(id) ON DELETE CASCADE
);
"""
        )
        conn.execute(
            """
CREATE TABLE IF NOT EXISTS versions(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    artifact_id INTEGER NOT NULL,
    version_no INTEGER NOT NULL,
    content_md TEXT NOT NULL,
    content_json TEXT NOT NULL,
    hash TEXT NOT NULL,
    created_at INTEGER NOT NULL,
    note TEXT DEFAULT '',
    FOREIGN KEY(artifact_id) REFERENCES artifacts(id) ON DELETE CASCADE
);
"""
        )
        conn.execute(
            """
CREATE TABLE IF NOT EXISTS edges(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    project_id INTEGER NOT NULL,
    child_artifact_id INTEGER NOT NULL,
    parent_artifact_id INTEGER NOT NULL,
    created_at INTEGER NOT NULL,
    FOREIGN KEY(project_id) REFERENCES projects(id) ON DELETE CASCADE,
    FOREIGN KEY(child_artifact_id) REFERENCES artifacts(id) ON DELETE CASCADE,
    FOREIGN KEY(parent_artifact_id) REFERENCES artifacts(id) ON DELETE CASCADE
);
"""
        )
        conn.commit()
        conn.close()

def ensure_db_schema():
    init_db()

def now_ts() -> int:
    return int(time.time())

def sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def sha256_bytes(data: bytes) -> str:
    return hashlib.sha256(data).hexdigest()

def compute_hash(content_md: str, content_json: Dict[str, Any], parent_hashes: List[str]) -> str:
    payload = {"content_md": content_md, "content_json": content_json, "parents": parent_hashes}
    return sha256_text(json.dumps(payload, ensure_ascii=False, sort_keys=True))

# ---------------------------
# æ•°æ®åº“æ“ä½œ
# ---------------------------
def get_projects() -> List[Tuple[int, str]]:
    conn = db()
    rows = conn.execute("SELECT id, name FROM projects ORDER BY id DESC;").fetchall()
    conn.close()
    return rows

def get_project_meta(project_id: int) -> Dict[str, Any]:
    conn = db()
    row = conn.execute("SELECT meta_json FROM projects WHERE id=?", (project_id,)).fetchone()
    conn.close()
    if not row:
        return {}
    try:
        return json.loads(row[0] or "{}")
    except Exception:
        return {}

def create_project(name: str, meta: Dict[str, Any]) -> int:
    with _DB_LOCK:
        conn = db()
        ts = now_ts()
        cur = conn.execute(
            "INSERT INTO projects(name, meta_json, created_at) VALUES(?,?,?)",
            (name, json.dumps(meta, ensure_ascii=False), ts),
        )
        conn.commit()
        pid = cur.lastrowid
        conn.close()
        return pid

def list_artifacts(project_id: int) -> List[Dict[str, Any]]:
    conn = db()
    try:
        rows = conn.execute(
            "SELECT id, type, title, hash, updated_at "
            "FROM artifacts WHERE project_id=? ORDER BY updated_at DESC",
            (project_id,),
        ).fetchall()
    except sqlite3.OperationalError:
        conn.close()
        ensure_db_schema()
        conn = db()
        rows = conn.execute(
            "SELECT id, type, title, hash, updated_at "
            "FROM artifacts WHERE project_id=? ORDER BY updated_at DESC",
            (project_id,),
        ).fetchall()
    conn.close()
    return [{"id": r[0], "type": r[1], "title": r[2], "hash": r[3], "updated_at": r[4]} for r in rows]

def get_artifact(project_id: int, a_type: str) -> Optional[Dict[str, Any]]:
    conn = db()
    row = conn.execute(
        "SELECT id, title, content_md, content_json, hash, created_at, updated_at "
        "FROM artifacts WHERE project_id=? AND type=? ORDER BY updated_at DESC LIMIT 1",
        (project_id, a_type),
    ).fetchone()
    conn.close()
    if not row:
        return None
    return {
        "id": row[0],
        "type": a_type,
        "title": row[1],
        "content_md": row[2],
        "content_json": json.loads(row[3] or "{}"),
        "hash": row[4],
        "created_at": row[5],
        "updated_at": row[6],
    }

def get_versions(artifact_id: int) -> List[Dict[str, Any]]:
    conn = db()
    rows = conn.execute(
        "SELECT version_no, hash, created_at, note FROM versions WHERE artifact_id=? ORDER BY version_no DESC",
        (artifact_id,),
    ).fetchall()
    conn.close()
    return [{"version_no": r[0], "hash": r[1], "created_at": r[2], "note": r[3]} for r in rows]

def set_edges(project_id: int, child_id: int, parent_ids: List[int]):
    with _DB_LOCK:
        conn = db()
        conn.execute("DELETE FROM edges WHERE project_id=? AND child_artifact_id=?", (project_id, child_id))
        ts = now_ts()
        for pid in parent_ids:
            conn.execute(
                "INSERT INTO edges(project_id, child_artifact_id, parent_artifact_id, created_at) VALUES(?,?,?,?)",
                (project_id, child_id, pid, ts),
            )
        conn.commit()
        conn.close()

def upsert_artifact(
    project_id: int,
    a_type: str,
    title: str,
    content_md: str,
    content_json: Dict[str, Any],
    parent_ids: List[int],
    note: str = "",
) -> Dict[str, Any]:
    existing = get_artifact(project_id, a_type)
    
    parent_hashes: List[str] = []
    for pid in parent_ids:
        conn = db()
        row = conn.execute("SELECT hash FROM artifacts WHERE id=? AND project_id=?", (pid, project_id)).fetchone()
        conn.close()
        if row:
            parent_hashes.append(row[0])
    
    new_hash = compute_hash(content_md, content_json, parent_hashes)
    ts = now_ts()
    
    with _DB_LOCK:
        conn = db()
        if existing:
            cur_ver = conn.execute("SELECT MAX(version_no) FROM versions WHERE artifact_id=?", (existing["id"],)).fetchone()
            next_ver = (cur_ver[0] or 0) + 1
            conn.execute(
                "INSERT INTO versions(artifact_id, version_no, content_md, content_json, hash, created_at, note) "
                "VALUES(?,?,?,?,?,?,?)",
                (
                    existing["id"],
                    next_ver,
                    existing["content_md"],
                    json.dumps(existing["content_json"], ensure_ascii=False),
                    existing["hash"],
                    ts,
                    note or "auto-save",
                ),
            )
            conn.execute(
                "UPDATE artifacts SET title=?, content_md=?, content_json=?, hash=?, updated_at=? "
                "WHERE id=? AND project_id=?",
                (title, content_md, json.dumps(content_json, ensure_ascii=False), new_hash, ts, existing["id"], project_id),
            )
            conn.commit()
        else:
            conn.execute(
                "INSERT INTO artifacts(project_id, type, title, content_md, content_json, hash, created_at, updated_at) "
                "VALUES(?,?,?,?,?,?,?,?)",
                (project_id, a_type, title, content_md, json.dumps(content_json, ensure_ascii=False), new_hash, ts, ts),
            )
            conn.commit()
        conn.close()
    
    a = get_artifact(project_id, a_type)
    if a:
        set_edges(project_id, a["id"], parent_ids)
    return a

# ---------------------------
# æ–‡æ¡£é“¾ & ä¾èµ–è§„åˆ™
# ---------------------------
DOC_TYPES = [
    ("overview", "é¦–é¡µæ€»è§ˆ"),
    ("training_plan", "åŸ¹å…»æ–¹æ¡ˆï¼ˆåº•åº§ï¼‰"),
    ("syllabus", "è¯¾ç¨‹æ•™å­¦å¤§çº²ï¼ˆä¾èµ–åŸ¹å…»æ–¹æ¡ˆï¼‰"),
    ("calendar", "æ•™å­¦æ—¥å†ï¼ˆä¾èµ–å¤§çº²ï¼‰"),
    ("lesson_plan", "æ•™æ¡ˆï¼ˆä¾èµ–æ—¥å†ï¼‰"),
    ("assessment", "ä½œä¸š/é¢˜åº“/è¯•å·æ–¹æ¡ˆï¼ˆä¾èµ–å¤§çº²ï¼‰"),
    ("review", "å®¡æ ¸è¡¨ï¼ˆä¾èµ–è¯•å·æ–¹æ¡ˆ/å¤§çº²ï¼‰"),
    ("report", "è¯¾ç¨‹ç›®æ ‡è¾¾æˆæŠ¥å‘Šï¼ˆä¾èµ–å¤§çº²/æˆç»©ï¼‰"),
    ("manual", "æˆè¯¾æ‰‹å†Œï¼ˆä¾èµ–æ•™æ¡ˆ/è¿‡ç¨‹è¯æ®ï¼‰"),
    ("evidence", "è¯¾å ‚çŠ¶æ€ä¸è¿‡ç¨‹è¯æ®ï¼ˆå¯é€‰ï¼‰"),
    ("vge", "è¯æ®é“¾ä¸å¯éªŒè¯ç”Ÿæˆï¼ˆVGEï¼‰"),
    ("dep_graph", "ä¾èµ–å›¾å¯è§†åŒ–ï¼ˆæ ‘/Graphvizï¼‰"),
    ("docx_export", "æ¨¡æ¿åŒ–DOCXå¯¼å‡ºï¼ˆå­—æ®µæ˜ å°„å¡«å……ï¼‰"),
]

DEP_RULES = {
    "training_plan": [],
    "syllabus": ["training_plan"],
    "calendar": ["syllabus"],
    "lesson_plan": ["calendar"],
    "assessment": ["syllabus"],
    "review": ["assessment", "syllabus"],
    "report": ["syllabus"],
    "manual": ["lesson_plan"],
    "evidence": [],
    "vge": [],
    "overview": [],
    "dep_graph": [],
    "docx_export": [],
}

# ---------------------------
# PDFå…¨é‡æŠ½å–æ ¸å¿ƒåŠŸèƒ½ï¼ˆæ•´åˆç‰ˆï¼‰
# ---------------------------
def clean_text(s: str) -> str:
    if s is None:
        return ""
    s = str(s)
    s = s.replace("\u00a0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()

def normalize_multiline(text: str) -> str:
    """ä¿ç•™æ¢è¡Œï¼ŒåšåŸºç¡€æ¸…ç†"""
    if text is None:
        return ""
    text = str(text).replace("\r\n", "\n").replace("\r", "\n")
    lines = [clean_text(ln) for ln in text.split("\n")]
    out: List[str] = []
    blank = 0
    for ln in lines:
        if ln.strip() == "":
            blank += 1
            if blank <= 2:
                out.append("")
        else:
            blank = 0
            out.append(ln)
    return "\n".join(out).strip()

def make_unique_columns(cols: List[str]) -> List[str]:
    seen: Dict[str, int] = {}
    out: List[str] = []
    for c in cols:
        c0 = clean_text(c) or "col"
        if c0 not in seen:
            seen[c0] = 1
            out.append(c0)
        else:
            seen[c0] += 1
            out.append(f"{c0}_{seen[c0]}")
    return out

def postprocess_table_df(df: pd.DataFrame) -> pd.DataFrame:
    """è¡¨æ ¼åå¤„ç†ï¼šå»ç©ºç™½ã€å»NaNã€åˆå¹¶æ ¼å‘ä¸‹å¡«å……"""
    if df is None or df.empty:
        return df
    
    df = df.copy()
    df = df.replace({None: ""}).fillna("")
    for c in df.columns:
        df[c] = df[c].astype(str).map(lambda x: clean_text(x))
    
    # åˆ é™¤å®Œå…¨ç©ºè¡Œ
    mask_all_empty = df.apply(lambda r: all((clean_text(x) == "" for x in r.values.tolist())), axis=1)
    df = df.loc[~mask_all_empty].reset_index(drop=True)
    
    # å‘ä¸‹å¡«å……ï¼ˆåˆå¹¶æ ¼å¸¸è§åˆ—ï¼‰
    fill_down_keywords = ["è¯¾ç¨‹ä½“ç³»", "è¯¾ç¨‹æ¨¡å—", "è¯¾ç¨‹æ€§è´¨", "è¯¾ç¨‹ç±»åˆ«", "ç±»åˆ«", "æ¨¡å—", "ç¯èŠ‚", "å­¦æœŸ", "æ–¹å‘"]
    for c in df.columns:
        if any(k in str(c) for k in fill_down_keywords):
            last = ""
            new_col = []
            for v in df[c].tolist():
                if v != "":
                    last = v
                    new_col.append(v)
                else:
                    new_col.append(last)
            df[c] = new_col
    
    return df

def extract_pages_text_and_tables(pdf_bytes: bytes, enable_ocr: bool = False) -> Tuple[List[Dict[str, Any]], str]:
    """
    æå–æ¯é¡µçš„æ–‡æœ¬å’Œè¡¨æ ¼
    è¿”å›ï¼šé¡µé¢æ•°æ®åˆ—è¡¨ï¼ˆå«æ–‡æœ¬å’Œè¡¨æ ¼ï¼‰ï¼Œå…¨æ–‡æ–‡æœ¬
    """
    if pdfplumber is None:
        return [], ""
    
    pages_data = []
    full_text_parts = []
    
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        # è¡¨æ ¼è®¾ç½®ï¼šå"å®½æ¾"ï¼Œæå‡è·¨é¡µ/å¤æ‚è¡¨æ ¼æå–æˆåŠŸç‡
        table_settings = {
            "vertical_strategy": "lines",
            "horizontal_strategy": "lines",
            "intersection_tolerance": 5,
            "snap_tolerance": 3,
            "join_tolerance": 3,
            "edge_min_length": 3,
            "min_words_vertical": 1,
            "min_words_horizontal": 1,
            "text_tolerance": 2,
        }
        
        for idx, page in enumerate(pdf.pages, start=1):
            # æå–æ–‡æœ¬
            text = page.extract_text() or ""
            text = normalize_multiline(text)
            
            # å¦‚æœéœ€è¦OCRä¸”æ–‡æœ¬å¤ªå°‘
            if enable_ocr and len(text) < 50:
                try:
                    import pytesseract
                    from PIL import Image
                    img = page.to_image(resolution=220).original
                    ocr_text = pytesseract.image_to_string(img, lang="chi_sim+eng")
                    if len(ocr_text) > len(text):
                        text = normalize_multiline(ocr_text)
                except Exception:
                    pass
            
            full_text_parts.append(text)
            
            # æå–è¡¨æ ¼
            raw_tables = []
            try:
                raw_tables = page.extract_tables(table_settings=table_settings) or []
            except Exception:
                raw_tables = []
            
            # æ¸…æ´—è¡¨æ ¼
            cleaned_tables = []
            for t in raw_tables:
                if t and len(t) > 0:
                    # æ¸…ç†è¡¨æ ¼æ•°æ®
                    cleaned = []
                    for row in t:
                        cleaned_row = [clean_text(cell) if cell is not None else "" for cell in row]
                        # è·³è¿‡å…¨ç©ºè¡Œ
                        if not all(cell == "" for cell in cleaned_row):
                            cleaned.append(cleaned_row)
                    if cleaned:
                        cleaned_tables.append(cleaned)
            
            pages_data.append({
                "page": idx,
                "text": text,
                "tables": cleaned_tables,
                "tables_count": len(cleaned_tables)
            })
    
    full_text = "\n".join(full_text_parts)
    return pages_data, full_text

def split_sections(full_text: str) -> Dict[str, str]:
    """æŒ‰ "ä¸€ã€/äºŒã€/ä¸‰ã€..." å¤§ç« åˆ‡åˆ†"""
    text = normalize_multiline(full_text)
    lines = text.splitlines()
    pat = re.compile(r"^\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s*[ã€\.ï¼]\s*([^\n\r]+?)\s*$")
    
    sections: Dict[str, List[str]] = {}
    cur_key = "å°é¢/å‰è¨€"
    
    for ln in lines:
        m = pat.match(ln)
        if m:
            num = m.group(1)
            title = clean_text(m.group(2))
            cur_key = f"{num}ã€{title}"
            sections.setdefault(cur_key, [])
        else:
            sections.setdefault(cur_key, []).append(ln)
    
    return {k: "\n".join(v).strip() for k, v in sections.items()}

def extract_appendix_titles(full_text: str) -> Dict[str, str]:
    """æŠ½å–"é™„è¡¨X -> æ ‡é¢˜" """
    titles: Dict[str, str] = {}
    text = normalize_multiline(full_text)
    for raw in text.splitlines():
        line = raw.strip()
        if not line:
            continue
        
        # 1) é™„è¡¨1ï¼šXXXX
        m = re.search(r"(é™„è¡¨\s*\d+)\s*[:ï¼š]\s*(.+)$", line)
        if m:
            key = re.sub(r"\s+", "", m.group(1))
            val = clean_text(m.group(2))
            if val:
                titles[key] = val
            continue
        
        # 2) ä¸ƒã€XXXXï¼ˆé™„è¡¨1ï¼‰
        m = re.search(r"^(?P<title>.+?)\s*[ï¼ˆ(]\s*(?P<key>é™„è¡¨\s*\d+)\s*[)ï¼‰]\s*$", line)
        if m:
            key = re.sub(r"\s+", "", m.group("key"))
            val = clean_text(m.group("title"))
            if val:
                titles[key] = val
            continue
        
        # 3) è¡Œå†…å‡ºç°ï¼ˆé™„è¡¨Xï¼‰
        m = re.search(r"(?P<title>.+?)\s*[ï¼ˆ(]\s*(?P<key>é™„è¡¨\s*\d+)\s*[)ï¼‰]", line)
        if m:
            key = re.sub(r"\s+", "", m.group("key"))
            val = clean_text(m.group("title"))
            if val and key not in titles:
                titles[key] = val
    
    return titles

def parse_training_objectives(section_text: str) -> Dict[str, Any]:
    """æå–"åŸ¹å…»ç›®æ ‡"æ¡ç›®"""
    raw = normalize_multiline(section_text)
    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
    items: List[str] = []
    
    pat = re.compile(r"^(?:ï¼ˆ?\s*\d+\s*ï¼‰?|\d+\s*[\.ã€ï¼])\s*(.+)$")
    for ln in lines:
        m = pat.match(ln)
        if m:
            body = clean_text(m.group(1))
            if body:
                items.append(body)
    
    # å¦‚æœæ²¡æŠ“åˆ°ç¼–å·æ¡ç›®ï¼Œé€€åŒ–ï¼šå–å‰è‹¥å¹²è¡Œ
    if not items:
        items = lines[:30]
    
    return {"count": len(items), "items": items, "raw": raw}

def parse_graduation_requirements(text_any: str) -> Dict[str, Any]:
    """æŠ½å–12æ¡æ¯•ä¸šè¦æ±‚åŠå…¶åˆ†é¡¹"""
    text = normalize_multiline(text_any or "")
    
    # å®šä½"äºŒã€æ¯•ä¸šè¦æ±‚"
    start = re.search(r"(?m)^\s*(äºŒ\s*[ã€\.ï¼]?\s*æ¯•ä¸šè¦æ±‚|æ¯•ä¸šè¦æ±‚)\s*$", text)
    if start:
        tail = text[start.start():]
    else:
        tail = text
    
    # æˆªæ–­åˆ°ä¸‹ä¸€å¤§ç« 
    end = re.search(r"(?m)^\s*[ä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*[ã€\.ï¼]", tail)
    if end:
        tail = tail[:end.start()]
    
    lines = [ln.strip() for ln in tail.splitlines()]
    
    main_pat = re.compile(r"^(?P<no>\d{1,2})\s*[\.ã€](?!\d)\s*(?P<body>.+)$")
    sub_pat = re.compile(r"^(?P<no>\d{1,2}\.\d{1,2})\s+(?P<body>.+)$")
    
    items: List[Dict[str, Any]] = []
    cur: Optional[Dict[str, Any]] = None
    cur_sub: Optional[Dict[str, Any]] = None
    
    def flush_sub():
        nonlocal cur_sub, cur
        if cur is not None and cur_sub is not None:
            cur.setdefault("subitems", []).append(cur_sub)
        cur_sub = None
    
    def flush_item():
        nonlocal cur
        if cur is not None:
            cur["title"] = clean_text(cur.get("title", ""))
            cur["body"] = clean_text(cur.get("body", ""))
            for s in cur.get("subitems", []):
                s["body"] = clean_text(s.get("body", ""))
            items.append(cur)
        cur = None
    
    for ln in lines:
        if not ln:
            continue
        
        m_main = main_pat.match(ln)
        m_sub = sub_pat.match(ln)
        
        if m_main:
            flush_sub()
            flush_item()
            no = int(m_main.group("no"))
            body_full = clean_text(m_main.group("body"))
            
            # å¤„ç†"å·¥ç¨‹çŸ¥è¯†ï¼š..."è¿™ç§
            title = ""
            body = body_full
            if "ï¼š" in body_full:
                title, body = body_full.split("ï¼š", 1)
                title = clean_text(title)
                body = clean_text(body)
            
            cur = {"no": no, "title": title, "body": body, "subitems": []}
            continue
        
        if m_sub and cur is not None:
            flush_sub()
            cur_sub = {"no": m_sub.group("no"), "body": clean_text(m_sub.group("body"))}
            continue
        
        # ç»­è¡Œ
        if cur_sub is not None:
            cur_sub["body"] += " " + ln
        elif cur is not None:
            cur["body"] += " " + ln
    
    flush_sub()
    flush_item()
    
    items = sorted(items, key=lambda x: x.get("no", 999))
    if len(items) > 12:
        items = [x for x in items if 1 <= x.get("no", 0) <= 12]
    
    return {"count": len(items), "items": items, "raw": tail.strip()}

def run_full_extract(pdf_bytes: bytes, use_ocr: bool = False) -> Dict[str, Any]:
    """
    è¿è¡Œå…¨é‡æŠ½å–
    è¿”å›ç»“æ„åŒ–çš„æŠ½å–ç»“æœ
    """
    # æå–é¡µé¢æ–‡æœ¬å’Œè¡¨æ ¼
    pages_data, full_text = extract_pages_text_and_tables(pdf_bytes, enable_ocr=use_ocr)
    
    # ç»“æ„åŒ–è§£æ
    sections = split_sections(full_text)
    appendix_titles = extract_appendix_titles(full_text)
    
    # å…³é”®ç»“æ„åŒ–ï¼šåŸ¹å…»ç›®æ ‡ã€æ¯•ä¸šè¦æ±‚
    obj_key = next((k for k in sections.keys() if "åŸ¹å…»ç›®æ ‡" in k), "")
    obj = parse_training_objectives(sections.get(obj_key, "") or full_text)
    grad = parse_graduation_requirements(full_text)
    
    # å¤„ç†è¡¨æ ¼
    all_tables = []
    total_tables = 0
    
    for page_data in pages_data:
        page_no = page_data["page"]
        page_text = page_data["text"]
        page_tables = page_data["tables"]
        
        total_tables += len(page_tables)
        
        for i, table_data in enumerate(page_tables):
            if table_data and len(table_data) > 0:
                # åˆ›å»ºDataFrame
                if len(table_data) > 1:
                    # å°è¯•å°†ç¬¬ä¸€è¡Œä½œä¸ºè¡¨å¤´
                    header = table_data[0]
                    body = table_data[1:]
                    
                    # åˆ¤æ–­è¡¨å¤´æ˜¯å¦å¯ç”¨
                    non_empty = sum(1 for x in header if clean_text(x) != "")
                    if non_empty >= max(1, len(header) // 2):
                        df = pd.DataFrame(body, columns=header)
                    else:
                        df = pd.DataFrame(table_data)
                else:
                    df = pd.DataFrame(table_data)
                
                # åå¤„ç†
                df = postprocess_table_df(df)
                
                # æ·»åŠ åˆ°ç»“æœ
                table_info = {
                    "page": page_no,
                    "title": f"ç¬¬{page_no}é¡µè¡¨æ ¼{i+1}",
                    "data": df.values.tolist(),
                    "columns": df.columns.tolist(),
                    "shape": df.shape
                }
                all_tables.append(table_info)
    
    # æ„å»ºç»“æœ
    result = {
        "page_count": len(pages_data),
        "table_count": total_tables,
        "ocr_used": use_ocr,
        "file_sha256": sha256_bytes(pdf_bytes),
        "extracted_at": datetime.now().isoformat(timespec="seconds"),
        "pages_data": pages_data,
        "sections": sections,
        "appendix_titles": appendix_titles,
        "training_objectives": obj,
        "graduation_requirements": grad,
        "tables": all_tables,
        "full_text": full_text
    }
    
    return result

# ---------------------------
# æ–‡ä»¶å¤„ç†
# ---------------------------
def extract_text_from_upload(file) -> str:
    name = (file.name or "").lower()
    file.seek(0)
    
    if name.endswith(".pdf") and pdfplumber is not None:
        with pdfplumber.open(file) as pdf:
            texts = []
            for p in pdf.pages:
                t = p.extract_text() or ""
                if t.strip():
                    texts.append(t)
            return "\n".join(texts).strip()
    
    if name.endswith(".docx") and Document is not None:
        file.seek(0)
        doc = Document(file)
        paras = [p.text for p in doc.paragraphs if p.text.strip()]
        return "\n".join(paras).strip()
    
    file.seek(0)
    try:
        return file.read().decode("utf-8", errors="ignore")
    except Exception:
        return ""

# ---------------------------
# é€šç”¨å·¥å…·å‡½æ•°
# ---------------------------
def type_label(a_type: str) -> str:
    for t, name in DOC_TYPES:
        if t == a_type:
            return name
    return a_type

def dep_status(project_id: int, a_type: str) -> Tuple[bool, List[Tuple[str, bool]]]:
    req = DEP_RULES.get(a_type, [])
    detail = []
    ok = True
    for r in req:
        exists = get_artifact(project_id, r) is not None
        detail.append((r, exists))
        ok = ok and exists
    return ok, detail

def render_depbar(project_id: int, a_type: str):
    ok, detail = dep_status(project_id, a_type)
    chips = []
    for r, exists in detail:
        cls = "ok" if exists else "bad"
        chips.append(f'<span class="badge {cls}">{type_label(r)}</span>')
    st.markdown(
        f"""
<div class="depbar">
    <div class="depitem"><b>ä¾èµ–æ£€æŸ¥</b>ï¼š{"âœ…é½å…¨" if ok else "âš ï¸ç¼ºå¤±ä¸Šæ¸¸"}</div>
    <div class="depitem">{''.join(chips) if chips else '<span class="badge ok">æ— ä¸Šæ¸¸ä¾èµ–</span>'}</div>
</div>
""",
        unsafe_allow_html=True,
    )

def artifact_toolbar(a: Dict[str, Any]):
    import html as _html
    st.markdown(
        f"""
<div class="card">
    <div style="display:flex; justify-content:space-between; gap:12px; align-items:center;">
        <div>
            <div style="font-size:18px; font-weight:800;">{_html.escape(a['title'])}</div>
            <div style="opacity:.75; font-size:12px; margin-top:4px;">
                ç±»å‹ï¼š{type_label(a['type'])} ï½œ Hashï¼š<code>{a['hash'][:12]}</code> ï½œ æ›´æ–°æ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(a['updated_at']))}
            </div>
        </div>
        <div>
            <span class="badge ok">å¯ç¼–è¾‘</span>
            <span class="badge warn">å¯ç‰ˆæœ¬åŒ–</span>
            <span class="badge warn">ä¾èµ–å¯è¿½æº¯</span>
        </div>
    </div>
</div>
""",
        unsafe_allow_html=True,
    )

def md_textarea(label: str, value: str, height: int = 420, key: str = "") -> str:
    return st.text_area(label, value=value, height=height, key=key)

# ---------------------------
# æ¨¡æ¿å‡½æ•°
# ---------------------------
def template_training_plan(major: str, grade: str, course_group: str) -> str:
    return f"""# {grade}çº§ã€Š{major}ã€‹åŸ¹å…»æ–¹æ¡ˆï¼ˆç¤ºä¾‹ï¼‰

## ä¸€ã€åŸ¹å…»ç›®æ ‡
- é¢å‘å·¥ç¨‹å®è·µï¼Œå…·å¤‡æ‰å®çš„æ•°å­¦/åŠ›å­¦/ææ–™åŸºç¡€
- å…·å¤‡ææ–™æˆå‹ä¸åˆ¶é€ è¿‡ç¨‹çš„åˆ†æã€è®¾è®¡ä¸ä¼˜åŒ–èƒ½åŠ›
- å…·å¤‡å·¥ç¨‹ä¼¦ç†ã€å›¢é˜Ÿåä½œä¸ç»ˆèº«å­¦ä¹ èƒ½åŠ›

## äºŒã€æ¯•ä¸šè¦æ±‚ï¼ˆç¤ºä¾‹ï¼‰
1. å·¥ç¨‹çŸ¥è¯†
2. é—®é¢˜åˆ†æ
3. è®¾è®¡/å¼€å‘è§£å†³æ–¹æ¡ˆ
4. ç ”ç©¶
5. ç°ä»£å·¥å…·ä½¿ç”¨
6. å·¥ç¨‹ä¸ç¤¾ä¼š
7. ç¯å¢ƒä¸å¯æŒç»­å‘å±•
8. èŒä¸šè§„èŒƒ
9. ä¸ªäººä¸å›¢é˜Ÿ
10. æ²Ÿé€š
11. é¡¹ç›®ç®¡ç†
12. ç»ˆèº«å­¦ä¹ 

## ä¸‰ã€è¯¾ç¨‹ä½“ç³»ï¼š{course_group}
- é€šè¯†ä¸åŸºç¡€
- ä¸“ä¸šæ ¸å¿ƒ
- ä¸“ä¸šæ–¹å‘
- å®è·µç¯èŠ‚
"""

# ---------------------------
# é¡¶éƒ¨ä¸ä¾§è¾¹æ 
# ---------------------------
def topbar():
    st.markdown(
        """
<div class="topbar">
    <div class="title">æ•™å­¦æ™ºèƒ½ä½“å¹³å°ï¼ˆPDFå…¨é‡æŠ½å–ç‰ˆï¼‰</div>
    <div class="sub">åŸ¹å…»æ–¹æ¡ˆPDFå…¨é‡æŠ½å– â†’ å¤§çº² â†’ æ—¥å† â†’ æ•™æ¡ˆ â†’ è¯•å·/å®¡æ ¸ â†’ è¾¾æˆæŠ¥å‘Š â†’ æˆè¯¾æ‰‹å†Œ ï½œ æ”¯æŒä¸Šä¼ ã€ä¿®æ”¹ã€ç‰ˆæœ¬ä¸ä¾èµ–è¿½æº¯</div>
</div>
""",
        unsafe_allow_html=True,
    )

# åˆå§‹åŒ–DB
ensure_db_schema()
topbar()

# ä¾§è¾¹æ é…ç½®
st.sidebar.markdown("## è¿è¡Œæ¨¡å¼")
run_mode = st.sidebar.radio("è¿è¡Œæ¨¡å¼", ["æ¼”ç¤ºæ¨¡å¼ï¼ˆæ— APIï¼‰", "åœ¨çº¿æ¨¡å¼ï¼ˆåƒé—®APIï¼‰"], index=0)
st.sidebar.caption("æ¼”ç¤ºæ¨¡å¼ä¸éœ€è¦ Keyï¼›åœ¨çº¿æ¨¡å¼è¯·åœ¨ Secrets ä¸­é…ç½® QWEN_API_KEYã€‚")

st.sidebar.markdown("## é¡¹ç›®ï¼ˆä¸“ä¸š/å¹´çº§/è¯¾ç¨‹ä½“ç³»ï¼‰")
projects = get_projects()
p_names = ["ï¼ˆæ–°å»ºé¡¹ç›®ï¼‰"] + [f"{pid} Â· {name}" for pid, name in projects]
p_sel = st.sidebar.selectbox("é€‰æ‹©é¡¹ç›®", p_names, index=0)

if p_sel == "ï¼ˆæ–°å»ºé¡¹ç›®ï¼‰":
    with st.sidebar.expander("åˆ›å»ºæ–°é¡¹ç›®", expanded=True):
        pname = st.text_input("é¡¹ç›®åç§°", value="ææ–™æˆå‹-æ•™è¯„ä¸€ä½“åŒ–ç¤ºä¾‹", key="new_pname")
        major = st.text_input("ä¸“ä¸š", value="ææ–™æˆå‹åŠæ§åˆ¶å·¥ç¨‹", key="new_major")
        grade = st.text_input("å¹´çº§", value="22", key="new_grade")
        course_group = st.text_input("è¯¾ç¨‹ä½“ç³»/æ–¹å‘", value="ææ–™æˆå‹-æ•°å€¼æ¨¡æ‹Ÿæ–¹å‘", key="new_group")
        if st.button("åˆ›å»ºé¡¹ç›®", type="primary"):
            pid = create_project(pname, {"major": major, "grade": grade, "course_group": course_group})
            st.success("å·²åˆ›å»ºé¡¹ç›®ï¼Œè¯·åœ¨ä¸‹æ‹‰ä¸­é€‰æ‹©å®ƒã€‚")
            st.rerun()
    project_id = None
else:
    project_id = int(p_sel.split("Â·")[0].strip())

st.sidebar.markdown("## åŠŸèƒ½æ¨¡å—")
module = st.sidebar.radio("å¯¼èˆª", [name for _, name in DOC_TYPES], index=1)
type_by_name = {name: t for t, name in DOC_TYPES}
current_type = type_by_name[module]

# ---------------------------
# é¡µé¢è·¯ç”±
# ---------------------------
def ensure_project():
    if project_id is None:
        st.info("è¯·å…ˆåœ¨å·¦ä¾§åˆ›å»ºå¹¶é€‰æ‹©ä¸€ä¸ªé¡¹ç›®ã€‚")
        st.stop()

def pick_parents_for(project_id: int, a_type: str) -> List[int]:
    req = DEP_RULES.get(a_type, [])
    parent_ids: List[int] = []
    for r in req:
        pa = get_artifact(project_id, r)
        if pa:
            parent_ids.append(pa["id"])
    if a_type == "manual":
        ev = get_artifact(project_id, "evidence")
        if ev:
            parent_ids.append(ev["id"])
    return parent_ids

def page_overview():
    ensure_project()
    st.markdown("### é¦–é¡µæ€»è§ˆ")
    arts = list_artifacts(project_id)
    if not arts:
        st.info("å½“å‰é¡¹ç›®è¿˜æ²¡æœ‰ä»»ä½•æ–‡æ¡£ã€‚å»ºè®®å…ˆä»â€˜åŸ¹å…»æ–¹æ¡ˆï¼ˆåº•åº§ï¼‰â€™å¼€å§‹ã€‚")
        return
    
    st.markdown('<div class="card">ğŸ“Œ å½“å‰é¡¹ç›®å·²æœ‰æ–‡æ¡£ï¼ˆæœ€è¿‘æ›´æ–°åœ¨å‰ï¼‰</div>', unsafe_allow_html=True)
    rows = []
    for a in arts:
        rows.append({
            "ç±»å‹": type_label(a["type"]),
            "æ ‡é¢˜": a["title"],
            "Hash(å‰12)": a["hash"][:12],
            "æ›´æ–°æ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(a["updated_at"])),
        })
    st.dataframe(rows, use_container_width=True)

def page_training_plan():
    ensure_project()
    a = get_artifact(project_id, "training_plan")
    render_depbar(project_id, "training_plan")
    
    st.markdown("### åŸ¹å…»æ–¹æ¡ˆï¼ˆåº•åº§ï¼‰")
    st.caption("æ¨èï¼šä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆPDF â†’ å…¨é‡æŠ½å– â†’ è¯†åˆ«æ¸…å•ç¡®è®¤/ä¿®æ­£ â†’ ä¿å­˜ï¼ˆç»“æ„åŒ–åº•åº§ï¼‰ã€‚")
    
    tab1, tab2, tab3, tab4 = st.tabs(["ç”Ÿæˆ/ä¸Šä¼ &è¯†åˆ«ç¡®è®¤", "é¢„è§ˆ", "ç¼–è¾‘", "ç‰ˆæœ¬/å¯¼å‡º"])
    
    with tab1:
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.markdown("#### æ–¹å¼Aï¼šä¸€é”®ç”Ÿæˆï¼ˆæ¼”ç¤º/å¿«é€Ÿï¼‰")
            major = st.text_input("ä¸“ä¸š", value="ææ–™æˆå‹åŠæ§åˆ¶å·¥ç¨‹", key="tp_major")
            grade = st.text_input("å¹´çº§", value="22", key="tp_grade")
            group = st.text_input("è¯¾ç¨‹ä½“ç³»/æ–¹å‘", value="ææ–™æˆå‹-æ•°å€¼æ¨¡æ‹Ÿæ–¹å‘", key="tp_group")
            if st.button("ç”ŸæˆåŸ¹å…»æ–¹æ¡ˆå¹¶ä¿å­˜", type="primary"):
                md = template_training_plan(major, grade, group)
                a = upsert_artifact(
                    project_id,
                    "training_plan",
                    f"{grade}çº§ã€Š{major}ã€‹åŸ¹å…»æ–¹æ¡ˆ",
                    md,
                    {"major": major, "grade": grade, "course_group": group, "confirmed": True},
                    [],
                    note="generate",
                )
                st.success("å·²ä¿å­˜åŸ¹å…»æ–¹æ¡ˆï¼ˆå¯ä½œä¸ºåç»­æ–‡ä»¶ä¾èµ–åº•åº§ï¼‰")
                st.rerun()
        
        with col2:
            st.markdown("#### æ–¹å¼Bï¼šä¸Šä¼ PDFå…¨é‡æŠ½å–ï¼ˆæ¨èï¼‰")
            up = st.file_uploader("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆPDFæ–‡ä»¶", type=["pdf"], key="tp_upload")
            use_ocr = st.checkbox("å¯ç”¨OCRï¼ˆé’ˆå¯¹æ‰«æç‰ˆPDFï¼‰", value=False)
            
            if up is not None and st.button("å¼€å§‹å…¨é‡æŠ½å–", key="tp_start_extract"):
                pdf_bytes = up.read()
                with st.spinner("æ­£åœ¨å…¨é‡æŠ½å–PDF..."):
                    extract_result = run_full_extract(pdf_bytes, use_ocr=use_ocr)
                
                # ä¿å­˜æŠ½å–ç»“æœåˆ°session
                st.session_state["tp_extract"] = {
                    "source": up.name,
                    "pdf_bytes": pdf_bytes,
                    "extract_result": extract_result,
                    "confirmed": False
                }
                st.success("PDFæŠ½å–å®Œæˆï¼è¯·åœ¨ä¸‹æ–¹ç¡®è®¤/ä¿®æ­£æŠ½å–ç»“æœã€‚")
        
        # è¯†åˆ«æ¸…å•ç¡®è®¤ç•Œé¢
        if "tp_extract" in st.session_state:
            ex = st.session_state["tp_extract"]
            extract_result = ex["extract_result"]
            
            st.markdown("---")
            st.markdown("### PDFå…¨é‡æŠ½å–ç»“æœï¼ˆè¯·ç¡®è®¤/ä¿®æ­£ï¼‰")
            
            # åŸºæœ¬ä¿¡æ¯
            colA, colB, colC = st.columns(3)
            with colA:
                major2 = st.text_input("ä¸“ä¸šï¼ˆä»PDFä¸­è¯†åˆ«ï¼‰", 
                                      value=extract_result.get("major_guess", "") or "ææ–™æˆå‹åŠæ§åˆ¶å·¥ç¨‹", 
                                      key="tp_major_fix")
                grade2 = st.text_input("å¹´çº§ï¼ˆä»PDFä¸­è¯†åˆ«ï¼‰", 
                                      value=extract_result.get("grade_guess", "") or "22", 
                                      key="tp_grade_fix")
            with colB:
                course_group2 = st.text_input("è¯¾ç¨‹ä½“ç³»/æ–¹å‘", 
                                             value=extract_result.get("course_group_guess", "") or "ææ–™æˆå‹æ–¹å‘", 
                                             key="tp_group_fix")
                confirmed_flag = st.checkbox("æˆ‘å·²ç¡®è®¤ä»¥ä¸Šä¿¡æ¯å¤§ä½“æ­£ç¡®", value=False, key="tp_confirm_flag")
            with colC:
                st.metric("æ€»é¡µæ•°", extract_result["page_count"])
                st.metric("è¡¨æ ¼æ€»æ•°", extract_result["table_count"])
            
            st.markdown("#### 1) åŸ¹å…»ç›®æ ‡ï¼ˆå¯ç¼–è¾‘ï¼‰")
            goals = extract_result["training_objectives"].get("items", [])
            goals_text = st.text_area(
                "æ¯è¡Œä¸€ä¸ªç›®æ ‡ï¼ˆå¯å¢åˆ /æ”¹å†™ï¼‰",
                value="\n".join(goals) if goals else "",
                height=140,
                key="tp_goals_edit",
            )
            goals_final = [x.strip() for x in goals_text.splitlines() if x.strip()]
            
            st.markdown("#### 2) æ¯•ä¸šè¦æ±‚ï¼ˆå¯ç¼–è¾‘ï¼‰")
            grad_items = extract_result["graduation_requirements"].get("items", [])
            if grad_items:
                # åˆ›å»ºå¯ç¼–è¾‘çš„DataFrame
                grad_data = []
                for item in grad_items:
                    grad_data.append({
                        "ç¼–å·": item.get("no", ""),
                        "æ ‡é¢˜": item.get("title", ""),
                        "å†…å®¹": item.get("body", "")
                    })
                df_grad = pd.DataFrame(grad_data)
                df_grad_edited = st.data_editor(df_grad, use_container_width=True, num_rows="dynamic", key="tp_grad_editor")
                outcomes_final = []
                for _, row in df_grad_edited.iterrows():
                    if str(row["ç¼–å·"]).strip():
                        outcomes_final.append({
                            "no": str(row["ç¼–å·"]).strip(),
                            "name": str(row["æ ‡é¢˜"]).strip(),
                            "body": str(row["å†…å®¹"]).strip()
                        })
            else:
                st.info("æœªè¯†åˆ«åˆ°æ¯•ä¸šè¦æ±‚ï¼Œè¯·æ‰‹å·¥å½•å…¥")
                grad_json = st.text_area(
                    "æ¯•ä¸šè¦æ±‚ JSON",
                    value=json.dumps([{"no": "1", "name": "å·¥ç¨‹çŸ¥è¯†", "body": ""}], ensure_ascii=False, indent=2),
                    height=160,
                    key="tp_grad_json",
                )
                try:
                    outcomes_final = json.loads(grad_json) if grad_json.strip() else []
                except Exception:
                    outcomes_final = []
            
            st.markdown("#### 3) æŠ½å–çš„è¡¨æ ¼ï¼ˆå¯ç¼–è¾‘ç¡®è®¤ï¼‰")
            tables = extract_result.get("tables", [])
            confirmed_tables = []
            
            if tables:
                for i, table_info in enumerate(tables[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªè¡¨æ ¼
                    st.markdown(f"**è¡¨æ ¼{i+1}ï¼ˆç¬¬{table_info['page']}é¡µï¼‰**")
                    df = pd.DataFrame(table_info["data"], columns=table_info["columns"])
                    df_edited = st.data_editor(df, use_container_width=True, height=200, key=f"tp_table_{i}")
                    
                    confirm_table = st.checkbox(f"ç¡®è®¤é‡‡ç”¨æ­¤è¡¨æ ¼", value=True, key=f"tp_table_confirm_{i}")
                    if confirm_table:
                        confirmed_tables.append({
                            "page": table_info["page"],
                            "title": table_info["title"],
                            "data": df_edited.values.tolist(),
                            "columns": df_edited.columns.tolist()
                        })
            else:
                st.info("æœªæŠ½å–åˆ°è¡¨æ ¼")
            
            st.markdown("#### 4) ç« èŠ‚ç»“æ„")
            sections = extract_result.get("sections", {})
            with st.expander("æŸ¥çœ‹ç« èŠ‚ç»“æ„", expanded=False):
                for section_name, section_content in list(sections.items())[:10]:  # æ˜¾ç¤ºå‰10ä¸ªç« èŠ‚
                    st.markdown(f"**{section_name}**")
                    st.text(section_content[:500] + "..." if len(section_content) > 500 else section_content)
            
            st.markdown("---")
            if st.button("âœ… ç¡®è®¤å¹¶ä¿å­˜ä¸ºåŸ¹å…»æ–¹æ¡ˆåº•åº§", type="primary", disabled=not confirmed_flag):
                # æ„å»ºcontent_json
                content_json = {
                    "source": ex["source"],
                    "confirmed": True,
                    "major": major2,
                    "grade": grade2,
                    "course_group": course_group2,
                    "goals": goals_final,
                    "outcomes": outcomes_final,
                    "tables": confirmed_tables,
                    "extract_metadata": {
                        "page_count": extract_result["page_count"],
                        "table_count": extract_result["table_count"],
                        "sections_count": len(sections),
                        "extracted_at": extract_result["extracted_at"]
                    }
                }
                
                # ç”Ÿæˆmarkdown
                md = f"# åŸ¹å…»æ–¹æ¡ˆï¼ˆPDFæŠ½å–-å·²ç¡®è®¤ï¼‰\n\n"
                md += f"- ä¸“ä¸šï¼š{major2}\n- å¹´çº§ï¼š{grade2}\n- è¯¾ç¨‹ä½“ç³»/æ–¹å‘ï¼š{course_group2}\n\n"
                md += "## ä¸€ã€åŸ¹å…»ç›®æ ‡ï¼ˆç¡®è®¤ç‰ˆï¼‰\n" + ("\n".join([f"- {x}" for x in goals_final]) if goals_final else "- ï¼ˆæœªå¡«ï¼‰") + "\n\n"
                md += "## äºŒã€æ¯•ä¸šè¦æ±‚ï¼ˆç¡®è®¤ç‰ˆï¼‰\n" + ("\n".join([f"- {o.get('no','')}. {o.get('name','')}: {o.get('body','')}" for o in outcomes_final]) if outcomes_final else "- ï¼ˆæœªå¡«ï¼‰") + "\n\n"
                md += "## ä¸‰ã€æŠ½å–è¡¨æ ¼ï¼ˆå…±{}ä¸ªï¼‰\n".format(len(confirmed_tables))
                for i, tbl in enumerate(confirmed_tables, 1):
                    md += f"- è¡¨æ ¼{i}ï¼ˆç¬¬{tbl['page']}é¡µï¼‰: {tbl['title']}\n"
                md += "\n## å››ã€ç« èŠ‚ç»“æ„\n"
                for section_name in list(sections.keys())[:5]:
                    md += f"- {section_name}\n"
                
                title = f"åŸ¹å…»æ–¹æ¡ˆï¼ˆPDFæŠ½å–ç¡®è®¤ç‰ˆï¼‰-{ex['source']}"
                a2 = upsert_artifact(project_id, "training_plan", title, md, content_json, [], note="pdf-extract-confirm")
                st.success("å·²ä¿å­˜â€˜ç¡®è®¤ç‰ˆåŸ¹å…»æ–¹æ¡ˆåº•åº§â€™ã€‚åç»­ç”Ÿæˆå¤§çº²ä¼šä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–å­—æ®µã€‚")
                st.session_state.pop("tp_extract", None)
                st.rerun()
            
            if st.button("æ¸…é™¤æœ¬æ¬¡æŠ½å–ç»“æœï¼ˆä¸ä¿å­˜ï¼‰"):
                st.session_state.pop("tp_extract", None)
                st.info("å·²æ¸…é™¤ã€‚")
    
    with tab2:
        if not a:
            st.info("æš‚æ— åŸ¹å…»æ–¹æ¡ˆã€‚è¯·å…ˆç”Ÿæˆæˆ–ä¸Šä¼ å¹¶ç¡®è®¤ã€‚")
        else:
            artifact_toolbar(a)
            st.markdown("#### ç»“æ„åŒ–å†…å®¹")
            st.json(a.get("content_json") or {})
            st.markdown("#### Markdowné¢„è§ˆ")
            st.markdown(a["content_md"][:2000] + "..." if len(a["content_md"]) > 2000 else a["content_md"])
    
    with tab3:
        if not a:
            st.info("æš‚æ— åŸ¹å…»æ–¹æ¡ˆã€‚è¯·å…ˆç”Ÿæˆæˆ–ä¸Šä¼ ã€‚")
        else:
            edited = md_textarea("åœ¨çº¿ç¼–è¾‘åŸ¹å…»æ–¹æ¡ˆï¼ˆæ”¯æŒç›´æ¥ä¿®æ”¹ï¼‰", a["content_md"], key="tp_edit")
            note = st.text_input("ä¿å­˜è¯´æ˜ï¼ˆå¯é€‰ï¼‰", value="edit", key="tp_note")
            if st.button("ä¿å­˜ä¿®æ”¹ï¼ˆç”Ÿæˆæ–°ç‰ˆæœ¬ï¼‰", type="primary", key="tp_save"):
                a2 = upsert_artifact(project_id, "training_plan", a["title"], edited, a["content_json"], [], note=note)
                st.success("å·²ä¿å­˜ã€‚åç»­ä¾èµ–æ–‡ä»¶å°†å¼•ç”¨æ›´æ–°åçš„åŸ¹å…»æ–¹æ¡ˆã€‚")
                st.rerun()
    
    with tab4:
        if not a:
            st.info("æš‚æ— åŸ¹å…»æ–¹æ¡ˆã€‚")
        else:
            vers = get_versions(a["id"])
            st.markdown("#### ç‰ˆæœ¬è®°å½•")
            st.dataframe(vers if vers else [], use_container_width=True)

# å…¶ä»–é¡µé¢å‡½æ•°ï¼ˆä¿æŒåŸæœ‰ç»“æ„ï¼Œä½†ç®€åŒ–å®ç°ï¼‰
def page_syllabus():
    ensure_project()
    render_depbar(project_id, "syllabus")
    tp = get_artifact(project_id, "training_plan")
    a = get_artifact(project_id, "syllabus")
    
    st.markdown("### è¯¾ç¨‹æ•™å­¦å¤§çº²")
    tab1, tab2, tab3, tab4 = st.tabs(["ç”Ÿæˆ", "é¢„è§ˆ", "ç¼–è¾‘", "ç‰ˆæœ¬/å¯¼å‡º"])
    
    with tab1:
        if not tp:
            st.warning("è¯·å…ˆåˆ›å»ºåŸ¹å…»æ–¹æ¡ˆ")
        else:
            course_name = st.text_input("è¯¾ç¨‹åç§°", value="æ•°å€¼æ¨¡æ‹Ÿåœ¨ææ–™æˆå‹ä¸­çš„åº”ç”¨")
            if st.button("ç”Ÿæˆæ•™å­¦å¤§çº²"):
                md = f"# ã€Š{course_name}ã€‹æ•™å­¦å¤§çº²\n\nåŸºäºåŸ¹å…»æ–¹æ¡ˆç”Ÿæˆçš„æ•™å­¦å¤§çº²..."
                a2 = upsert_artifact(project_id, "syllabus", f"ã€Š{course_name}ã€‹æ•™å­¦å¤§çº²", md, {}, [tp["id"]], note="generate")
                st.success("å·²ç”Ÿæˆæ•™å­¦å¤§çº²")
                st.rerun()
    
    with tab2:
        if a:
            artifact_toolbar(a)
            st.markdown(a["content_md"])
    
    with tab3:
        if a:
            edited = md_textarea("ç¼–è¾‘æ•™å­¦å¤§çº²", a["content_md"])
            if st.button("ä¿å­˜"):
                parents = pick_parents_for(project_id, "syllabus")
                a2 = upsert_artifact(project_id, "syllabus", a["title"], edited, a["content_json"], parents, note="edit")
                st.success("å·²ä¿å­˜")
                st.rerun()

def page_calendar():
    ensure_project()
    render_depbar(project_id, "calendar")
    st.markdown("### æ•™å­¦æ—¥å†")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_lesson_plan():
    ensure_project()
    render_depbar(project_id, "lesson_plan")
    st.markdown("### æ•™æ¡ˆ")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_assessment():
    ensure_project()
    render_depbar(project_id, "assessment")
    st.markdown("### ä½œä¸š/é¢˜åº“/è¯•å·æ–¹æ¡ˆ")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_review():
    ensure_project()
    render_depbar(project_id, "review")
    st.markdown("### å®¡æ ¸è¡¨")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_report():
    ensure_project()
    render_depbar(project_id, "report")
    st.markdown("### è¯¾ç¨‹ç›®æ ‡è¾¾æˆæŠ¥å‘Š")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_manual():
    ensure_project()
    render_depbar(project_id, "manual")
    st.markdown("### æˆè¯¾æ‰‹å†Œ")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_evidence():
    ensure_project()
    render_depbar(project_id, "evidence")
    st.markdown("### è¯¾å ‚çŠ¶æ€ä¸è¿‡ç¨‹è¯æ®")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_vge():
    ensure_project()
    st.markdown("### è¯æ®é“¾ä¸å¯éªŒè¯ç”Ÿæˆï¼ˆVGEï¼‰")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_dep_graph():
    ensure_project()
    st.markdown("### ä¾èµ–å›¾å¯è§†åŒ–")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_docx_export():
    ensure_project()
    st.markdown("### æ¨¡æ¿åŒ–DOCXå¯¼å‡º")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

# ---------------------------
# è·¯ç”±é…ç½®
# ---------------------------
ROUTES = {
    "overview": page_overview,
    "training_plan": page_training_plan,
    "syllabus": page_syllabus,
    "calendar": page_calendar,
    "lesson_plan": page_lesson_plan,
    "assessment": page_assessment,
    "review": page_review,
    "report": page_report,
    "manual": page_manual,
    "evidence": page_evidence,
    "vge": page_vge,
    "dep_graph": page_dep_graph,
    "docx_export": page_docx_export,
}

# æ‰§è¡Œå½“å‰é¡µé¢
if project_id:
    fn = ROUTES.get(current_type, page_overview)
    fn()
else:
    st.info("è¯·å…ˆåœ¨å·¦ä¾§åˆ›å»ºæˆ–é€‰æ‹©é¡¹ç›®")