# -*- coding: utf-8 -*-
"""
æ•™å­¦æ™ºèƒ½ä½“å¹³å° - æ•´åˆPDFå…¨é‡æŠ½å–ç‰ˆï¼ˆå¢å¼ºç‰ˆï¼‰
æ•´åˆäº†å®Œæ•´çš„PDFè§£æèƒ½åŠ›å’Œæ•™å­¦æ–‡æ¡£é“¾ç®¡ç†ï¼Œç¡®ä¿å¯¹åŸ¹å…»æ–¹æ¡ˆPDFçš„æ‰€æœ‰ç« èŠ‚å’Œè¡¨æ ¼å®Œæ•´æå–å’Œæ˜¾ç¤ºã€‚
"""

import os
import io
import re
import json
import time
import base64
import hashlib
import sqlite3
import zipfile
import threading
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple
import pandas as pd
import streamlit as st
from dataclasses import asdict, dataclass


# -------- å¯é€‰è§£æä¾èµ– --------
try:
    import pdfplumber
except Exception:
    pdfplumber = None
    st.error("ç¼ºå°‘ä¾èµ– pdfplumberï¼Œè¯·å®‰è£…ï¼špip install pdfplumber")

try:
    from docx import Document
except Exception:
    Document = None

# ---------------------------
# åŸºç¡€é…ç½®
# ---------------------------
st.set_page_config(page_title="æ•™å­¦æ™ºèƒ½ä½“å¹³å°", layout="wide")

DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)
DB_PATH = os.path.join(DATA_DIR, "app.db")

_DB_LOCK = threading.Lock()

# ---------------------------
# UI ç¾åŒ–ï¼ˆCSSï¼‰
# ---------------------------
def inject_css():
    st.markdown(
        """
<style>
.main .block-container {
    padding-top: 1.0rem;
    padding-bottom: 2rem;
    max-width: 100% !important;
    padding-left: 2rem;
    padding-right: 2rem;
}
h1, h2, h3 { letter-spacing: .2px; }
code { font-size: 0.9em; }

.topbar{
    padding: 18px 18px;
    border-radius: 18px;
    background: linear-gradient(90deg, #0ea5e9 0%, #6366f1 55%, #8b5cf6 100%);
    color: white;
    box-shadow: 0 8px 24px rgba(0,0,0,.12);
}
.topbar .title{ font-size: 30px; font-weight: 800; }
.topbar .sub{ opacity: .9; margin-top: 6px; font-size: 14px; }

.card{
    border: 1px solid rgba(0,0,0,.08);
    border-radius: 18px;
    padding: 16px 16px;
    background: rgba(255,255,255,.6);
    box-shadow: 0 6px 16px rgba(0,0,0,.06);
}
.badge{
    display:inline-block; padding: 2px 10px; border-radius: 999px;
    font-size: 12px; border: 1px solid rgba(0,0,0,.12); margin-right: 6px;
}
.badge.ok { background:#ecfdf5; color:#065f46; border-color:#a7f3d0; }
.badge.warn { background:#fffbeb; color:#92400e; border-color:#fde68a; }
.badge.bad { background:#fef2f2; color:#991b1b; border-color:#fecaca; }

.depbar{ display:flex; gap:8px; flex-wrap: wrap; padding: 10px 0; }
.depitem{
    padding: 8px 10px; border-radius: 14px; border: 1px solid rgba(0,0,0,.10);
    background: rgba(255,255,255,.7); font-size: 13px;
}
.depitem b{ margin-right:6px; }

.docbox{
    border: 1px solid rgba(0,0,0,.10);
    border-radius: 18px;
    padding: 14px 16px;
    background: rgba(255,255,255,.75);
    line-height: 1.55;
    white-space: normal;
}
section[data-testid="stSidebar"] .stMarkdown h2{ font-size: 18px; font-weight: 800; }
div[data-testid="stDataFrame"] { border-radius: 14px; overflow:hidden; }

/* ç¡®ä¿è¡¨æ ¼åˆ—åæœ‰æ•ˆ */
.stDataFrame th {
    font-weight: 600 !important;
}
</style>
""",
        unsafe_allow_html=True,
    )

inject_css()

# ---------------------------
# æ•°æ®åº“å±‚
# ---------------------------
def db() -> sqlite3.Connection:
    os.makedirs(DATA_DIR, exist_ok=True)
    conn = sqlite3.connect(DB_PATH, check_same_thread=False, timeout=30)
    conn.execute("PRAGMA foreign_keys=ON;")
    conn.execute("PRAGMA busy_timeout=5000;")
    try:
        conn.execute("PRAGMA journal_mode=WAL;")
    except Exception:
        conn.execute("PRAGMA journal_mode=DELETE;")
    return conn

def init_db():
    with _DB_LOCK:
        conn = db()
        conn.execute(
            """
CREATE TABLE IF NOT EXISTS projects(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL,
    meta_json TEXT DEFAULT '{}',
    created_at INTEGER NOT NULL
);
"""
        )
        conn.execute(
            """
CREATE TABLE IF NOT EXISTS artifacts(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    project_id INTEGER NOT NULL,
    type TEXT NOT NULL,
    title TEXT NOT NULL,
    content_md TEXT NOT NULL,
    content_json TEXT NOT NULL DEFAULT '{}',
    hash TEXT NOT NULL,
    created_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL,
    FOREIGN KEY(project_id) REFERENCES projects(id) ON DELETE CASCADE
);
"""
        )
        conn.execute(
            """
CREATE TABLE IF NOT EXISTS versions(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    artifact_id INTEGER NOT NULL,
    version_no INTEGER NOT NULL,
    content_md TEXT NOT NULL,
    content_json TEXT NOT NULL,
    hash TEXT NOT NULL,
    created_at INTEGER NOT NULL,
    note TEXT DEFAULT '',
    FOREIGN KEY(artifact_id) REFERENCES artifacts(id) ON DELETE CASCADE
);
"""
        )
        conn.execute(
            """
CREATE TABLE IF NOT EXISTS edges(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    project_id INTEGER NOT NULL,
    child_artifact_id INTEGER NOT NULL,
    parent_artifact_id INTEGER NOT NULL,
    created_at INTEGER NOT NULL,
    FOREIGN KEY(project_id) REFERENCES projects(id) ON DELETE CASCADE,
    FOREIGN KEY(child_artifact_id) REFERENCES artifacts(id) ON DELETE CASCADE,
    FOREIGN KEY(parent_artifact_id) REFERENCES artifacts(id) ON DELETE CASCADE
);
"""
        )
        conn.commit()
        conn.close()

def ensure_db_schema():
    init_db()

def now_ts() -> int:
    return int(time.time())

def sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def compute_hash(content_md: str, content_json: Dict[str, Any], parent_hashes: List[str]) -> str:
    payload = {"content_md": content_md, "content_json": content_json, "parents": parent_hashes}
    return sha256_text(json.dumps(payload, ensure_ascii=False, sort_keys=True))

# ---------------------------
# æ•°æ®åº“æ“ä½œ
# ---------------------------
def get_projects() -> List[Tuple[int, str]]:
    conn = db()
    rows = conn.execute("SELECT id, name FROM projects ORDER BY id DESC;").fetchall()
    conn.close()
    return rows

def get_project_meta(project_id: int) -> Dict[str, Any]:
    conn = db()
    row = conn.execute("SELECT meta_json FROM projects WHERE id=?", (project_id,)).fetchone()
    conn.close()
    if not row:
        return {}
    try:
        return json.loads(row[0] or "{}")
    except Exception:
        return {}

def create_project(name: str, meta: Dict[str, Any]) -> int:
    with _DB_LOCK:
        conn = db()
        ts = now_ts()
        cur = conn.execute(
            "INSERT INTO projects(name, meta_json, created_at) VALUES(?,?,?)",
            (name, json.dumps(meta, ensure_ascii=False), ts),
        )
        conn.commit()
        pid = cur.lastrowid
        conn.close()
        return pid

def list_artifacts(project_id: int) -> List[Dict[str, Any]]:
    conn = db()
    try:
        rows = conn.execute(
            "SELECT id, type, title, hash, updated_at "
            "FROM artifacts WHERE project_id=? ORDER BY updated_at DESC",
            (project_id,),
        ).fetchall()
    except sqlite3.OperationalError:
        conn.close()
        ensure_db_schema()
        conn = db()
        rows = conn.execute(
            "SELECT id, type, title, hash, updated_at "
            "FROM artifacts WHERE project_id=? ORDER BY updated_at DESC",
            (project_id,),
        ).fetchall()
    conn.close()
    return [{"id": r[0], "type": r[1], "title": r[2], "hash": r[3], "updated_at": r[4]} for r in rows]

def get_artifact(project_id: int, a_type: str) -> Optional[Dict[str, Any]]:
    conn = db()
    row = conn.execute(
        "SELECT id, title, content_md, content_json, hash, created_at, updated_at "
        "FROM artifacts WHERE project_id=? AND type=? ORDER BY updated_at DESC LIMIT 1",
        (project_id, a_type),
    ).fetchone()
    conn.close()
    if not row:
        return None
    return {
        "id": row[0],
        "type": a_type,
        "title": row[1],
        "content_md": row[2],
        "content_json": json.loads(row[3] or "{}"),
        "hash": row[4],
        "created_at": row[5],
        "updated_at": row[6],
    }

def get_versions(artifact_id: int) -> List[Dict[str, Any]]:
    conn = db()
    rows = conn.execute(
        "SELECT version_no, hash, created_at, note FROM versions WHERE artifact_id=? ORDER BY version_no DESC",
        (artifact_id,),
    ).fetchall()
    conn.close()
    return [{"version_no": r[0], "hash": r[1], "created_at": r[2], "note": r[3]} for r in rows]

def set_edges(project_id: int, child_id: int, parent_ids: List[int]):
    with _DB_LOCK:
        conn = db()
        conn.execute("DELETE FROM edges WHERE project_id=? AND child_artifact_id=?", (project_id, child_id))
        ts = now_ts()
        for pid in parent_ids:
            conn.execute(
                "INSERT INTO edges(project_id, child_artifact_id, parent_artifact_id, created_at) VALUES(?,?,?,?)",
                (project_id, child_id, pid, ts),
            )
        conn.commit()
        conn.close()

def upsert_artifact(
    project_id: int,
    a_type: str,
    title: str,
    content_md: str,
    content_json: Dict[str, Any],
    parent_ids: List[int],
    note: str = "",
) -> Dict[str, Any]:
    existing = get_artifact(project_id, a_type)
    
    parent_hashes: List[str] = []
    for pid in parent_ids:
        conn = db()
        row = conn.execute("SELECT hash FROM artifacts WHERE id=? AND project_id=?", (pid, project_id)).fetchone()
        conn.close()
        if row:
            parent_hashes.append(row[0])
    
    new_hash = compute_hash(content_md, content_json, parent_hashes)
    ts = now_ts()
    
    with _DB_LOCK:
        conn = db()
        if existing:
            cur_ver = conn.execute("SELECT MAX(version_no) FROM versions WHERE artifact_id=?", (existing["id"],)).fetchone()
            next_ver = (cur_ver[0] or 0) + 1
            conn.execute(
                "INSERT INTO versions(artifact_id, version_no, content_md, content_json, hash, created_at, note) "
                "VALUES(?,?,?,?,?,?,?)",
                (
                    existing["id"],
                    next_ver,
                    existing["content_md"],
                    json.dumps(existing["content_json"], ensure_ascii=False),
                    existing["hash"],
                    ts,
                    note or "auto-save",
                ),
            )
            conn.execute(
                "UPDATE artifacts SET title=?, content_md=?, content_json=?, hash=?, updated_at=? "
                "WHERE id=? AND project_id=?",
                (title, content_md, json.dumps(content_json, ensure_ascii=False), new_hash, ts, existing["id"], project_id),
            )
            conn.commit()
        else:
            conn.execute(
                "INSERT INTO artifacts(project_id, type, title, content_md, content_json, hash, created_at, updated_at) "
                "VALUES(?,?,?,?,?,?,?,?)",
                (project_id, a_type, title, content_md, json.dumps(content_json, ensure_ascii=False), new_hash, ts, ts),
            )
            conn.commit()
        conn.close()
    
    a = get_artifact(project_id, a_type)
    if a:
        set_edges(project_id, a["id"], parent_ids)
    return a

# ---------------------------
# æ–‡æ¡£é“¾ & ä¾èµ–è§„åˆ™
# ---------------------------
DOC_TYPES = [
    ("overview", "é¦–é¡µæ€»è§ˆ"),
    ("training_plan", "åŸ¹å…»æ–¹æ¡ˆï¼ˆåº•åº§ï¼‰"),
    ("syllabus", "è¯¾ç¨‹æ•™å­¦å¤§çº²ï¼ˆä¾èµ–åŸ¹å…»æ–¹æ¡ˆï¼‰"),
    ("calendar", "æ•™å­¦æ—¥å†ï¼ˆä¾èµ–å¤§çº²ï¼‰"),
    ("lesson_plan", "æ•™æ¡ˆï¼ˆä¾èµ–æ—¥å†ï¼‰"),
    ("assessment", "ä½œä¸š/é¢˜åº“/è¯•å·æ–¹æ¡ˆï¼ˆä¾èµ–å¤§çº²ï¼‰"),
    ("review", "å®¡æ ¸è¡¨ï¼ˆä¾èµ–è¯•å·æ–¹æ¡ˆ/å¤§çº²ï¼‰"),
    ("report", "è¯¾ç¨‹ç›®æ ‡è¾¾æˆæŠ¥å‘Šï¼ˆä¾èµ–å¤§çº²/æˆç»©ï¼‰"),
    ("manual", "æˆè¯¾æ‰‹å†Œï¼ˆä¾èµ–æ•™æ¡ˆ/è¿‡ç¨‹è¯æ®ï¼‰"),
    ("evidence", "è¯¾å ‚çŠ¶æ€ä¸è¿‡ç¨‹è¯æ®ï¼ˆå¯é€‰ï¼‰"),
    ("vge", "è¯æ®é“¾ä¸å¯éªŒè¯ç”Ÿæˆï¼ˆVGEï¼‰"),
    ("dep_graph", "ä¾èµ–å›¾å¯è§†åŒ–ï¼ˆæ ‘/Graphvizï¼‰"),
    ("docx_export", "æ¨¡æ¿åŒ–DOCXå¯¼å‡ºï¼ˆå­—æ®µæ˜ å°„å¡«å……ï¼‰"),
]

DEP_RULES = {
    "training_plan": [],
    "syllabus": ["training_plan"],
    "calendar": ["syllabus"],
    "lesson_plan": ["calendar"],
    "assessment": ["syllabus"],
    "review": ["assessment", "syllabus"],
    "report": ["syllabus"],
    "manual": ["lesson_plan"],
    "evidence": [],
    "vge": [],
    "overview": [],
    "dep_graph": [],
    "docx_export": [],
}

# ---------------------------
# é€šç”¨å·¥å…·å‡½æ•°
# ---------------------------
def type_label(a_type: str) -> str:
    for t, name in DOC_TYPES:
        if t == a_type:
            return name
    return a_type

def dep_status(project_id: int, a_type: str) -> Tuple[bool, List[Tuple[str, bool]]]:
    req = DEP_RULES.get(a_type, [])
    detail = []
    ok = True
    for r in req:
        exists = get_artifact(project_id, r) is not None
        detail.append((r, exists))
        ok = ok and exists
    return ok, detail

def render_depbar(project_id: int, a_type: str):
    ok, detail = dep_status(project_id, a_type)
    chips = []
    for r, exists in detail:
        cls = "ok" if exists else "bad"
        chips.append(f'<span class="badge {cls}">{type_label(r)}</span>')
    st.markdown(
        f"""
<div class="depbar">
    <div class="depitem"><b>ä¾èµ–æ£€æŸ¥</b>ï¼š{"âœ…é½å…¨" if ok else "âš ï¸ç¼ºå¤±ä¸Šæ¸¸"}</div>
    <div class="depitem">{''.join(chips) if chips else '<span class="badge ok">æ— ä¸Šæ¸¸ä¾èµ–</span>'}</div>
</div>
""",
        unsafe_allow_html=True,
    )

def artifact_toolbar(a: Dict[str, Any]):
    import html as _html
    st.markdown(
        f"""
<div class="card">
    <div style="display:flex; justify-content:space-between; gap:12px; align-items:center;">
        <div>
            <div style="font-size:18px; font-weight:800;">{_html.escape(a['title'])}</div>
            <div style="opacity:.75; font-size:12px; margin-top:4px;">
                ç±»å‹ï¼š{type_label(a['type'])} ï½œ Hashï¼š<code>{a['hash'][:12]}</code> ï½œ æ›´æ–°æ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(a['updated_at']))}
            </div>
        </div>
        <div>
            <span class="badge ok">å¯ç¼–è¾‘</span>
            <span class="badge warn">å¯ç‰ˆæœ¬åŒ–</span>
            <span class="badge warn">ä¾èµ–å¯è¿½æº¯</span>
        </div>
    </div>
</div>
""",
        unsafe_allow_html=True,
    )

def md_textarea(label: str, value: str, height: int = 420, key: str = "") -> str:
    return st.text_area(label, value=value, height=height, key=key)

# ---------------------------
# æ¨¡æ¿å‡½æ•°
# ---------------------------
def template_training_plan(major: str, grade: str, course_group: str) -> str:
    return f"""# {grade}çº§ã€Š{major}ã€‹åŸ¹å…»æ–¹æ¡ˆï¼ˆç¤ºä¾‹ï¼‰

## ä¸€ã€åŸ¹å…»ç›®æ ‡
- é¢å‘å·¥ç¨‹å®è·µï¼Œå…·å¤‡æ‰å®çš„æ•°å­¦/åŠ›å­¦/ææ–™åŸºç¡€
- å…·å¤‡ææ–™æˆå‹ä¸åˆ¶é€ è¿‡ç¨‹çš„åˆ†æã€è®¾è®¡ä¸ä¼˜åŒ–èƒ½åŠ›
- å…·å¤‡å·¥ç¨‹ä¼¦ç†ã€å›¢é˜Ÿåä½œä¸ç»ˆèº«å­¦ä¹ èƒ½åŠ›

## äºŒã€æ¯•ä¸šè¦æ±‚ï¼ˆç¤ºä¾‹ï¼‰
1. å·¥ç¨‹çŸ¥è¯†
2. é—®é¢˜åˆ†æ
3. è®¾è®¡/å¼€å‘è§£å†³æ–¹æ¡ˆ
4. ç ”ç©¶
5. ç°ä»£å·¥å…·ä½¿ç”¨
6. å·¥ç¨‹ä¸ç¤¾ä¼š
7. ç¯å¢ƒä¸å¯æŒç»­å‘å±•
8. èŒä¸šè§„èŒƒ
9. ä¸ªäººä¸å›¢é˜Ÿ
10. æ²Ÿé€š
11. é¡¹ç›®ç®¡ç†
12. ç»ˆèº«å­¦ä¹ 

## ä¸‰ã€è¯¾ç¨‹ä½“ç³»ï¼š{course_group}
- é€šè¯†ä¸åŸºç¡€
- ä¸“ä¸šæ ¸å¿ƒ
- ä¸“ä¸šæ–¹å‘
- å®è·µç¯èŠ‚
"""

# ---------------------------
# é¡¶éƒ¨ä¸ä¾§è¾¹æ 
# ---------------------------
def topbar():
    st.markdown(
        """
<div class="topbar">
    <div class="title">æ•™å­¦æ™ºèƒ½ä½“å¹³å° - PDFå…¨é‡æŠ½å–ç‰ˆ</div>
    <div class="sub">åŸ¹å…»æ–¹æ¡ˆPDFå…¨é‡æŠ½å–ï¼ˆæ–‡æœ¬+è¡¨æ ¼+ç»“æ„ï¼‰â†’ å¤§çº² â†’ æ—¥å† â†’ æ•™æ¡ˆ â†’ è¯•å·/å®¡æ ¸ â†’ è¾¾æˆæŠ¥å‘Š â†’ æˆè¯¾æ‰‹å†Œ</div>
</div>
""",
        unsafe_allow_html=True,
    )

# åˆå§‹åŒ–DB
ensure_db_schema()
topbar()

# ä¾§è¾¹æ é…ç½®
st.sidebar.markdown("## è¿è¡Œæ¨¡å¼")
run_mode = st.sidebar.radio("è¿è¡Œæ¨¡å¼", ["æ¼”ç¤ºæ¨¡å¼ï¼ˆæ— APIï¼‰", "åœ¨çº¿æ¨¡å¼ï¼ˆåƒé—®APIï¼‰"], index=0)
st.sidebar.caption("æ¼”ç¤ºæ¨¡å¼ä¸éœ€è¦ Keyï¼›åœ¨çº¿æ¨¡å¼è¯·åœ¨ Secrets ä¸­é…ç½® QWEN_API_KEYã€‚")

st.sidebar.markdown("## é¡¹ç›®ï¼ˆä¸“ä¸š/å¹´çº§/è¯¾ç¨‹ä½“ç³»ï¼‰")
projects = get_projects()
p_names = ["ï¼ˆæ–°å»ºé¡¹ç›®ï¼‰"] + [f"{pid} Â· {name}" for pid, name in projects]
p_sel = st.sidebar.selectbox("é€‰æ‹©é¡¹ç›®", p_names, index=0)

if p_sel == "ï¼ˆæ–°å»ºé¡¹ç›®ï¼‰":
    with st.sidebar.expander("åˆ›å»ºæ–°é¡¹ç›®", expanded=True):
        pname = st.text_input("é¡¹ç›®åç§°", value="ææ–™æˆå‹-æ•™è¯„ä¸€ä½“åŒ–ç¤ºä¾‹", key="new_pname")
        major = st.text_input("ä¸“ä¸š", value="ææ–™æˆå‹åŠæ§åˆ¶å·¥ç¨‹", key="new_major")
        grade = st.text_input("å¹´çº§", value="22", key="new_grade")
        course_group = st.text_input("è¯¾ç¨‹ä½“ç³»/æ–¹å‘", value="ææ–™æˆå‹-æ•°å€¼æ¨¡æ‹Ÿæ–¹å‘", key="new_group")
        if st.button("åˆ›å»ºé¡¹ç›®", type="primary"):
            pid = create_project(pname, {"major": major, "grade": grade, "course_group": course_group})
            st.success("å·²åˆ›å»ºé¡¹ç›®ï¼Œè¯·åœ¨ä¸‹æ‹‰ä¸­é€‰æ‹©å®ƒã€‚")
            st.rerun()
    project_id = None
else:
    project_id = int(p_sel.split("Â·")[0].strip())

st.sidebar.markdown("## åŠŸèƒ½æ¨¡å—")
module = st.sidebar.radio("å¯¼èˆª", [name for _, name in DOC_TYPES], index=1)
type_by_name = {name: t for t, name in DOC_TYPES}
current_type = type_by_name[module]

# ---------------------------
# é¡µé¢è·¯ç”±
# ---------------------------
def ensure_project():
    if project_id is None:
        st.info("è¯·å…ˆåœ¨å·¦ä¾§åˆ›å»ºå¹¶é€‰æ‹©ä¸€ä¸ªé¡¹ç›®ã€‚")
        st.stop()

def pick_parents_for(project_id: int, a_type: str) -> List[int]:
    req = DEP_RULES.get(a_type, [])
    parent_ids: List[int] = []
    for r in req:
        pa = get_artifact(project_id, r)
        if pa:
            parent_ids.append(pa["id"])
    if a_type == "manual":
        ev = get_artifact(project_id, "evidence")
        if ev:
            parent_ids.append(ev["id"])
    return parent_ids

def page_overview():
    ensure_project()
    st.markdown("### é¦–é¡µæ€»è§ˆ")
    arts = list_artifacts(project_id)
    if not arts:
        st.info("å½“å‰é¡¹ç›®è¿˜æ²¡æœ‰ä»»ä½•æ–‡æ¡£ã€‚å»ºè®®å…ˆä»'åŸ¹å…»æ–¹æ¡ˆï¼ˆåº•åº§ï¼‰'å¼€å§‹ã€‚")
        return
    
    st.markdown('<div class="card">ğŸ“Œ å½“å‰é¡¹ç›®å·²æœ‰æ–‡æ¡£ï¼ˆæœ€è¿‘æ›´æ–°åœ¨å‰ï¼‰</div>', unsafe_allow_html=True)
    rows = []
    for a in arts:
        rows.append({
            "ç±»å‹": type_label(a["type"]),
            "æ ‡é¢˜": a["title"],
            "Hash(å‰12)": a["hash"][:12],
            "æ›´æ–°æ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(a["updated_at"])),
        })
    st.dataframe(rows, use_container_width=True)


# ----------------------------
# åŸºç¡€å·¥å…·
# ----------------------------
def sha256_bytes(data: bytes) -> str:
    h = hashlib.sha256()
    h.update(data)
    return h.hexdigest()

def clean_text(s: str) -> str:
    if s is None:
        return ""
    s = str(s)
    s = s.replace("\u00a0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()

def normalize_multiline(text: str) -> str:
    """ä¿ç•™æ¢è¡Œï¼ŒåšåŸºç¡€æ¸…ç†ï¼Œä¾¿äºæ­£åˆ™åˆ†æ®µã€‚"""
    if text is None:
        return ""
    text = str(text).replace("\r\n", "\n").replace("\r", "\n")
    lines = [clean_text(ln) for ln in text.split("\n")]
    out: List[str] = []
    blank = 0
    for ln in lines:
        if ln.strip() == "":
            blank += 1
            if blank <= 2:
                out.append("")
        else:
            blank = 0
            out.append(ln)
    return "\n".join(out).strip()

def make_unique_columns(cols: List[str]) -> List[str]:
    seen: Dict[str, int] = {}
    out: List[str] = []
    for c in cols:
        c0 = clean_text(c) or "col"
        if c0 not in seen:
            seen[c0] = 1
            out.append(c0)
        else:
            seen[c0] += 1
            out.append(f"{c0}_{seen[c0]}")
    return out

def postprocess_table_df(df: pd.DataFrame) -> pd.DataFrame:
    """è¡¨æ ¼åå¤„ç†ï¼šå»ç©ºç™½ã€å» NaNã€åˆå¹¶æ ¼é€ æˆçš„ç©ºç™½åšå‘ä¸‹å¡«å……ã€‚"""
    if df is None or df.empty:
        return df

    df = df.copy()
    df = df.replace({None: ""}).fillna("")
    for c in df.columns:
        df[c] = df[c].astype(str).map(lambda x: clean_text(x))

    # 1) åˆ é™¤å®Œå…¨ç©ºè¡Œ
    mask_all_empty = df.apply(lambda r: all((clean_text(x) == "" for x in r.values.tolist())), axis=1)
    df = df.loc[~mask_all_empty].reset_index(drop=True)

    # 2) å‘ä¸‹å¡«å……ï¼ˆåˆå¹¶æ ¼å¸¸è§åˆ—ï¼‰
    fill_down_keywords = ["è¯¾ç¨‹ä½“ç³»", "è¯¾ç¨‹æ¨¡å—", "è¯¾ç¨‹æ€§è´¨", "è¯¾ç¨‹ç±»åˆ«", "ç±»åˆ«", "æ¨¡å—", "ç¯èŠ‚", "å­¦æœŸ", "æ–¹å‘"]
    for c in df.columns:
        if any(k in str(c) for k in fill_down_keywords):
            last = ""
            new_col = []
            for v in df[c].tolist():
                if v != "":
                    last = v
                    new_col.append(v)
                else:
                    new_col.append(last)
            df[c] = new_col

    return df

def normalize_table(raw_table: List[List[Any]]) -> List[List[str]]:
    """
    pdfplumber.extract_tables() è¿”å› list[list[str|None]]
    è¿™é‡ŒåšåŸºç¡€æ¸…æ´—ï¼šå»ç©ºè¡Œã€è¡¥é½åˆ—æ•°ã€å»æ‰å…¨ç©ºåˆ—
    """
    if not raw_table:
        return []

    rows = []
    max_cols = 0
    for r in raw_table:
        if r is None:
            continue
        rr = [clean_text(c) for c in r]
        # è·³è¿‡å…¨ç©ºè¡Œ
        if all(c == "" for c in rr):
            continue
        rows.append(rr)
        max_cols = max(max_cols, len(rr))

    if not rows or max_cols == 0:
        return []

    # è¡¥é½åˆ—æ•°
    for i in range(len(rows)):
        if len(rows[i]) < max_cols:
            rows[i] = rows[i] + [""] * (max_cols - len(rows[i]))

    # å»æ‰å…¨ç©ºåˆ—
    keep_cols = []
    for j in range(max_cols):
        col = [rows[i][j] for i in range(len(rows))]
        if any(c != "" for c in col):
            keep_cols.append(j)

    if not keep_cols:
        return []

    cleaned = [[row[j] for j in keep_cols] for row in rows]
    return cleaned

def table_to_df(cleaned_table: List[List[str]]) -> pd.DataFrame:
    """
    å°è¯•æŠŠç¬¬ä¸€è¡Œå½“è¡¨å¤´ï¼›å¦‚æœè¡¨å¤´å¤ªå·®å°±ç”¨é»˜è®¤åˆ—åã€‚
    """
    if not cleaned_table or len(cleaned_table) == 0:
        return pd.DataFrame()
    
    if len(cleaned_table) == 1:
        # åªæœ‰ä¸€è¡Œï¼Œåšå•è¡Œdf
        return pd.DataFrame([cleaned_table[0]])

    header = cleaned_table[0]
    body = cleaned_table[1:]

    # è¡¨å¤´åˆ¤å®šï¼šè‡³å°‘æœ‰ä¸€åŠå•å…ƒæ ¼éç©º
    non_empty = sum(1 for x in header if clean_text(x) != "")
    if non_empty >= max(1, len(header) // 2):
        cols = [h if h else f"col_{i+1}" for i, h in enumerate(header)]
        df = pd.DataFrame(body, columns=cols)
    else:
        # å¦åˆ™ä¸ç”¨è¡¨å¤´
        df = pd.DataFrame(cleaned_table)

    return postprocess_table_df(df)

# ----------------------------
# PDF æŠ½å–ï¼šæ–‡æœ¬ + è¡¨æ ¼ (ä½¿ç”¨ pdfplumber çš„è¡¨æ ¼æå–)
# ----------------------------
def extract_pages_text_and_tables(pdf_bytes: bytes, enable_ocr: bool = False) -> Tuple[List[Dict[str, Any]], str]:
    """
    æå–æ¯é¡µçš„æ–‡æœ¬å’Œè¡¨æ ¼
    è¿”å›ï¼šé¡µé¢æ•°æ®åˆ—è¡¨ï¼ˆå«æ–‡æœ¬å’Œè¡¨æ ¼ï¼‰ï¼Œå…¨æ–‡æ–‡æœ¬
    """
    if pdfplumber is None:
        return [], ""
    
    pages_data = []
    full_text_parts = []
    
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        # è¡¨æ ¼è®¾ç½®ï¼šå"å®½æ¾"ï¼Œæå‡è·¨é¡µ/å¤æ‚è¡¨æ ¼æå–æˆåŠŸç‡
        table_settings = {
            "vertical_strategy": "lines",
            "horizontal_strategy": "lines",
            "intersection_tolerance": 5,
            "snap_tolerance": 3,
            "join_tolerance": 3,
            "edge_min_length": 3,
            "min_words_vertical": 1,
            "min_words_horizontal": 1,
            "text_tolerance": 2,
        }
        
        for idx, page in enumerate(pdf.pages, start=1):
            # æå–æ–‡æœ¬
            text = page.extract_text() or ""
            text = normalize_multiline(text)
            
            # å¦‚æœéœ€è¦OCRä¸”æ–‡æœ¬å¤ªå°‘
            if enable_ocr and len(text) < 50:
                try:
                    import pytesseract
                    from PIL import Image
                    img = page.to_image(resolution=220).original
                    ocr_text = pytesseract.image_to_string(img, lang="chi_sim+eng")
                    if len(ocr_text) > len(text):
                        text = normalize_multiline(ocr_text)
                except Exception:
                    pass
            
            full_text_parts.append(text)
            
            # æå–è¡¨æ ¼
            raw_tables = []
            try:
                raw_tables = page.extract_tables(table_settings=table_settings) or []
            except Exception:
                raw_tables = []
            
            # æ¸…æ´—è¡¨æ ¼
            cleaned_tables = []
            for t in raw_tables:
                ct = normalize_table(t)
                if ct:
                    cleaned_tables.append(ct)
            
            pages_data.append({
                "page": idx,
                "text": text,
                "tables": cleaned_tables,
                "tables_count": len(cleaned_tables)
            })
    
    full_text = "\n".join(full_text_parts)
    return pages_data, full_text

# ----------------------------
# ç»“æ„åŒ–è§£æï¼šç« èŠ‚/æ¯•ä¸šè¦æ±‚/åŸ¹å…»ç›®æ ‡/é™„è¡¨æ ‡é¢˜
# ----------------------------
def split_sections(full_text: str) -> Dict[str, str]:
    """
    æŒ‰ "ä¸€ã€/äºŒã€/ä¸‰ã€..." å¤§ç« åˆ‡åˆ†ã€‚
    å…¼å®¹ï¼šä¸‰ã€ / ä¸‰. / ä¸‰ï¼
    """
    text = normalize_multiline(full_text)
    lines = text.splitlines()
    pat = re.compile(r"^\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s*[ã€\.ï¼]\s*([^\n\r]+?)\s*$")

    sections: Dict[str, List[str]] = {}
    cur_key = "å°é¢/å‰è¨€"

    for ln in lines:
        m = pat.match(ln)
        if m:
            num = m.group(1)
            title = clean_text(m.group(2))
            cur_key = f"{num}ã€{title}"
            sections.setdefault(cur_key, [])
        else:
            sections.setdefault(cur_key, []).append(ln)

    return {k: "\n".join(v).strip() for k, v in sections.items()}

def extract_appendix_titles(full_text: str) -> Dict[str, str]:
    """æŠ½å–"é™„è¡¨X -> æ ‡é¢˜ï¼ˆå¯èƒ½å«ä¸ƒã€å…«â€¦ï¼‰"""
    titles: Dict[str, str] = {}
    text = normalize_multiline(full_text)
    for raw in text.splitlines():
        line = raw.strip()
        if not line:
            continue

        # 1) é™„è¡¨1ï¼šXXXX
        m = re.search(r"(é™„è¡¨\s*\d+)\s*[:ï¼š]\s*(.+)$", line)
        if m:
            key = re.sub(r"\s+", "", m.group(1))
            val = clean_text(m.group(2))
            if val:
                titles[key] = val
            continue

        # 2) ä¸ƒã€XXXXï¼ˆé™„è¡¨1ï¼‰
        m = re.search(r"^(?P<title>.+?)\s*[ï¼ˆ(]\s*(?P<key>é™„è¡¨\s*\d+)\s*[)ï¼‰]\s*$", line)
        if m:
            key = re.sub(r"\s+", "", m.group("key"))
            val = clean_text(m.group("title"))
            if val:
                titles[key] = val
            continue

        # 3) è¡Œå†…å‡ºç°ï¼ˆé™„è¡¨Xï¼‰
        m = re.search(r"(?P<title>.+?)\s*[ï¼ˆ(]\s*(?P<key>é™„è¡¨\s*\d+)\s*[)ï¼‰]", line)
        if m:
            key = re.sub(r"\s+", "", m.group("key"))
            val = clean_text(m.group("title"))
            if val and key not in titles:
                titles[key] = val

    return titles

def parse_training_objectives(section_text: str) -> Dict[str, Any]:
    """
    æå–"åŸ¹å…»ç›®æ ‡"æ¡ç›®ã€‚è¿”å› items(list[str]) + rawã€‚
    å°½é‡åŒ…å®¹ï¼š1) / 1ï¼ / 1ã€ / ï¼ˆ1ï¼‰ç­‰ã€‚
    """
    raw = normalize_multiline(section_text)
    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
    items: List[str] = []

    pat = re.compile(r"^(?:ï¼ˆ?\s*\d+\s*ï¼‰?|\d+\s*[\.ã€ï¼])\s*(.+)$")
    for ln in lines:
        m = pat.match(ln)
        if m:
            body = clean_text(m.group(1))
            if body:
                items.append(body)

    # å¦‚æœæ²¡æŠ“åˆ°ç¼–å·æ¡ç›®ï¼Œé€€åŒ–ï¼šå–å‰è‹¥å¹²è¡Œï¼ˆä¸ä¸¢ä¿¡æ¯ï¼‰
    if not items:
        items = lines[:30]

    return {"count": len(items), "items": items, "raw": raw}

def parse_graduation_requirements(text_any: str) -> Dict[str, Any]:
    """
    æŠ½å– 12 æ¡æ¯•ä¸šè¦æ±‚åŠå…¶åˆ†é¡¹ 1.1/1.2â€¦
    è¿”å›ç»“æ„ï¼š{"count":..,"items":[{"no":1,"title":"å·¥ç¨‹çŸ¥è¯†","body":"...","subitems":[...]}], "raw":...}
    """
    text = normalize_multiline(text_any or "")

    # å®šä½"äºŒã€æ¯•ä¸šè¦æ±‚"
    start = re.search(r"(?m)^\s*(äºŒ\s*[ã€\.ï¼]?\s*æ¯•ä¸šè¦æ±‚|æ¯•ä¸šè¦æ±‚)\s*$", text)
    if start:
        tail = text[start.start():]
    else:
        tail = text

    # æˆªæ–­åˆ°ä¸‹ä¸€å¤§ç« 
    end = re.search(r"(?m)^\s*[ä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*[ã€\.ï¼]", tail)
    if end:
        tail = tail[:end.start()]

    lines = [ln.strip() for ln in tail.splitlines()]

    main_pat = re.compile(r"^(?P<no>\d{1,2})\s*[\.ã€](?!\d)\s*(?P<body>.+)$")   # 1. xxx (æ’é™¤ 1.1)
    sub_pat = re.compile(r"^(?P<no>\d{1,2}\.\d{1,2})\s+(?P<body>.+)$")       # 1.1 xxx

    items: List[Dict[str, Any]] = []
    cur: Optional[Dict[str, Any]] = None
    cur_sub: Optional[Dict[str, Any]] = None

    def flush_sub():
        nonlocal cur_sub, cur
        if cur is not None and cur_sub is not None:
            cur.setdefault("subitems", []).append(cur_sub)
        cur_sub = None

    def flush_item():
        nonlocal cur
        if cur is not None:
            cur["title"] = clean_text(cur.get("title", ""))
            cur["body"] = clean_text(cur.get("body", ""))
            for s in cur.get("subitems", []):
                s["body"] = clean_text(s.get("body", ""))
            items.append(cur)
        cur = None

    for ln in lines:
        if not ln:
            continue

        m_main = main_pat.match(ln)
        m_sub = sub_pat.match(ln)

        if m_main:
            flush_sub()
            flush_item()
            no = int(m_main.group("no"))
            body_full = clean_text(m_main.group("body"))

            # å¤„ç†"å·¥ç¨‹çŸ¥è¯†ï¼š..."è¿™ç§
            title = ""
            body = body_full
            if "ï¼š" in body_full:
                title, body = body_full.split("ï¼š", 1)
                title = clean_text(title)
                body = clean_text(body)

            cur = {"no": no, "title": title, "body": body, "subitems": []}
            continue

        if m_sub and cur is not None:
            flush_sub()
            cur_sub = {"no": m_sub.group("no"), "body": clean_text(m_sub.group("body"))}
            continue

        # ç»­è¡Œ
        if cur_sub is not None:
            cur_sub["body"] += " " + ln
        elif cur is not None:
            cur["body"] += " " + ln

    flush_sub()
    flush_item()

    items = sorted(items, key=lambda x: x.get("no", 999))
    if len(items) > 12:
        items = [x for x in items if 1 <= x.get("no", 0) <= 12]

    return {"count": len(items), "items": items, "raw": tail.strip()}

# ----------------------------
# è¡¨æ ¼æ ‡é¢˜/æ–¹å‘è¯†åˆ«
# ----------------------------
def guess_table_appendix_by_page(page_no: int) -> Optional[str]:
    """
    é’ˆå¯¹å¸¸è§åŸ¹å…»æ–¹æ¡ˆï¼ˆæœ¬æ ·ä¾‹ 18 é¡µï¼‰ï¼š
    10-11 é™„è¡¨1ï¼Œ12 é™„è¡¨2ï¼Œ13-14 é™„è¡¨3ï¼Œ15 é™„è¡¨4ï¼Œ16 é™„è¡¨5
    å¦‚æœæ¢ä¸åŒæ¨¡æ¿ï¼Œè¯·è‡ªè¡Œè°ƒæ•´æˆ–æ”¹ä¸ºæ›´æ™ºèƒ½çš„é¡µå†…æ£€æµ‹ã€‚
    """
    mapping = {
        10: "é™„è¡¨1", 11: "é™„è¡¨1",
        12: "é™„è¡¨2",
        13: "é™„è¡¨3", 14: "é™„è¡¨3",
        15: "é™„è¡¨4",
        16: "é™„è¡¨5",
    }
    return mapping.get(page_no)

def infer_table_title_from_page_text(page_text: str, appendix: Optional[str], appendix_titles: Dict[str, str], page_no: int) -> str:
    if appendix and appendix in appendix_titles:
        return appendix_titles[appendix]

    if appendix:
        m = re.search(rf"(?P<title>[^\n\r]{{2,120}}?)\s*[ï¼ˆ(]\s*{re.escape(appendix)}\s*[)ï¼‰]", page_text)
        if m:
            return clean_text(m.group("title"))

    m = re.search(r"(é™„è¡¨\s*\d+)\s*[:ï¼š]\s*([^\n\r]{2,120})", page_text)
    if m:
        return clean_text(m.group(2))

    return appendix or f"ç¬¬{page_no}é¡µè¡¨æ ¼"

def infer_direction_for_page(page_text: str) -> str:
    has_weld = "ç„Šæ¥" in page_text
    has_ndt = ("æ— æŸ" in page_text) or ("æ— æŸæ£€æµ‹" in page_text)
    if has_weld and has_ndt:
        return "æ··åˆï¼ˆç„Šæ¥+æ— æŸæ£€æµ‹ï¼‰"
    if has_weld:
        return "ç„Šæ¥"
    if has_ndt:
        return "æ— æŸæ£€æµ‹"
    return ""

def add_direction_column_rowwise(df: pd.DataFrame, page_direction: str) -> pd.DataFrame:
    """
    è¡Œçº§æ–¹å‘è¯†åˆ«ï¼šè‹¥è¡¨å†…æœ‰"ç„Šæ¥æ–¹å‘/æ— æŸæ£€æµ‹æ–¹å‘"åˆ†éš”è¡Œï¼Œåˆ™ä»è¯¥è¡Œå¼€å§‹å‘ä¸‹æ ‡æ³¨ã€‚
    è‹¥è¯†åˆ«ä¸åˆ°ï¼Œåˆ™ä½¿ç”¨ page_directionã€‚
    """
    if df is None or df.empty:
        return df

    df = df.copy()
    cur_dir = ""
    dirs = []
    for _, row in df.iterrows():
        row_txt = " ".join([clean_text(x) for x in row.values.tolist()])
        if re.search(r"ç„Šæ¥.*æ–¹å‘", row_txt):
            cur_dir = "ç„Šæ¥"
        elif re.search(r"æ— æŸ.*æ–¹å‘", row_txt) or re.search(r"æ— æŸæ£€æµ‹.*æ–¹å‘", row_txt):
            cur_dir = "æ— æŸæ£€æµ‹"

        dirs.append(cur_dir or page_direction)

    # æ’åˆ°æœ€å‰
    if "ä¸“ä¸šæ–¹å‘" not in df.columns:
        df.insert(0, "ä¸“ä¸šæ–¹å‘", dirs)
    else:
        df["ä¸“ä¸šæ–¹å‘"] = [d or page_direction for d in dirs]

    return df

# ----------------------------
# è¾“å‡ºç»“æ„
# ----------------------------
@dataclass
class TablePack:
    page: int
    title: str
    appendix: str
    direction: str
    columns: List[str]
    rows: List[List[Any]]

@dataclass
class ExtractResult:
    page_count: int
    table_count: int
    ocr_used: bool
    file_sha256: str
    extracted_at: str
    pages_data: List[Dict[str, Any]]
    sections: Dict[str, str]
    appendix_titles: Dict[str, str]
    training_objectives: Dict[str, Any]
    graduation_requirements: Dict[str, Any]
    tables: List[Dict[str, Any]]  # TablePack as dict

# ----------------------------
# ä¸»æµç¨‹
# ----------------------------
def run_full_extract(pdf_bytes: bytes, use_ocr: bool = False) -> ExtractResult:
    # 1) æå–é¡µé¢æ–‡æœ¬å’Œè¡¨æ ¼
    pages_data, full_text = extract_pages_text_and_tables(pdf_bytes, enable_ocr=use_ocr)
    
    # 2) ç»“æ„åŒ–è§£æ
    sections = split_sections(full_text)
    appendix_titles = extract_appendix_titles(full_text)
    
    # 3) å…³é”®ç»“æ„åŒ–ï¼šåŸ¹å…»ç›®æ ‡ã€æ¯•ä¸šè¦æ±‚
    obj_key = next((k for k in sections.keys() if "åŸ¹å…»ç›®æ ‡" in k), "")
    obj = parse_training_objectives(sections.get(obj_key, "") or full_text)
    grad = parse_graduation_requirements(full_text)
    
    # 4) å¤„ç†è¡¨æ ¼
    tables: List[TablePack] = []
    total_tables = 0
    
    for page_data in pages_data:
        page_no = page_data["page"]
        page_text = page_data["text"]
        page_tables = page_data["tables"]
        
        total_tables += len(page_tables)
        
        appendix = guess_table_appendix_by_page(page_no) or ""
        base_title = infer_table_title_from_page_text(page_text, appendix or None, appendix_titles, page_no)
        title = f"{base_title}ï¼ˆ{appendix}ï¼‰" if appendix and appendix not in base_title else base_title
        page_dir = infer_direction_for_page(page_text)
        
        for i, table_data in enumerate(page_tables):
            df = table_to_df(table_data)
            if df is not None and not df.empty:
                df2 = add_direction_column_rowwise(df, page_dir)
                sub_title = title if len(page_tables) == 1 else f"{title} - è¡¨{i+1}"
                pack = TablePack(
                    page=page_no,
                    title=sub_title,
                    appendix=appendix,
                    direction=page_dir,
                    columns=[str(c) for c in df2.columns],
                    rows=df2.values.tolist(),
                )
                tables.append(pack)
    
    result = ExtractResult(
        page_count=len(pages_data),
        table_count=total_tables,
        ocr_used=use_ocr,
        file_sha256=sha256_bytes(pdf_bytes),
        extracted_at=datetime.now().isoformat(timespec="seconds"),
        pages_data=pages_data,
        sections=sections,
        appendix_titles=appendix_titles,
        training_objectives=obj,
        graduation_requirements=grad,
        tables=[asdict(t) for t in tables],
    )
    return result

# ----------------------------
# å¯¼å‡ºåŠŸèƒ½
# ----------------------------
def safe_df_from_tablepack(t: Dict[str, Any]) -> pd.DataFrame:
    """ä» TablePack å­—å…¸åˆ›å»º DataFrame"""
    cols = t.get("columns") or []
    rows = t.get("rows") or []
    
    if rows and len(rows) > 0:
        df = pd.DataFrame(rows, columns=cols)
        return postprocess_table_df(df)
    return pd.DataFrame()

def make_tables_zip(tables: List[Dict[str, Any]]) -> bytes:
    """CSV + tables.json æ‰“åŒ…"""
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        zf.writestr("tables.json", json.dumps(tables, ensure_ascii=False, indent=2))
        for idx, t in enumerate(tables, start=1):
            title = clean_text(t.get("title") or f"table_{idx}")
            title_safe = re.sub(r"[^0-9A-Za-z\u4e00-\u9fff_\-]+", "_", title)[:80].strip("_") or f"table_{idx}"

            df = safe_df_from_tablepack(t)

            # æ–¹å‘åˆ—
            direction = clean_text(t.get("direction") or "")
            if direction and "ä¸“ä¸šæ–¹å‘" not in df.columns:
                df.insert(0, "ä¸“ä¸šæ–¹å‘", direction)

            csv_bytes = df.to_csv(index=False, encoding="utf-8-sig")
            zf.writestr(f"{idx:02d}_{title_safe}.csv", csv_bytes)
    return buf.getvalue()

def build_json_bytes(result: ExtractResult) -> bytes:
    """æ„å»º JSON å¯¼å‡ºæ–‡ä»¶"""
    return json.dumps(asdict(result), ensure_ascii=False, indent=2).encode("utf-8")


def page_training_plan():
    ensure_project()
    render_depbar(project_id, "training_plan")
    tp = get_artifact(project_id, "training_plan")
    
    st.markdown("### åŸ¹å…»æ–¹æ¡ˆåº•åº§ï¼ˆtraining_planï¼‰")
    st.caption("åŸ¹å…»æ–¹æ¡ˆæ˜¯æ•™å­¦æ–‡æ¡£é“¾çš„èµ·ç‚¹ï¼Œéœ€ä¸Šä¼ PDFå¹¶ç¡®è®¤æå–ç»“æœï¼Œæˆ–ç›´æ¥ç¼–è¾‘ã€‚")
    
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["PDFå…¨é‡æŠ½å–ç‹¬ç«‹ç•Œé¢ï¼ˆå¢å¼ºï¼‰", "PDFä¸Šä¼ /æŠ½å–/ç¡®è®¤", "æŸ¥çœ‹å½“å‰", "ç¼–è¾‘", "ç‰ˆæœ¬"])
    

    with tab1:
            st.markdown("### PDFå…¨é‡æŠ½å–ç‹¬ç«‹ç•Œé¢ï¼ˆå¢å¼ºç‰ˆï¼‰")
            st.caption("ç¡®ä¿æ˜¾ç¤ºæ‰€æœ‰ç« èŠ‚ï¼ˆå¦‚ä¸€åˆ°å…­ï¼‰å’Œæ‰€æœ‰é™„è¡¨ï¼ˆå¦‚é™„è¡¨1åˆ°5ï¼Œå¯¹åº”ä¸ƒåˆ°åä¸€ï¼‰")
            
            if "extract_result" not in st.session_state:
                st.session_state["extract_result"] = None
            
            uploaded = st.file_uploader("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDF", type=["pdf"], key="full_extract_upload")
            use_ocr = st.checkbox("å¯¹æ— æ–‡æœ¬é¡µå¯ç”¨ OCRï¼ˆå¯é€‰ï¼‰", value=True, key="full_extract_ocr")  # é»˜è®¤å¯ç”¨OCRä»¥ç¡®ä¿å®Œæ•´
            
            if uploaded and st.button("å¼€å§‹å…¨é‡æŠ½å–", type="primary", key="full_extract_btn"):
                pdf_bytes = uploaded.getvalue()
                with st.spinner("æ­£åœ¨æŠ½å–æ‰€æœ‰å†…å®¹â€¦"):
                    extract_result = run_full_extract(pdf_bytes, use_ocr=use_ocr)
                    st.session_state["extract_result"] = extract_result
            
            result = st.session_state.get("extract_result")
            if result is None:
                st.stop()
            
            # æ¦‚è§ˆæŒ‡æ ‡
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("æ€»é¡µæ•°", result.page_count)
            c2.metric("è¡¨æ ¼æ€»æ•°", result.table_count)
            c3.metric("OCRå¯ç”¨", "æ˜¯" if result.ocr_used else "å¦")
            c4.caption(f"SHA256: {result.file_sha256[:16]}...")

            tabs = st.tabs(["æ¦‚è§ˆä¸ä¸‹è½½", "ç« èŠ‚å¤§æ ‡é¢˜ï¼ˆå…¨éƒ¨ï¼‰", "åŸ¹å…»ç›®æ ‡", "æ¯•ä¸šè¦æ±‚ï¼ˆ12æ¡ï¼‰", "é™„è¡¨è¡¨æ ¼ï¼ˆå¯ä¸‹è½½CSVï¼‰", "åˆ†é¡µåŸæ–‡ä¸è¡¨æ ¼"])

            # ---- Tab 0 æ¦‚è§ˆä¸ä¸‹è½½
            with tabs[0]:
                st.markdown("### ç»“æ„åŒ–è¯†åˆ«ç»“æœï¼ˆå¯å…ˆåœ¨è¿™é‡Œæ ¡å¯¹ï¼‰")

                # ä¸‹è½½ JSONï¼ˆå…¨é‡ï¼‰
                json_bytes = build_json_bytes(result)
                st.download_button(
                    "ä¸‹è½½æŠ½å–ç»“æœ JSONï¼ˆå…¨é‡åŸºç¡€åº“ï¼‰",
                    data=json_bytes,
                    file_name="training_plan_full_extract.json",
                    mime="application/json",
                    use_container_width=True,
                )

                if result.tables:
                    zip_bytes = make_tables_zip(result.tables)
                    st.download_button(
                        "ä¸‹è½½è¡¨æ ¼ ZIPï¼ˆCSV + tables.jsonï¼‰",
                        data=zip_bytes,
                        file_name="training_plan_tables.zip",
                        mime="application/zip",
                        use_container_width=True,
                    )
                
                st.markdown("#### é™„è¡¨æ ‡é¢˜æ˜ å°„ï¼ˆç”¨äºç»™è¡¨æ ¼å‘½åï¼‰")
                if result.appendix_titles:
                    st.json(result.appendix_titles)
                else:
                    st.info("æœªåœ¨æ­£æ–‡ä¸­æ£€æµ‹åˆ°é™„è¡¨æ ‡é¢˜æ˜ å°„ï¼ˆä¸å½±å“è¡¨æ ¼æŠ½å–ï¼Œä½†è¡¨åå¯èƒ½ä¸å¤Ÿç²¾å‡†ï¼‰ã€‚")

            # ---- Tab 1 ç« èŠ‚å¤§æ ‡é¢˜
            with tabs[1]:
                st.markdown("### ç« èŠ‚å¤§æ ‡é¢˜ï¼ˆç”¨äºç¡®ä¿'ä¸‰~å…­'ç­‰å†…å®¹ä¸ä¸¢ï¼‰")
                st.caption("è¿™é‡Œå±•ç¤º split_sections æŠ½åˆ°çš„å…¨éƒ¨å¤§ç« æ ‡é¢˜ï¼Œç‚¹å‡»å¯å±•å¼€æŸ¥çœ‹æ­£æ–‡ï¼ˆç”¨äºæº¯æºå’Œæ ¡å¯¹ï¼‰ã€‚")
                for k in result.sections.keys():
                    with st.expander(k, expanded=False):
                        st.text(result.sections.get(k, ""))

            # ---- Tab 2 åŸ¹å…»ç›®æ ‡
            with tabs[2]:
                st.markdown("### 1ï¼‰åŸ¹å…»ç›®æ ‡ï¼ˆå¯ç¼–è¾‘/æ ¡å¯¹ï¼‰")
                st.caption("è‹¥åŸ¹å…»ç›®æ ‡æœ‰å¤šæ–¹å‘ç‰ˆæœ¬ï¼ˆç„Šæ¥/æ— æŸï¼‰ï¼Œåç»­å¯åœ¨æ­¤åŸºç¡€ä¸Šå¢å¼ºä¸ºåˆ†æ–¹å‘æŠ½å–ã€‚")

                obj = result.training_objectives
                st.write(f"è¯†åˆ«æ¡ç›®æ•°ï¼š**{obj.get('count', 0)}**")
                st.text_area("åŸ¹å…»ç›®æ ‡ï¼ˆé€æ¡ï¼‰", value="\n".join(obj.get("items", [])), height=220)
                with st.expander("åŸå§‹æ–‡æœ¬ï¼ˆåŸ¹å…»ç›®æ ‡æ®µï¼‰"):
                    st.text(obj.get("raw", ""))

            # ---- Tab 3 æ¯•ä¸šè¦æ±‚
            with tabs[3]:
                st.markdown("### 2ï¼‰æ¯•ä¸šè¦æ±‚ï¼ˆ12æ¡ + åˆ†é¡¹ï¼‰")
                grad = result.graduation_requirements
                st.write(f"è¯†åˆ«ä¸»æ¡ç›®æ•°ï¼š**{grad.get('count', 0)}**ï¼ˆç†æƒ³ä¸º 12ï¼‰")

                items = grad.get("items", [])
                if not items:
                    st.warning("æœªè¯†åˆ«åˆ°æ¯•ä¸šè¦æ±‚ï¼Œè¯·åœ¨'åˆ†é¡µåŸæ–‡'ä¸­ç¡®è®¤ PDF æ˜¯å¦å¯æå–æ–‡æœ¬ã€‚")
                else:
                    for it in items:
                        no = it.get("no")
                        title = it.get("title") or ""
                        body = it.get("body") or ""
                        header = f"{no}. {title}".strip()
                        with st.expander(header, expanded=(no in [1, 2])):
                            st.write(body)
                            subs = it.get("subitems", [])
                            if subs:
                                st.markdown("**åˆ†é¡¹ï¼š**")
                                for s in subs:
                                    st.write(f"- {s.get('no')}: {s.get('body')}")
                with st.expander("åŸå§‹æ–‡æœ¬ï¼ˆæ¯•ä¸šè¦æ±‚æ®µï¼‰"):
                    st.text(grad.get("raw", ""))

            # ---- Tab 4 è¡¨æ ¼
            with tabs[4]:
                st.markdown("### 3ï¼‰é™„è¡¨è¡¨æ ¼ï¼ˆè¡¨å + æ–¹å‘å°½é‡æ¸…æ™°ï¼‰")
                if not result.tables:
                    st.info("æœªæ£€æµ‹åˆ°è¡¨æ ¼ã€‚è¯·æ£€æŸ¥PDFæ˜¯å¦æœ‰è¡¨æ ¼ï¼Œæˆ–å°è¯•å¯ç”¨OCRã€‚")
                else:
                    # æ–¹å‘è¿‡æ»¤
                    all_dirs = sorted({clean_text(t.get("direction") or "") for t in result.tables if clean_text(t.get("direction") or "")})
                    opt_dirs = ["å…¨éƒ¨"] + all_dirs
                    sel = st.selectbox("æ–¹å‘è¿‡æ»¤", opt_dirs, index=0)

                    for t in result.tables:
                        direction = clean_text(t.get("direction") or "")
                        if sel != "å…¨éƒ¨" and direction != sel:
                            continue

                        st.subheader(f"ç¬¬{t.get('page')}é¡µï½œ{t.get('title')}")
                        if direction:
                            st.caption(f"é¡µé¢æ–¹å‘æç¤ºï¼š{direction}")

                        df = safe_df_from_tablepack(t)
                        st.dataframe(df, use_container_width=True, hide_index=True)

            # ---- Tab 5 åˆ†é¡µåŸæ–‡ä¸è¡¨æ ¼
            with tabs[5]:
                st.markdown("### 4ï¼‰åˆ†é¡µåŸæ–‡ä¸è¡¨æ ¼ï¼ˆç”¨äºæº¯æº/è°ƒè¯•æŠ½å–ç¼ºå¤±ï¼‰")
                
                for page_data in result.pages_data:
                    page_no = page_data["page"]
                    page_text = page_data["text"]
                    page_tables = page_data["tables"]
                    
                    with st.expander(f"ç¬¬{page_no}é¡µï¼ˆ{len(page_tables)}ä¸ªè¡¨æ ¼ï¼‰", expanded=False):
                        st.text(page_text)
                        
                        if page_tables:
                            st.markdown(f"**è¡¨æ ¼ ({len(page_tables)}ä¸ª):**")
                            for i, table_data in enumerate(page_tables, start=1):
                                df = table_to_df(table_data)
                                if not df.empty:
                                    st.markdown(f"**è¡¨æ ¼ {i}:**")
                                    st.dataframe(df, use_container_width=True, height=200)
                                else:
                                    st.info(f"è¡¨æ ¼ {i} ä¸ºç©ºæˆ–æ— æ³•è§£æ")

    with tab2:
            st.markdown("#### æ–¹å¼Aï¼šä¸€é”®ç”Ÿæˆï¼ˆæ¼”ç¤º/å¿«é€Ÿï¼‰")
            major = st.text_input("ä¸“ä¸š", value="ææ–™æˆå‹åŠæ§åˆ¶å·¥ç¨‹", key="tp_major")
            grade = st.text_input("å¹´çº§", value="22", key="tp_grade")
            group = st.text_input("è¯¾ç¨‹ä½“ç³»/æ–¹å‘", value="ææ–™æˆå‹-æ•°å€¼æ¨¡æ‹Ÿæ–¹å‘", key="tp_group")
            if st.button("ç”ŸæˆåŸ¹å…»æ–¹æ¡ˆå¹¶ä¿å­˜", type="primary"):
                md = template_training_plan(major, grade, group)
                a = upsert_artifact(
                    project_id,
                    "training_plan",
                    f"{grade}çº§ã€Š{major}ã€‹åŸ¹å…»æ–¹æ¡ˆ",
                    md,
                    {"major": major, "grade": grade, "course_group": group, "confirmed": True},
                    [],
                    note="generate",
                )
                st.success("å·²ä¿å­˜åŸ¹å…»æ–¹æ¡ˆï¼ˆå¯ä½œä¸ºåç»­æ–‡ä»¶ä¾èµ–åº•åº§ï¼‰")
                st.rerun()
        
    with tab3:
        if not tp:
            st.info("æš‚æ— åŸ¹å…»æ–¹æ¡ˆã€‚è¯·å…ˆç”Ÿæˆæˆ–ä¸Šä¼ å¹¶ç¡®è®¤ã€‚")
        else:
            artifact_toolbar(a)
            st.markdown("#### ç»“æ„åŒ–å†…å®¹")
            st.json(a.get("content_json") or {})
            st.markdown("#### Markdowné¢„è§ˆ")
            st.markdown(a["content_md"][:2000] + "..." if len(a["content_md"]) > 2000 else a["content_md"])
    
    with tab4:
        if not tp:
            st.info("æš‚æ— åŸ¹å…»æ–¹æ¡ˆã€‚è¯·å…ˆç”Ÿæˆæˆ–ä¸Šä¼ ã€‚")
        else:
            edited = md_textarea("åœ¨çº¿ç¼–è¾‘åŸ¹å…»æ–¹æ¡ˆï¼ˆæ”¯æŒç›´æ¥ä¿®æ”¹ï¼‰", a["content_md"], key="tp_edit")
            note = st.text_input("ä¿å­˜è¯´æ˜ï¼ˆå¯é€‰ï¼‰", value="edit", key="tp_note")
            if st.button("ä¿å­˜ä¿®æ”¹ï¼ˆç”Ÿæˆæ–°ç‰ˆæœ¬ï¼‰", type="primary", key="tp_save"):
                a2 = upsert_artifact(project_id, "training_plan", a["title"], edited, a["content_json"], [], note=note)
                st.success("å·²ä¿å­˜ã€‚åç»­ä¾èµ–æ–‡ä»¶å°†å¼•ç”¨æ›´æ–°åçš„åŸ¹å…»æ–¹æ¡ˆã€‚")
                st.rerun()
    
    with tab5:
        if not tp:
            st.info("æš‚æ— åŸ¹å…»æ–¹æ¡ˆã€‚")
        else:
            vers = get_versions(a["id"])
            st.markdown("#### ç‰ˆæœ¬è®°å½•")
            st.dataframe(vers if vers else [], use_container_width=True)


# å…¶ä»–é¡µé¢å‡½æ•°ï¼ˆç®€åŒ–å®ç°ï¼‰
def page_syllabus():
    ensure_project()
    render_depbar(project_id, "syllabus")
    tp = get_artifact(project_id, "training_plan")
    a = get_artifact(project_id, "syllabus")
    
    st.markdown("### è¯¾ç¨‹æ•™å­¦å¤§çº²")
    
    if not tp:
        st.warning("è¯·å…ˆåˆ›å»ºåŸ¹å…»æ–¹æ¡ˆ")
    else:
        st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_calendar():
    ensure_project()
    render_depbar(project_id, "calendar")
    st.markdown("### æ•™å­¦æ—¥å†")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_lesson_plan():
    ensure_project()
    render_depbar(project_id, "lesson_plan")
    st.markdown("### æ•™æ¡ˆ")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_assessment():
    ensure_project()
    render_depbar(project_id, "assessment")
    st.markdown("### ä½œä¸š/é¢˜åº“/è¯•å·æ–¹æ¡ˆ")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_review():
    ensure_project()
    render_depbar(project_id, "review")
    st.markdown("### å®¡æ ¸è¡¨")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_report():
    ensure_project()
    render_depbar(project_id, "report")
    st.markdown("### è¯¾ç¨‹ç›®æ ‡è¾¾æˆæŠ¥å‘Š")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_manual():
    ensure_project()
    render_depbar(project_id, "manual")
    st.markdown("### æˆè¯¾æ‰‹å†Œ")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_evidence():
    ensure_project()
    render_depbar(project_id, "evidence")
    st.markdown("### è¯¾å ‚çŠ¶æ€ä¸è¿‡ç¨‹è¯æ®")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_vge():
    ensure_project()
    st.markdown("### è¯æ®é“¾ä¸å¯éªŒè¯ç”Ÿæˆï¼ˆVGEï¼‰")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_dep_graph():
    ensure_project()
    st.markdown("### ä¾èµ–å›¾å¯è§†åŒ–")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_docx_export():
    ensure_project()
    st.markdown("### æ¨¡æ¿åŒ–DOCXå¯¼å‡º")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")



# ---------------------------
# è·¯ç”±é…ç½®
# ---------------------------
ROUTES = {
    "overview": page_overview,
    "training_plan": page_training_plan,
    "syllabus": page_syllabus,
    "calendar": page_calendar,
    "lesson_plan": page_lesson_plan,
    "assessment": page_assessment,
    "review": page_review,
    "report": page_report,
    "manual": page_manual,
    "evidence": page_evidence,
    "vge": page_vge,
    "dep_graph": page_dep_graph,
    "docx_export": page_docx_export,
}

# æ‰§è¡Œå½“å‰é¡µé¢
if project_id:
    fn = ROUTES.get(current_type, page_overview)
    fn()
else:
    st.info("è¯·å…ˆåœ¨å·¦ä¾§åˆ›å»ºæˆ–é€‰æ‹©é¡¹ç›®")