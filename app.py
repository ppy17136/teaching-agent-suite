# -*- coding: utf-8 -*-
"""
æ•™å­¦æ™ºèƒ½ä½“å¹³å° - æ•´åˆPDFå…¨é‡æŠ½å–ç‰ˆ
æ•´åˆäº†å®Œæ•´çš„PDFè§£æèƒ½åŠ›å’Œæ•™å­¦æ–‡æ¡£é“¾ç®¡ç†
"""

import os
import io
import re
import json
import time
import base64
import hashlib
import sqlite3
import zipfile
import threading
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple
import pandas as pd
import streamlit as st
from dataclasses import asdict, dataclass

# -------- å¯é€‰è§£æä¾èµ– --------
try:
    import pdfplumber
except Exception:
    pdfplumber = None
    st.error("ç¼ºå°‘ä¾èµ– pdfplumberï¼Œè¯·å®‰è£…ï¼špip install pdfplumber")

try:
    from docx import Document
except Exception:
    Document = None

# ---------------------------
# åŸºç¡€é…ç½®
# ---------------------------
st.set_page_config(page_title="æ•™å­¦æ™ºèƒ½ä½“å¹³å°", layout="wide")

DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)
DB_PATH = os.path.join(DATA_DIR, "app.db")

_DB_LOCK = threading.Lock()

# ---------------------------
# UI ç¾åŒ–ï¼ˆCSSï¼‰
# ---------------------------
def inject_css():
    st.markdown(
        """
<style>
.main .block-container {
    padding-top: 1.0rem;
    padding-bottom: 2rem;
    max-width: 100% !important;
    padding-left: 2rem;
    padding-right: 2rem;
}
h1, h2, h3 { letter-spacing: .2px; }
code { font-size: 0.9em; }

.topbar{
    padding: 18px 18px;
    border-radius: 18px;
    background: linear-gradient(90deg, #0ea5e9 0%, #6366f1 55%, #8b5cf6 100%);
    color: white;
    box-shadow: 0 8px 24px rgba(0,0,0,.12);
}
.topbar .title{ font-size: 30px; font-weight: 800; }
.topbar .sub{ opacity: .9; margin-top: 6px; font-size: 14px; }

.card{
    border: 1px solid rgba(0,0,0,.08);
    border-radius: 18px;
    padding: 16px 16px;
    background: rgba(255,255,255,.6);
    box-shadow: 0 6px 16px rgba(0,0,0,.06);
}
.badge{
    display:inline-block; padding: 2px 10px; border-radius: 999px;
    font-size: 12px; border: 1px solid rgba(0,0,0,.12); margin-right: 6px;
}
.badge.ok { background:#ecfdf5; color:#065f46; border-color:#a7f3d0; }
.badge.warn { background:#fffbeb; color:#92400e; border-color:#fde68a; }
.badge.bad { background:#fef2f2; color:#991b1b; border-color:#fecaca; }

.depbar{ display:flex; gap:8px; flex-wrap: wrap; padding: 10px 0; }
.depitem{
    padding: 8px 10px; border-radius: 14px; border: 1px solid rgba(0,0,0,.10);
    background: rgba(255,255,255,.7); font-size: 13px;
}
.depitem b{ margin-right:6px; }

.docbox{
    border: 1px solid rgba(0,0,0,.10);
    border-radius: 18px;
    padding: 14px 16px;
    background: rgba(255,255,255,.75);
    line-height: 1.55;
    white-space: normal;
}
section[data-testid="stSidebar"] .stMarkdown h2{ font-size: 18px; font-weight: 800; }
div[data-testid="stDataFrame"] { border-radius: 14px; overflow:hidden; }

/* ç¡®ä¿è¡¨æ ¼åˆ—åæœ‰æ•ˆ */
.stDataFrame th {
    font-weight: 600 !important;
}
</style>
""",
        unsafe_allow_html=True,
    )

inject_css()

# ---------------------------
# PDFå…¨é‡æŠ½å–æ ¸å¿ƒåŠŸèƒ½
# ---------------------------
def sha256_bytes(data: bytes) -> str:
    h = hashlib.sha256()
    h.update(data)
    return h.hexdigest()

def clean_text(s: str) -> str:
    if s is None:
        return ""
    s = str(s)
    s = s.replace("\u00a0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()

def normalize_multiline(text: str) -> str:
    """ä¿ç•™æ¢è¡Œï¼ŒåšåŸºç¡€æ¸…ç†"""
    if text is None:
        return ""
    text = str(text).replace("\r\n", "\n").replace("\r", "\n")
    lines = [clean_text(ln) for ln in text.split("\n")]
    out: List[str] = []
    blank = 0
    for ln in lines:
        if ln.strip() == "":
            blank += 1
            if blank <= 2:
                out.append("")
        else:
            blank = 0
            out.append(ln)
    return "\n".join(out).strip()

def make_unique_columns(cols: List[str]) -> List[str]:
    """ç¡®ä¿åˆ—åå”¯ä¸€ä¸”æœ‰æ•ˆ"""
    seen: Dict[str, int] = {}
    out: List[str] = []
    
    for i, c in enumerate(cols):
        c0 = clean_text(c) or f"col_{i+1}"
        
        # ç¡®ä¿åˆ—åä¸å«ç‰¹æ®Šå­—ç¬¦
        c0 = re.sub(r'[^\w\u4e00-\u9fff]+', '_', c0)
        if not c0:
            c0 = f"col_{i+1}"
        
        if c0 not in seen:
            seen[c0] = 1
            out.append(c0)
        else:
            seen[c0] += 1
            out.append(f"{c0}_{seen[c0]}")
    
    return out

def postprocess_table_df(df: pd.DataFrame) -> pd.DataFrame:
    """è¡¨æ ¼åå¤„ç†ï¼šå»ç©ºç™½ã€å»NaNã€åˆå¹¶æ ¼å‘ä¸‹å¡«å……"""
    if df is None or df.empty:
        return df
    
    df = df.copy()
    df = df.replace({None: ""}).fillna("")
    
    # ç¡®ä¿åˆ—åæ˜¯å”¯ä¸€ä¸”æœ‰æ•ˆçš„
    df.columns = make_unique_columns([str(c) for c in df.columns])
    
    for c in df.columns:
        df[c] = df[c].astype(str).map(lambda x: clean_text(x))
    
    # åˆ é™¤å®Œå…¨ç©ºè¡Œ
    mask_all_empty = df.apply(lambda r: all((clean_text(x) == "" for x in r.values.tolist())), axis=1)
    df = df.loc[~mask_all_empty].reset_index(drop=True)
    
    # å‘ä¸‹å¡«å……ï¼ˆåˆå¹¶æ ¼å¸¸è§åˆ—ï¼‰
    fill_down_keywords = ["è¯¾ç¨‹ä½“ç³»", "è¯¾ç¨‹æ¨¡å—", "è¯¾ç¨‹æ€§è´¨", "è¯¾ç¨‹ç±»åˆ«", "ç±»åˆ«", "æ¨¡å—", "ç¯èŠ‚", "å­¦æœŸ", "æ–¹å‘"]
    for c in df.columns:
        if any(k in str(c) for k in fill_down_keywords):
            last = ""
            new_col = []
            for v in df[c].tolist():
                if v != "":
                    last = v
                    new_col.append(v)
                else:
                    new_col.append(last)
            df[c] = new_col
    
    return df

def normalize_table(raw_table: List[List[Any]]) -> List[List[str]]:
    """
    pdfplumber.extract_tables() è¿”å› list[list[str|None]]
    è¿™é‡ŒåšåŸºç¡€æ¸…æ´—ï¼šå»ç©ºè¡Œã€è¡¥é½åˆ—æ•°ã€å»æ‰å…¨ç©ºåˆ—
    """
    if not raw_table:
        return []
    
    rows = []
    max_cols = 0
    for r in raw_table:
        if r is None:
            continue
        rr = [clean_text(c) for c in r]
        # è·³è¿‡å…¨ç©ºè¡Œ
        if all(c == "" for c in rr):
            continue
        rows.append(rr)
        max_cols = max(max_cols, len(rr))
    
    if not rows or max_cols == 0:
        return []
    
    # è¡¥é½åˆ—æ•°
    for i in range(len(rows)):
        if len(rows[i]) < max_cols:
            rows[i] = rows[i] + [""] * (max_cols - len(rows[i]))
    
    # å»æ‰å…¨ç©ºåˆ—
    keep_cols = []
    for j in range(max_cols):
        col = [rows[i][j] for i in range(len(rows))]
        if any(c != "" for c in col):
            keep_cols.append(j)
    
    if not keep_cols:
        return []
    
    cleaned = [[row[j] for j in keep_cols] for row in rows]
    return cleaned

def table_to_df(cleaned_table: List[List[str]]) -> pd.DataFrame:
    """
    å°è¯•æŠŠç¬¬ä¸€è¡Œå½“è¡¨å¤´ï¼›å¦‚æœè¡¨å¤´å¤ªå·®å°±ç”¨é»˜è®¤åˆ—åã€‚
    """
    if not cleaned_table or len(cleaned_table) == 0:
        return pd.DataFrame()
    
    if len(cleaned_table) == 1:
        # åªæœ‰ä¸€è¡Œï¼Œåšå•è¡Œdf
        df = pd.DataFrame([cleaned_table[0]])
    else:
        header = cleaned_table[0]
        body = cleaned_table[1:]
        
        # è¡¨å¤´åˆ¤å®šï¼šè‡³å°‘æœ‰ä¸€åŠå•å…ƒæ ¼éç©º
        non_empty = sum(1 for x in header if clean_text(x) != "")
        if non_empty >= max(1, len(header) // 2):
            cols = [h if h else f"col_{i+1}" for i, h in enumerate(header)]
            df = pd.DataFrame(body, columns=cols)
        else:
            # å¦åˆ™ä¸ç”¨è¡¨å¤´
            df = pd.DataFrame(cleaned_table)
    
    return postprocess_table_df(df)

def extract_pages_text_and_tables(pdf_bytes: bytes, enable_ocr: bool = False) -> Tuple[List[Dict[str, Any]], str]:
    """
    æå–æ¯é¡µçš„æ–‡æœ¬å’Œè¡¨æ ¼
    è¿”å›ï¼šé¡µé¢æ•°æ®åˆ—è¡¨ï¼ˆå«æ–‡æœ¬å’Œè¡¨æ ¼ï¼‰ï¼Œå…¨æ–‡æ–‡æœ¬
    """
    if pdfplumber is None:
        return [], ""
    
    pages_data = []
    full_text_parts = []
    
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        # è¡¨æ ¼è®¾ç½®ï¼šå"å®½æ¾"ï¼Œæå‡è·¨é¡µ/å¤æ‚è¡¨æ ¼æå–æˆåŠŸç‡
        table_settings = {
            "vertical_strategy": "lines",
            "horizontal_strategy": "lines",
            "intersection_tolerance": 5,
            "snap_tolerance": 3,
            "join_tolerance": 3,
            "edge_min_length": 3,
            "min_words_vertical": 1,
            "min_words_horizontal": 1,
            "text_tolerance": 2,
        }
        
        for idx, page in enumerate(pdf.pages, start=1):
            # æå–æ–‡æœ¬
            text = page.extract_text() or ""
            text = normalize_multiline(text)
            
            # å¦‚æœéœ€è¦OCRä¸”æ–‡æœ¬å¤ªå°‘
            if enable_ocr and len(text) < 50:
                try:
                    import pytesseract
                    from PIL import Image
                    img = page.to_image(resolution=220).original
                    ocr_text = pytesseract.image_to_string(img, lang="chi_sim+eng")
                    if len(ocr_text) > len(text):
                        text = normalize_multiline(ocr_text)
                except Exception:
                    pass
            
            full_text_parts.append(text)
            
            # æå–è¡¨æ ¼
            raw_tables = []
            try:
                raw_tables = page.extract_tables(table_settings=table_settings) or []
            except Exception:
                raw_tables = []
            
            # æ¸…æ´—è¡¨æ ¼
            cleaned_tables = []
            for t in raw_tables:
                ct = normalize_table(t)
                if ct:
                    cleaned_tables.append(ct)
            
            pages_data.append({
                "page": idx,
                "text": text,
                "tables": cleaned_tables,
                "tables_count": len(cleaned_tables)
            })
    
    full_text = "\n".join(full_text_parts)
    return pages_data, full_text

def split_sections(full_text: str) -> Dict[str, str]:
    """
    æŒ‰ "ä¸€ã€/äºŒã€/ä¸‰ã€..." å¤§ç« åˆ‡åˆ†ã€‚
    å…¼å®¹ï¼šä¸‰ã€ / ä¸‰. / ä¸‰ï¼
    """
    text = normalize_multiline(full_text)
    lines = text.splitlines()
    pat = re.compile(r"^\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s*[ã€\.ï¼]\s*([^\n\r]+?)\s*$")
    
    sections: Dict[str, List[str]] = {}
    cur_key = "å°é¢/å‰è¨€"
    
    for ln in lines:
        m = pat.match(ln)
        if m:
            num = m.group(1)
            title = clean_text(m.group(2))
            cur_key = f"{num}ã€{title}"
            sections.setdefault(cur_key, [])
        else:
            sections.setdefault(cur_key, []).append(ln)
    
    return {k: "\n".join(v).strip() for k, v in sections.items()}

def extract_appendix_titles(full_text: str) -> Dict[str, str]:
    """æŠ½å–"é™„è¡¨X -> æ ‡é¢˜" """
    titles: Dict[str, str] = {}
    text = normalize_multiline(full_text)
    for raw in text.splitlines():
        line = raw.strip()
        if not line:
            continue
        
        # 1) é™„è¡¨1ï¼šXXXX
        m = re.search(r"(é™„è¡¨\s*\d+)\s*[:ï¼š]\s*(.+)$", line)
        if m:
            key = re.sub(r"\s+", "", m.group(1))
            val = clean_text(m.group(2))
            if val:
                titles[key] = val
            continue
        
        # 2) ä¸ƒã€XXXXï¼ˆé™„è¡¨1ï¼‰
        m = re.search(r"^(?P<title>.+?)\s*[ï¼ˆ(]\s*(?P<key>é™„è¡¨\s*\d+)\s*[)ï¼‰]\s*$", line)
        if m:
            key = re.sub(r"\s+", "", m.group("key"))
            val = clean_text(m.group("title"))
            if val:
                titles[key] = val
            continue
        
        # 3) è¡Œå†…å‡ºç°ï¼ˆé™„è¡¨Xï¼‰
        m = re.search(r"(?P<title>.+?)\s*[ï¼ˆ(]\s*(?P<key>é™„è¡¨\s*\d+)\s*[)ï¼‰]", line)
        if m:
            key = re.sub(r"\s+", "", m.group("key"))
            val = clean_text(m.group("title"))
            if val and key not in titles:
                titles[key] = val
    
    return titles

def parse_training_objectives(section_text: str) -> Dict[str, Any]:
    """
    æå–"åŸ¹å…»ç›®æ ‡"æ¡ç›®
    """
    raw = normalize_multiline(section_text)
    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
    items: List[str] = []
    
    pat = re.compile(r"^(?:ï¼ˆ?\s*\d+\s*ï¼‰?|\d+\s*[\.ã€ï¼])\s*(.+)$")
    for ln in lines:
        m = pat.match(ln)
        if m:
            body = clean_text(m.group(1))
            if body:
                items.append(body)
    
    # å¦‚æœæ²¡æŠ“åˆ°ç¼–å·æ¡ç›®ï¼Œé€€åŒ–ï¼šå–å‰è‹¥å¹²è¡Œ
    if not items:
        items = lines[:30]
    
    return {"count": len(items), "items": items, "raw": raw}

def parse_graduation_requirements(text_any: str) -> Dict[str, Any]:
    """
    æŠ½å–12æ¡æ¯•ä¸šè¦æ±‚åŠå…¶åˆ†é¡¹
    """
    text = normalize_multiline(text_any or "")
    
    # å®šä½"äºŒã€æ¯•ä¸šè¦æ±‚"
    start = re.search(r"(?m)^\s*(äºŒ\s*[ã€\.ï¼]?\s*æ¯•ä¸šè¦æ±‚|æ¯•ä¸šè¦æ±‚)\s*$", text)
    if start:
        tail = text[start.start():]
    else:
        tail = text
    
    # æˆªæ–­åˆ°ä¸‹ä¸€å¤§ç« 
    end = re.search(r"(?m)^\s*[ä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*[ã€\.ï¼]", tail)
    if end:
        tail = tail[:end.start()]
    
    lines = [ln.strip() for ln in tail.splitlines()]
    
    main_pat = re.compile(r"^(?P<no>\d{1,2})\s*[\.ã€](?!\d)\s*(?P<body>.+)$")
    sub_pat = re.compile(r"^(?P<no>\d{1,2}\.\d{1,2})\s+(?P<body>.+)$")
    
    items: List[Dict[str, Any]] = []
    cur: Optional[Dict[str, Any]] = None
    cur_sub: Optional[Dict[str, Any]] = None
    
    def flush_sub():
        nonlocal cur_sub, cur
        if cur is not None and cur_sub is not None:
            cur.setdefault("subitems", []).append(cur_sub)
        cur_sub = None
    
    def flush_item():
        nonlocal cur
        if cur is not None:
            cur["title"] = clean_text(cur.get("title", ""))
            cur["body"] = clean_text(cur.get("body", ""))
            for s in cur.get("subitems", []):
                s["body"] = clean_text(s.get("body", ""))
            items.append(cur)
        cur = None
    
    for ln in lines:
        if not ln:
            continue
        
        m_main = main_pat.match(ln)
        m_sub = sub_pat.match(ln)
        
        if m_main:
            flush_sub()
            flush_item()
            no = int(m_main.group("no"))
            body_full = clean_text(m_main.group("body"))
            
            # å¤„ç†"å·¥ç¨‹çŸ¥è¯†ï¼š..."è¿™ç§
            title = ""
            body = body_full
            if "ï¼š" in body_full:
                title, body = body_full.split("ï¼š", 1)
                title = clean_text(title)
                body = clean_text(body)
            
            cur = {"no": no, "title": title, "body": body, "subitems": []}
            continue
        
        if m_sub and cur is not None:
            flush_sub()
            cur_sub = {"no": m_sub.group("no"), "body": clean_text(m_sub.group("body"))}
            continue
        
        # ç»­è¡Œ
        if cur_sub is not None:
            cur_sub["body"] += " " + ln
        elif cur is not None:
            cur["body"] += " " + ln
    
    flush_sub()
    flush_item()
    
    items = sorted(items, key=lambda x: x.get("no", 999))
    if len(items) > 12:
        items = [x for x in items if 1 <= x.get("no", 0) <= 12]
    
    return {"count": len(items), "items": items, "raw": tail.strip()}

def guess_table_appendix_by_page(page_no: int) -> Optional[str]:
    """
    é’ˆå¯¹å¸¸è§åŸ¹å…»æ–¹æ¡ˆï¼ˆæœ¬æ ·ä¾‹ 18 é¡µï¼‰ï¼š
    10-11 é™„è¡¨1ï¼Œ12 é™„è¡¨2ï¼Œ13-14 é™„è¡¨3ï¼Œ15 é™„è¡¨4ï¼Œ16 é™„è¡¨5
    """
    mapping = {
        10: "é™„è¡¨1", 11: "é™„è¡¨1",
        12: "é™„è¡¨2",
        13: "é™„è¡¨3", 14: "é™„è¡¨3",
        15: "é™„è¡¨4",
        16: "é™„è¡¨5",
    }
    return mapping.get(page_no)

def infer_table_title_from_page_text(page_text: str, appendix: Optional[str], appendix_titles: Dict[str, str], page_no: int) -> str:
    if appendix and appendix in appendix_titles:
        return appendix_titles[appendix]
    
    if appendix:
        m = re.search(rf"(?P<title>[^\n\r]{{2,120}}?)\s*[ï¼ˆ(]\s*{re.escape(appendix)}\s*[)ï¼‰]", page_text)
        if m:
            return clean_text(m.group("title"))
    
    m = re.search(r"(é™„è¡¨\s*\d+)\s*[:ï¼š]\s*([^\n\r]{2,120})", page_text)
    if m:
        return clean_text(m.group(2))
    
    return appendix or f"ç¬¬{page_no}é¡µè¡¨æ ¼"

def infer_direction_for_page(page_text: str) -> str:
    has_weld = "ç„Šæ¥" in page_text
    has_ndt = ("æ— æŸ" in page_text) or ("æ— æŸæ£€æµ‹" in page_text)
    if has_weld and has_ndt:
        return "æ··åˆï¼ˆç„Šæ¥+æ— æŸæ£€æµ‹ï¼‰"
    if has_weld:
        return "ç„Šæ¥"
    if has_ndt:
        return "æ— æŸæ£€æµ‹"
    return ""

def add_direction_column_rowwise(df: pd.DataFrame, page_direction: str) -> pd.DataFrame:
    """
    è¡Œçº§æ–¹å‘è¯†åˆ«
    """
    if df is None or df.empty:
        return df
    
    df = df.copy()
    cur_dir = ""
    dirs = []
    for _, row in df.iterrows():
        row_txt = " ".join([clean_text(x) for x in row.values.tolist()])
        if re.search(r"ç„Šæ¥.*æ–¹å‘", row_txt):
            cur_dir = "ç„Šæ¥"
        elif re.search(r"æ— æŸ.*æ–¹å‘", row_txt) or re.search(r"æ— æŸæ£€æµ‹.*æ–¹å‘", row_txt):
            cur_dir = "æ— æŸæ£€æµ‹"
        
        dirs.append(cur_dir or page_direction)
    
    # æ’åˆ°æœ€å‰
    if "ä¸“ä¸šæ–¹å‘" not in df.columns:
        df.insert(0, "ä¸“ä¸šæ–¹å‘", dirs)
    else:
        df["ä¸“ä¸šæ–¹å‘"] = [d or page_direction for d in dirs]
    
    return df

@dataclass
class TablePack:
    page: int
    title: str
    appendix: str
    direction: str
    columns: List[str]
    rows: List[List[Any]]

@dataclass
class ExtractResult:
    page_count: int
    table_count: int
    ocr_used: bool
    file_sha256: str
    extracted_at: str
    pages_data: List[Dict[str, Any]]
    sections: Dict[str, str]
    appendix_titles: Dict[str, str]
    training_objectives: Dict[str, Any]
    graduation_requirements: Dict[str, Any]
    tables: List[Dict[str, Any]]  # TablePack as dict
    full_text: str

def run_full_extract(pdf_bytes: bytes, use_ocr: bool = False) -> ExtractResult:
    """
    è¿è¡Œå…¨é‡æŠ½å–
    """
    # 1) æå–é¡µé¢æ–‡æœ¬å’Œè¡¨æ ¼
    pages_data, full_text = extract_pages_text_and_tables(pdf_bytes, enable_ocr=use_ocr)
    
    # 2) ç»“æ„åŒ–è§£æ
    sections = split_sections(full_text)
    appendix_titles = extract_appendix_titles(full_text)
    
    # 3) å…³é”®ç»“æ„åŒ–ï¼šåŸ¹å…»ç›®æ ‡ã€æ¯•ä¸šè¦æ±‚
    obj_key = next((k for k in sections.keys() if "åŸ¹å…»ç›®æ ‡" in k), "")
    obj = parse_training_objectives(sections.get(obj_key, "") or full_text)
    grad = parse_graduation_requirements(full_text)
    
    # 4) å¤„ç†è¡¨æ ¼
    tables: List[TablePack] = []
    total_tables = 0
    
    for page_data in pages_data:
        page_no = page_data["page"]
        page_text = page_data["text"]
        page_tables = page_data["tables"]
        
        total_tables += len(page_tables)
        
        appendix = guess_table_appendix_by_page(page_no) or ""
        base_title = infer_table_title_from_page_text(page_text, appendix or None, appendix_titles, page_no)
        title = f"{base_title}ï¼ˆ{appendix}ï¼‰" if appendix and appendix not in base_title else base_title
        page_dir = infer_direction_for_page(page_text)
        
        for i, table_data in enumerate(page_tables):
            df = table_to_df(table_data)
            if df is not None and not df.empty:
                df2 = add_direction_column_rowwise(df, page_dir)
                sub_title = title if len(page_tables) == 1 else f"{title} - è¡¨{i+1}"
                pack = TablePack(
                    page=page_no,
                    title=sub_title,
                    appendix=appendix,
                    direction=page_dir,
                    columns=[str(c) for c in df2.columns],
                    rows=df2.values.tolist(),
                )
                tables.append(pack)
    
    result = ExtractResult(
        page_count=len(pages_data),
        table_count=total_tables,
        ocr_used=use_ocr,
        file_sha256=sha256_bytes(pdf_bytes),
        extracted_at=datetime.now().isoformat(timespec="seconds"),
        pages_data=pages_data,
        sections=sections,
        appendix_titles=appendix_titles,
        training_objectives=obj,
        graduation_requirements=grad,
        tables=[asdict(t) for t in tables],
        full_text=full_text
    )
    return result

def safe_df_from_tablepack(t: Dict[str, Any]) -> pd.DataFrame:
    """ä» TablePack å­—å…¸åˆ›å»º DataFrame"""
    cols = t.get("columns") or []
    rows = t.get("rows") or []
    
    if rows and len(rows) > 0:
        df = pd.DataFrame(rows, columns=cols)
        return postprocess_table_df(df)
    return pd.DataFrame()

def make_tables_zip(tables: List[Dict[str, Any]]) -> bytes:
    """CSV + tables.json æ‰“åŒ…"""
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        zf.writestr("tables.json", json.dumps(tables, ensure_ascii=False, indent=2))
        for idx, t in enumerate(tables, start=1):
            title = clean_text(t.get("title") or f"table_{idx}")
            title_safe = re.sub(r"[^0-9A-Za-z\u4e00-\u9fff_\-]+", "_", title)[:80].strip("_") or f"table_{idx}"
            
            df = safe_df_from_tablepack(t)
            
            # æ–¹å‘åˆ—
            direction = clean_text(t.get("direction") or "")
            if direction and "ä¸“ä¸šæ–¹å‘" not in df.columns:
                df.insert(0, "ä¸“ä¸šæ–¹å‘", direction)
            
            csv_bytes = df.to_csv(index=False, encoding="utf-8-sig")
            zf.writestr(f"{idx:02d}_{title_safe}.csv", csv_bytes)
    return buf.getvalue()

# ---------------------------
# æ•°æ®åº“å±‚
# ---------------------------
def db() -> sqlite3.Connection:
    os.makedirs(DATA_DIR, exist_ok=True)
    conn = sqlite3.connect(DB_PATH, check_same_thread=False, timeout=30)
    conn.execute("PRAGMA foreign_keys=ON;")
    conn.execute("PRAGMA busy_timeout=5000;")
    try:
        conn.execute("PRAGMA journal_mode=WAL;")
    except Exception:
        conn.execute("PRAGMA journal_mode=DELETE;")
    return conn

def init_db():
    with _DB_LOCK:
        conn = db()
        conn.execute(
            """
CREATE TABLE IF NOT EXISTS projects(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL,
    meta_json TEXT DEFAULT '{}',
    created_at INTEGER NOT NULL
);
"""
        )
        conn.execute(
            """
CREATE TABLE IF NOT EXISTS artifacts(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    project_id INTEGER NOT NULL,
    type TEXT NOT NULL,
    title TEXT NOT NULL,
    content_md TEXT NOT NULL,
    content_json TEXT NOT NULL DEFAULT '{}',
    hash TEXT NOT NULL,
    created_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL,
    FOREIGN KEY(project_id) REFERENCES projects(id) ON DELETE CASCADE
);
"""
        )
        conn.execute(
            """
CREATE TABLE IF NOT EXISTS versions(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    artifact_id INTEGER NOT NULL,
    version_no INTEGER NOT NULL,
    content_md TEXT NOT NULL,
    content_json TEXT NOT NULL,
    hash TEXT NOT NULL,
    created_at INTEGER NOT NULL,
    note TEXT DEFAULT '',
    FOREIGN KEY(artifact_id) REFERENCES artifacts(id) ON DELETE CASCADE
);
"""
        )
        conn.execute(
            """
CREATE TABLE IF NOT EXISTS edges(
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    project_id INTEGER NOT NULL,
    child_artifact_id INTEGER NOT NULL,
    parent_artifact_id INTEGER NOT NULL,
    created_at INTEGER NOT NULL,
    FOREIGN KEY(project_id) REFERENCES projects(id) ON DELETE CASCADE,
    FOREIGN KEY(child_artifact_id) REFERENCES artifacts(id) ON DELETE CASCADE,
    FOREIGN KEY(parent_artifact_id) REFERENCES artifacts(id) ON DELETE CASCADE
);
"""
        )
        conn.commit()
        conn.close()

def ensure_db_schema():
    init_db()

def now_ts() -> int:
    return int(time.time())

def sha256_text(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def compute_hash(content_md: str, content_json: Dict[str, Any], parent_hashes: List[str]) -> str:
    payload = {"content_md": content_md, "content_json": content_json, "parents": parent_hashes}
    return sha256_text(json.dumps(payload, ensure_ascii=False, sort_keys=True))

# ---------------------------
# æ•°æ®åº“æ“ä½œ
# ---------------------------
def get_projects() -> List[Tuple[int, str]]:
    conn = db()
    rows = conn.execute("SELECT id, name FROM projects ORDER BY id DESC;").fetchall()
    conn.close()
    return rows

def get_project_meta(project_id: int) -> Dict[str, Any]:
    conn = db()
    row = conn.execute("SELECT meta_json FROM projects WHERE id=?", (project_id,)).fetchone()
    conn.close()
    if not row:
        return {}
    try:
        return json.loads(row[0] or "{}")
    except Exception:
        return {}

def create_project(name: str, meta: Dict[str, Any]) -> int:
    with _DB_LOCK:
        conn = db()
        ts = now_ts()
        cur = conn.execute(
            "INSERT INTO projects(name, meta_json, created_at) VALUES(?,?,?)",
            (name, json.dumps(meta, ensure_ascii=False), ts),
        )
        conn.commit()
        pid = cur.lastrowid
        conn.close()
        return pid

def list_artifacts(project_id: int) -> List[Dict[str, Any]]:
    conn = db()
    try:
        rows = conn.execute(
            "SELECT id, type, title, hash, updated_at "
            "FROM artifacts WHERE project_id=? ORDER BY updated_at DESC",
            (project_id,),
        ).fetchall()
    except sqlite3.OperationalError:
        conn.close()
        ensure_db_schema()
        conn = db()
        rows = conn.execute(
            "SELECT id, type, title, hash, updated_at "
            "FROM artifacts WHERE project_id=? ORDER BY updated_at DESC",
            (project_id,),
        ).fetchall()
    conn.close()
    return [{"id": r[0], "type": r[1], "title": r[2], "hash": r[3], "updated_at": r[4]} for r in rows]

def get_artifact(project_id: int, a_type: str) -> Optional[Dict[str, Any]]:
    conn = db()
    row = conn.execute(
        "SELECT id, title, content_md, content_json, hash, created_at, updated_at "
        "FROM artifacts WHERE project_id=? AND type=? ORDER BY updated_at DESC LIMIT 1",
        (project_id, a_type),
    ).fetchone()
    conn.close()
    if not row:
        return None
    return {
        "id": row[0],
        "type": a_type,
        "title": row[1],
        "content_md": row[2],
        "content_json": json.loads(row[3] or "{}"),
        "hash": row[4],
        "created_at": row[5],
        "updated_at": row[6],
    }

def get_versions(artifact_id: int) -> List[Dict[str, Any]]:
    conn = db()
    rows = conn.execute(
        "SELECT version_no, hash, created_at, note FROM versions WHERE artifact_id=? ORDER BY version_no DESC",
        (artifact_id,),
    ).fetchall()
    conn.close()
    return [{"version_no": r[0], "hash": r[1], "created_at": r[2], "note": r[3]} for r in rows]

def set_edges(project_id: int, child_id: int, parent_ids: List[int]):
    with _DB_LOCK:
        conn = db()
        conn.execute("DELETE FROM edges WHERE project_id=? AND child_artifact_id=?", (project_id, child_id))
        ts = now_ts()
        for pid in parent_ids:
            conn.execute(
                "INSERT INTO edges(project_id, child_artifact_id, parent_artifact_id, created_at) VALUES(?,?,?,?)",
                (project_id, child_id, pid, ts),
            )
        conn.commit()
        conn.close()

def upsert_artifact(
    project_id: int,
    a_type: str,
    title: str,
    content_md: str,
    content_json: Dict[str, Any],
    parent_ids: List[int],
    note: str = "",
) -> Dict[str, Any]:
    existing = get_artifact(project_id, a_type)
    
    parent_hashes: List[str] = []
    for pid in parent_ids:
        conn = db()
        row = conn.execute("SELECT hash FROM artifacts WHERE id=? AND project_id=?", (pid, project_id)).fetchone()
        conn.close()
        if row:
            parent_hashes.append(row[0])
    
    new_hash = compute_hash(content_md, content_json, parent_hashes)
    ts = now_ts()
    
    with _DB_LOCK:
        conn = db()
        if existing:
            cur_ver = conn.execute("SELECT MAX(version_no) FROM versions WHERE artifact_id=?", (existing["id"],)).fetchone()
            next_ver = (cur_ver[0] or 0) + 1
            conn.execute(
                "INSERT INTO versions(artifact_id, version_no, content_md, content_json, hash, created_at, note) "
                "VALUES(?,?,?,?,?,?,?)",
                (
                    existing["id"],
                    next_ver,
                    existing["content_md"],
                    json.dumps(existing["content_json"], ensure_ascii=False),
                    existing["hash"],
                    ts,
                    note or "auto-save",
                ),
            )
            conn.execute(
                "UPDATE artifacts SET title=?, content_md=?, content_json=?, hash=?, updated_at=? "
                "WHERE id=? AND project_id=?",
                (title, content_md, json.dumps(content_json, ensure_ascii=False), new_hash, ts, existing["id"], project_id),
            )
            conn.commit()
        else:
            conn.execute(
                "INSERT INTO artifacts(project_id, type, title, content_md, content_json, hash, created_at, updated_at) "
                "VALUES(?,?,?,?,?,?,?,?)",
                (project_id, a_type, title, content_md, json.dumps(content_json, ensure_ascii=False), new_hash, ts, ts),
            )
            conn.commit()
        conn.close()
    
    a = get_artifact(project_id, a_type)
    if a:
        set_edges(project_id, a["id"], parent_ids)
    return a

# ---------------------------
# æ–‡æ¡£é“¾ & ä¾èµ–è§„åˆ™
# ---------------------------
DOC_TYPES = [
    ("overview", "é¦–é¡µæ€»è§ˆ"),
    ("training_plan", "åŸ¹å…»æ–¹æ¡ˆï¼ˆåº•åº§ï¼‰"),
    ("syllabus", "è¯¾ç¨‹æ•™å­¦å¤§çº²ï¼ˆä¾èµ–åŸ¹å…»æ–¹æ¡ˆï¼‰"),
    ("calendar", "æ•™å­¦æ—¥å†ï¼ˆä¾èµ–å¤§çº²ï¼‰"),
    ("lesson_plan", "æ•™æ¡ˆï¼ˆä¾èµ–æ—¥å†ï¼‰"),
    ("assessment", "ä½œä¸š/é¢˜åº“/è¯•å·æ–¹æ¡ˆï¼ˆä¾èµ–å¤§çº²ï¼‰"),
    ("review", "å®¡æ ¸è¡¨ï¼ˆä¾èµ–è¯•å·æ–¹æ¡ˆ/å¤§çº²ï¼‰"),
    ("report", "è¯¾ç¨‹ç›®æ ‡è¾¾æˆæŠ¥å‘Šï¼ˆä¾èµ–å¤§çº²/æˆç»©ï¼‰"),
    ("manual", "æˆè¯¾æ‰‹å†Œï¼ˆä¾èµ–æ•™æ¡ˆ/è¿‡ç¨‹è¯æ®ï¼‰"),
    ("evidence", "è¯¾å ‚çŠ¶æ€ä¸è¿‡ç¨‹è¯æ®ï¼ˆå¯é€‰ï¼‰"),
    ("vge", "è¯æ®é“¾ä¸å¯éªŒè¯ç”Ÿæˆï¼ˆVGEï¼‰"),
    ("dep_graph", "ä¾èµ–å›¾å¯è§†åŒ–ï¼ˆæ ‘/Graphvizï¼‰"),
    ("docx_export", "æ¨¡æ¿åŒ–DOCXå¯¼å‡ºï¼ˆå­—æ®µæ˜ å°„å¡«å……ï¼‰"),
]

DEP_RULES = {
    "training_plan": [],
    "syllabus": ["training_plan"],
    "calendar": ["syllabus"],
    "lesson_plan": ["calendar"],
    "assessment": ["syllabus"],
    "review": ["assessment", "syllabus"],
    "report": ["syllabus"],
    "manual": ["lesson_plan"],
    "evidence": [],
    "vge": [],
    "overview": [],
    "dep_graph": [],
    "docx_export": [],
}

# ---------------------------
# é€šç”¨å·¥å…·å‡½æ•°
# ---------------------------
def type_label(a_type: str) -> str:
    for t, name in DOC_TYPES:
        if t == a_type:
            return name
    return a_type

def dep_status(project_id: int, a_type: str) -> Tuple[bool, List[Tuple[str, bool]]]:
    req = DEP_RULES.get(a_type, [])
    detail = []
    ok = True
    for r in req:
        exists = get_artifact(project_id, r) is not None
        detail.append((r, exists))
        ok = ok and exists
    return ok, detail

def render_depbar(project_id: int, a_type: str):
    ok, detail = dep_status(project_id, a_type)
    chips = []
    for r, exists in detail:
        cls = "ok" if exists else "bad"
        chips.append(f'<span class="badge {cls}">{type_label(r)}</span>')
    st.markdown(
        f"""
<div class="depbar">
    <div class="depitem"><b>ä¾èµ–æ£€æŸ¥</b>ï¼š{"âœ…é½å…¨" if ok else "âš ï¸ç¼ºå¤±ä¸Šæ¸¸"}</div>
    <div class="depitem">{''.join(chips) if chips else '<span class="badge ok">æ— ä¸Šæ¸¸ä¾èµ–</span>'}</div>
</div>
""",
        unsafe_allow_html=True,
    )

def artifact_toolbar(a: Dict[str, Any]):
    import html as _html
    st.markdown(
        f"""
<div class="card">
    <div style="display:flex; justify-content:space-between; gap:12px; align-items:center;">
        <div>
            <div style="font-size:18px; font-weight:800;">{_html.escape(a['title'])}</div>
            <div style="opacity:.75; font-size:12px; margin-top:4px;">
                ç±»å‹ï¼š{type_label(a['type'])} ï½œ Hashï¼š<code>{a['hash'][:12]}</code> ï½œ æ›´æ–°æ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(a['updated_at']))}
            </div>
        </div>
        <div>
            <span class="badge ok">å¯ç¼–è¾‘</span>
            <span class="badge warn">å¯ç‰ˆæœ¬åŒ–</span>
            <span class="badge warn">ä¾èµ–å¯è¿½æº¯</span>
        </div>
    </div>
</div>
""",
        unsafe_allow_html=True,
    )

def md_textarea(label: str, value: str, height: int = 420, key: str = "") -> str:
    return st.text_area(label, value=value, height=height, key=key)

# ---------------------------
# æ¨¡æ¿å‡½æ•°
# ---------------------------
def template_training_plan(major: str, grade: str, course_group: str) -> str:
    return f"""# {grade}çº§ã€Š{major}ã€‹åŸ¹å…»æ–¹æ¡ˆï¼ˆç¤ºä¾‹ï¼‰

## ä¸€ã€åŸ¹å…»ç›®æ ‡
- é¢å‘å·¥ç¨‹å®è·µï¼Œå…·å¤‡æ‰å®çš„æ•°å­¦/åŠ›å­¦/ææ–™åŸºç¡€
- å…·å¤‡ææ–™æˆå‹ä¸åˆ¶é€ è¿‡ç¨‹çš„åˆ†æã€è®¾è®¡ä¸ä¼˜åŒ–èƒ½åŠ›
- å…·å¤‡å·¥ç¨‹ä¼¦ç†ã€å›¢é˜Ÿåä½œä¸ç»ˆèº«å­¦ä¹ èƒ½åŠ›

## äºŒã€æ¯•ä¸šè¦æ±‚ï¼ˆç¤ºä¾‹ï¼‰
1. å·¥ç¨‹çŸ¥è¯†
2. é—®é¢˜åˆ†æ
3. è®¾è®¡/å¼€å‘è§£å†³æ–¹æ¡ˆ
4. ç ”ç©¶
5. ç°ä»£å·¥å…·ä½¿ç”¨
6. å·¥ç¨‹ä¸ç¤¾ä¼š
7. ç¯å¢ƒä¸å¯æŒç»­å‘å±•
8. èŒä¸šè§„èŒƒ
9. ä¸ªäººä¸å›¢é˜Ÿ
10. æ²Ÿé€š
11. é¡¹ç›®ç®¡ç†
12. ç»ˆèº«å­¦ä¹ 

## ä¸‰ã€è¯¾ç¨‹ä½“ç³»ï¼š{course_group}
- é€šè¯†ä¸åŸºç¡€
- ä¸“ä¸šæ ¸å¿ƒ
- ä¸“ä¸šæ–¹å‘
- å®è·µç¯èŠ‚
"""

# ---------------------------
# é¡¶éƒ¨ä¸ä¾§è¾¹æ 
# ---------------------------
def topbar():
    st.markdown(
        """
<div class="topbar">
    <div class="title">æ•™å­¦æ™ºèƒ½ä½“å¹³å° - PDFå…¨é‡æŠ½å–ç‰ˆ</div>
    <div class="sub">åŸ¹å…»æ–¹æ¡ˆPDFå…¨é‡æŠ½å–ï¼ˆæ–‡æœ¬+è¡¨æ ¼+ç»“æ„ï¼‰â†’ å¤§çº² â†’ æ—¥å† â†’ æ•™æ¡ˆ â†’ è¯•å·/å®¡æ ¸ â†’ è¾¾æˆæŠ¥å‘Š â†’ æˆè¯¾æ‰‹å†Œ</div>
</div>
""",
        unsafe_allow_html=True,
    )

# åˆå§‹åŒ–DB
ensure_db_schema()
topbar()

# ä¾§è¾¹æ é…ç½®
st.sidebar.markdown("## è¿è¡Œæ¨¡å¼")
run_mode = st.sidebar.radio("è¿è¡Œæ¨¡å¼", ["æ¼”ç¤ºæ¨¡å¼ï¼ˆæ— APIï¼‰", "åœ¨çº¿æ¨¡å¼ï¼ˆåƒé—®APIï¼‰"], index=0)
st.sidebar.caption("æ¼”ç¤ºæ¨¡å¼ä¸éœ€è¦ Keyï¼›åœ¨çº¿æ¨¡å¼è¯·åœ¨ Secrets ä¸­é…ç½® QWEN_API_KEYã€‚")

st.sidebar.markdown("## é¡¹ç›®ï¼ˆä¸“ä¸š/å¹´çº§/è¯¾ç¨‹ä½“ç³»ï¼‰")
projects = get_projects()
p_names = ["ï¼ˆæ–°å»ºé¡¹ç›®ï¼‰"] + [f"{pid} Â· {name}" for pid, name in projects]
p_sel = st.sidebar.selectbox("é€‰æ‹©é¡¹ç›®", p_names, index=0)

if p_sel == "ï¼ˆæ–°å»ºé¡¹ç›®ï¼‰":
    with st.sidebar.expander("åˆ›å»ºæ–°é¡¹ç›®", expanded=True):
        pname = st.text_input("é¡¹ç›®åç§°", value="ææ–™æˆå‹-æ•™è¯„ä¸€ä½“åŒ–ç¤ºä¾‹", key="new_pname")
        major = st.text_input("ä¸“ä¸š", value="ææ–™æˆå‹åŠæ§åˆ¶å·¥ç¨‹", key="new_major")
        grade = st.text_input("å¹´çº§", value="22", key="new_grade")
        course_group = st.text_input("è¯¾ç¨‹ä½“ç³»/æ–¹å‘", value="ææ–™æˆå‹-æ•°å€¼æ¨¡æ‹Ÿæ–¹å‘", key="new_group")
        if st.button("åˆ›å»ºé¡¹ç›®", type="primary"):
            pid = create_project(pname, {"major": major, "grade": grade, "course_group": course_group})
            st.success("å·²åˆ›å»ºé¡¹ç›®ï¼Œè¯·åœ¨ä¸‹æ‹‰ä¸­é€‰æ‹©å®ƒã€‚")
            st.rerun()
    project_id = None
else:
    project_id = int(p_sel.split("Â·")[0].strip())

st.sidebar.markdown("## åŠŸèƒ½æ¨¡å—")
module = st.sidebar.radio("å¯¼èˆª", [name for _, name in DOC_TYPES], index=1)
type_by_name = {name: t for t, name in DOC_TYPES}
current_type = type_by_name[module]

# ---------------------------
# é¡µé¢è·¯ç”±
# ---------------------------
def ensure_project():
    if project_id is None:
        st.info("è¯·å…ˆåœ¨å·¦ä¾§åˆ›å»ºå¹¶é€‰æ‹©ä¸€ä¸ªé¡¹ç›®ã€‚")
        st.stop()

def pick_parents_for(project_id: int, a_type: str) -> List[int]:
    req = DEP_RULES.get(a_type, [])
    parent_ids: List[int] = []
    for r in req:
        pa = get_artifact(project_id, r)
        if pa:
            parent_ids.append(pa["id"])
    if a_type == "manual":
        ev = get_artifact(project_id, "evidence")
        if ev:
            parent_ids.append(ev["id"])
    return parent_ids

def page_overview():
    ensure_project()
    st.markdown("### é¦–é¡µæ€»è§ˆ")
    arts = list_artifacts(project_id)
    if not arts:
        st.info("å½“å‰é¡¹ç›®è¿˜æ²¡æœ‰ä»»ä½•æ–‡æ¡£ã€‚å»ºè®®å…ˆä»'åŸ¹å…»æ–¹æ¡ˆï¼ˆåº•åº§ï¼‰'å¼€å§‹ã€‚")
        return
    
    st.markdown('<div class="card">ğŸ“Œ å½“å‰é¡¹ç›®å·²æœ‰æ–‡æ¡£ï¼ˆæœ€è¿‘æ›´æ–°åœ¨å‰ï¼‰</div>', unsafe_allow_html=True)
    rows = []
    for a in arts:
        rows.append({
            "ç±»å‹": type_label(a["type"]),
            "æ ‡é¢˜": a["title"],
            "Hash(å‰12)": a["hash"][:12],
            "æ›´æ–°æ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(a["updated_at"])),
        })
    st.dataframe(rows, use_container_width=True)

def page_training_plan():
    ensure_project()
    a = get_artifact(project_id, "training_plan")
    render_depbar(project_id, "training_plan")
    
    st.markdown("### åŸ¹å…»æ–¹æ¡ˆï¼ˆåº•åº§ï¼‰")
    st.caption("æ¨èï¼šä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆPDF â†’ å…¨é‡æŠ½å– â†’ è¯†åˆ«æ¸…å•ç¡®è®¤/ä¿®æ­£ â†’ ä¿å­˜ï¼ˆç»“æ„åŒ–åº•åº§ï¼‰ã€‚")
    
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ç”Ÿæˆ/ä¸Šä¼ &è¯†åˆ«ç¡®è®¤", "é¢„è§ˆ", "ç¼–è¾‘", "ç‰ˆæœ¬/å¯¼å‡º", "PDFå…¨é‡æŠ½å–ç•Œé¢"])
    
    with tab1:
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.markdown("#### æ–¹å¼Aï¼šä¸€é”®ç”Ÿæˆï¼ˆæ¼”ç¤º/å¿«é€Ÿï¼‰")
            major = st.text_input("ä¸“ä¸š", value="ææ–™æˆå‹åŠæ§åˆ¶å·¥ç¨‹", key="tp_major")
            grade = st.text_input("å¹´çº§", value="22", key="tp_grade")
            group = st.text_input("è¯¾ç¨‹ä½“ç³»/æ–¹å‘", value="ææ–™æˆå‹-æ•°å€¼æ¨¡æ‹Ÿæ–¹å‘", key="tp_group")
            if st.button("ç”ŸæˆåŸ¹å…»æ–¹æ¡ˆå¹¶ä¿å­˜", type="primary"):
                md = template_training_plan(major, grade, group)
                a = upsert_artifact(
                    project_id,
                    "training_plan",
                    f"{grade}çº§ã€Š{major}ã€‹åŸ¹å…»æ–¹æ¡ˆ",
                    md,
                    {"major": major, "grade": grade, "course_group": group, "confirmed": True},
                    [],
                    note="generate",
                )
                st.success("å·²ä¿å­˜åŸ¹å…»æ–¹æ¡ˆï¼ˆå¯ä½œä¸ºåç»­æ–‡ä»¶ä¾èµ–åº•åº§ï¼‰")
                st.rerun()
        
        with col2:
            st.markdown("#### æ–¹å¼Bï¼šä¸Šä¼ PDFå…¨é‡æŠ½å–ï¼ˆæ¨èï¼‰")
            up = st.file_uploader("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆPDFæ–‡ä»¶", type=["pdf"], key="tp_upload")
            use_ocr = st.checkbox("å¯ç”¨OCRï¼ˆé’ˆå¯¹æ‰«æç‰ˆPDFï¼‰", value=False)
            
            if up is not None and st.button("å¼€å§‹å…¨é‡æŠ½å–", key="tp_start_extract"):
                pdf_bytes = up.read()
                with st.spinner("æ­£åœ¨å…¨é‡æŠ½å–PDF..."):
                    extract_result = run_full_extract(pdf_bytes, use_ocr=use_ocr)
                
                # ä¿å­˜æŠ½å–ç»“æœåˆ°session
                st.session_state["tp_extract"] = {
                    "source": up.name,
                    "pdf_bytes": pdf_bytes,
                    "extract_result": asdict(extract_result),
                    "confirmed": False
                }
                st.success("PDFæŠ½å–å®Œæˆï¼è¯·åœ¨ä¸‹æ–¹ç¡®è®¤/ä¿®æ­£æŠ½å–ç»“æœã€‚")
        
        # è¯†åˆ«æ¸…å•ç¡®è®¤ç•Œé¢
        if "tp_extract" in st.session_state:
            ex = st.session_state["tp_extract"]
            extract_result = ex["extract_result"]
            
            st.markdown("---")
            st.markdown("### PDFå…¨é‡æŠ½å–ç»“æœï¼ˆè¯·ç¡®è®¤/ä¿®æ­£ï¼‰")
            
            # åŸºæœ¬ä¿¡æ¯
            colA, colB, colC = st.columns(3)
            with colA:
                major2 = st.text_input("ä¸“ä¸š", 
                                      value=extract_result.get("major_guess", "") or "ææ–™æˆå‹åŠæ§åˆ¶å·¥ç¨‹", 
                                      key="tp_major_fix")
                grade2 = st.text_input("å¹´çº§", 
                                      value=extract_result.get("grade_guess", "") or "22", 
                                      key="tp_grade_fix")
            with colB:
                course_group2 = st.text_input("è¯¾ç¨‹ä½“ç³»/æ–¹å‘", 
                                             value=extract_result.get("course_group_guess", "") or "ææ–™æˆå‹æ–¹å‘", 
                                             key="tp_group_fix")
                confirmed_flag = st.checkbox("æˆ‘å·²ç¡®è®¤ä»¥ä¸Šä¿¡æ¯å¤§ä½“æ­£ç¡®", value=False, key="tp_confirm_flag")
            with colC:
                st.metric("æ€»é¡µæ•°", extract_result.get("page_count", 0))
                st.metric("è¡¨æ ¼æ€»æ•°", extract_result.get("table_count", 0))
            
            st.markdown("#### 1) åŸ¹å…»ç›®æ ‡ï¼ˆå¯ç¼–è¾‘ï¼‰")
            goals = extract_result.get("training_objectives", {}).get("items", [])
            goals_text = st.text_area(
                "æ¯è¡Œä¸€ä¸ªç›®æ ‡ï¼ˆå¯å¢åˆ /æ”¹å†™ï¼‰",
                value="\n".join(goals) if goals else "",
                height=140,
                key="tp_goals_edit",
            )
            goals_final = [x.strip() for x in goals_text.splitlines() if x.strip()]
            
            st.markdown("#### 2) æ¯•ä¸šè¦æ±‚ï¼ˆå¯ç¼–è¾‘ï¼‰")
            grad_items = extract_result.get("graduation_requirements", {}).get("items", [])
            if grad_items:
                # åˆ›å»ºå¯ç¼–è¾‘çš„DataFrame
                grad_data = []
                for item in grad_items:
                    grad_data.append({
                        "ç¼–å·": item.get("no", ""),
                        "æ ‡é¢˜": item.get("title", ""),
                        "å†…å®¹": item.get("body", "")
                    })
                df_grad = pd.DataFrame(grad_data)
                df_grad_edited = st.data_editor(df_grad, use_container_width=True, num_rows="dynamic", key="tp_grad_editor")
                outcomes_final = []
                for _, row in df_grad_edited.iterrows():
                    if str(row["ç¼–å·"]).strip():
                        outcomes_final.append({
                            "no": str(row["ç¼–å·"]).strip(),
                            "title": str(row["æ ‡é¢˜"]).strip(),
                            "body": str(row["å†…å®¹"]).strip()
                        })
            else:
                st.info("æœªè¯†åˆ«åˆ°æ¯•ä¸šè¦æ±‚ï¼Œè¯·æ‰‹å·¥å½•å…¥")
                grad_json = st.text_area(
                    "æ¯•ä¸šè¦æ±‚ JSON",
                    value=json.dumps([{"no": "1", "title": "å·¥ç¨‹çŸ¥è¯†", "body": ""}], ensure_ascii=False, indent=2),
                    height=160,
                    key="tp_grad_json",
                )
                try:
                    outcomes_final = json.loads(grad_json) if grad_json.strip() else []
                except Exception:
                    outcomes_final = []
            
            st.markdown("#### 3) æŠ½å–çš„è¡¨æ ¼ï¼ˆå¯ç¼–è¾‘ç¡®è®¤ï¼‰")
            tables = extract_result.get("tables", [])
            confirmed_tables = []
            
            if tables:
                for i, table_info in enumerate(tables[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªè¡¨æ ¼
                    st.markdown(f"**è¡¨æ ¼{i+1}ï¼ˆç¬¬{table_info.get('page', '?')}é¡µï¼‰**")
                    
                    # ç¡®ä¿DataFrameæœ‰æ­£ç¡®çš„åˆ—å
                    try:
                        df = safe_df_from_tablepack(table_info)
                        if not df.empty:
                            # ä½¿ç”¨st.data_editor
                            df_edited = st.data_editor(df, use_container_width=True, key=f"tp_table_{i}")
                            
                            confirm_table = st.checkbox(f"ç¡®è®¤é‡‡ç”¨æ­¤è¡¨æ ¼", value=True, key=f"tp_table_confirm_{i}")
                            if confirm_table:
                                confirmed_tables.append({
                                    "page": table_info.get("page", 0),
                                    "title": table_info.get("title", ""),
                                    "data": df_edited.values.tolist(),
                                    "columns": df_edited.columns.tolist()
                                })
                    except Exception as e:
                        st.error(f"è¡¨æ ¼{i+1}æ˜¾ç¤ºé”™è¯¯: {str(e)}")
                        # æ˜¾ç¤ºåŸå§‹æ•°æ®
                        st.write("åŸå§‹æ•°æ®:", table_info.get("data", []))
            else:
                st.info("æœªæŠ½å–åˆ°è¡¨æ ¼")
            
            st.markdown("#### 4) ç« èŠ‚ç»“æ„")
            sections = extract_result.get("sections", {})
            with st.expander("æŸ¥çœ‹ç« èŠ‚ç»“æ„", expanded=False):
                for section_name, section_content in list(sections.items())[:10]:  # æ˜¾ç¤ºå‰10ä¸ªç« èŠ‚
                    st.markdown(f"**{section_name}**")
                    st.text(section_content[:500] + "..." if len(section_content) > 500 else section_content)
            
            st.markdown("---")
            if st.button("âœ… ç¡®è®¤å¹¶ä¿å­˜ä¸ºåŸ¹å…»æ–¹æ¡ˆåº•åº§", type="primary", disabled=not confirmed_flag):
                # æ„å»ºcontent_json
                content_json = {
                    "source": ex["source"],
                    "confirmed": True,
                    "major": major2,
                    "grade": grade2,
                    "course_group": course_group2,
                    "goals": goals_final,
                    "outcomes": outcomes_final,
                    "tables": confirmed_tables,
                    "extract_metadata": {
                        "page_count": extract_result.get("page_count", 0),
                        "table_count": extract_result.get("table_count", 0),
                        "sections_count": len(sections),
                        "extracted_at": extract_result.get("extracted_at", "")
                    },
                    "full_extract": extract_result  # ä¿å­˜å®Œæ•´çš„æŠ½å–ç»“æœ
                }
                
                # ç”Ÿæˆmarkdown
                md = f"# åŸ¹å…»æ–¹æ¡ˆï¼ˆPDFæŠ½å–-å·²ç¡®è®¤ï¼‰\n\n"
                md += f"- ä¸“ä¸šï¼š{major2}\n- å¹´çº§ï¼š{grade2}\n- è¯¾ç¨‹ä½“ç³»/æ–¹å‘ï¼š{course_group2}\n\n"
                md += "## ä¸€ã€åŸ¹å…»ç›®æ ‡ï¼ˆç¡®è®¤ç‰ˆï¼‰\n" + ("\n".join([f"- {x}" for x in goals_final]) if goals_final else "- ï¼ˆæœªå¡«ï¼‰") + "\n\n"
                md += "## äºŒã€æ¯•ä¸šè¦æ±‚ï¼ˆç¡®è®¤ç‰ˆï¼‰\n" + ("\n".join([f"- {o.get('no','')}. {o.get('title','')}: {o.get('body','')}" for o in outcomes_final]) if outcomes_final else "- ï¼ˆæœªå¡«ï¼‰") + "\n\n"
                md += "## ä¸‰ã€æŠ½å–è¡¨æ ¼ï¼ˆå…±{}ä¸ªï¼‰\n".format(len(confirmed_tables))
                for i, tbl in enumerate(confirmed_tables, 1):
                    md += f"- è¡¨æ ¼{i}ï¼ˆç¬¬{tbl['page']}é¡µï¼‰: {tbl['title']}\n"
                md += "\n## å››ã€ç« èŠ‚ç»“æ„\n"
                for section_name in list(sections.keys())[:5]:
                    md += f"- {section_name}\n"
                
                title = f"åŸ¹å…»æ–¹æ¡ˆï¼ˆPDFæŠ½å–ç¡®è®¤ç‰ˆï¼‰-{ex['source']}"
                a2 = upsert_artifact(project_id, "training_plan", title, md, content_json, [], note="pdf-extract-confirm")
                st.success("å·²ä¿å­˜'ç¡®è®¤ç‰ˆåŸ¹å…»æ–¹æ¡ˆåº•åº§'ã€‚åç»­ç”Ÿæˆå¤§çº²ä¼šä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–å­—æ®µã€‚")
                st.session_state.pop("tp_extract", None)
                st.rerun()
            
            if st.button("æ¸…é™¤æœ¬æ¬¡æŠ½å–ç»“æœï¼ˆä¸ä¿å­˜ï¼‰"):
                st.session_state.pop("tp_extract", None)
                st.info("å·²æ¸…é™¤ã€‚")
    
    with tab2:
        if not a:
            st.info("æš‚æ— åŸ¹å…»æ–¹æ¡ˆã€‚è¯·å…ˆç”Ÿæˆæˆ–ä¸Šä¼ å¹¶ç¡®è®¤ã€‚")
        else:
            artifact_toolbar(a)
            st.markdown("#### ç»“æ„åŒ–å†…å®¹")
            st.json(a.get("content_json") or {})
            st.markdown("#### Markdowné¢„è§ˆ")
            st.markdown(a["content_md"][:2000] + "..." if len(a["content_md"]) > 2000 else a["content_md"])
    
    with tab3:
        if not a:
            st.info("æš‚æ— åŸ¹å…»æ–¹æ¡ˆã€‚è¯·å…ˆç”Ÿæˆæˆ–ä¸Šä¼ ã€‚")
        else:
            edited = md_textarea("åœ¨çº¿ç¼–è¾‘åŸ¹å…»æ–¹æ¡ˆï¼ˆæ”¯æŒç›´æ¥ä¿®æ”¹ï¼‰", a["content_md"], key="tp_edit")
            note = st.text_input("ä¿å­˜è¯´æ˜ï¼ˆå¯é€‰ï¼‰", value="edit", key="tp_note")
            if st.button("ä¿å­˜ä¿®æ”¹ï¼ˆç”Ÿæˆæ–°ç‰ˆæœ¬ï¼‰", type="primary", key="tp_save"):
                a2 = upsert_artifact(project_id, "training_plan", a["title"], edited, a["content_json"], [], note=note)
                st.success("å·²ä¿å­˜ã€‚åç»­ä¾èµ–æ–‡ä»¶å°†å¼•ç”¨æ›´æ–°åçš„åŸ¹å…»æ–¹æ¡ˆã€‚")
                st.rerun()
    
    with tab4:
        if not a:
            st.info("æš‚æ— åŸ¹å…»æ–¹æ¡ˆã€‚")
        else:
            vers = get_versions(a["id"])
            st.markdown("#### ç‰ˆæœ¬è®°å½•")
            st.dataframe(vers if vers else [], use_container_width=True)
    
    with tab5:
        # å®Œæ•´çš„PDFå…¨é‡æŠ½å–ç•Œé¢
        st.markdown("### PDFå…¨é‡æŠ½å–ç‹¬ç«‹ç•Œé¢")
        st.caption("è¿™æ˜¯å®Œæ•´çš„PDFæŠ½å–ç•Œé¢ï¼ŒåŒ…å«æ‰€æœ‰æŠ½å–ç»“æœçš„å±•ç¤ºå’Œç¼–è¾‘åŠŸèƒ½")
        
        if "extract_result" not in st.session_state:
            st.session_state["extract_result"] = None
        
        uploaded = st.file_uploader("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDF", type=["pdf"], key="full_extract_upload")
        use_ocr = st.checkbox("å¯¹æ— æ–‡æœ¬é¡µå¯ç”¨ OCRï¼ˆå¯é€‰ï¼‰", value=False, key="full_extract_ocr")
        
        if uploaded and st.button("å¼€å§‹å…¨é‡æŠ½å–", type="primary", key="full_extract_btn"):
            pdf_bytes = uploaded.getvalue()
            with st.spinner("æ­£åœ¨æŠ½å–â€¦"):
                extract_result = run_full_extract(pdf_bytes, use_ocr=use_ocr)
                st.session_state["extract_result"] = extract_result
        
        result = st.session_state.get("extract_result")
        if result is None:
            st.stop()
        
        # æ¦‚è§ˆæŒ‡æ ‡
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("æ€»é¡µæ•°", result.page_count)
        c2.metric("è¡¨æ ¼æ€»æ•°", result.table_count)
        c3.metric("OCRå¯ç”¨", "æ˜¯" if result.ocr_used else "å¦")
        c4.caption(f"SHA256: {result.file_sha256[:16]}...")
        
        tabs_full = st.tabs(["æ¦‚è§ˆä¸ä¸‹è½½", "ç« èŠ‚å¤§æ ‡é¢˜", "åŸ¹å…»ç›®æ ‡", "æ¯•ä¸šè¦æ±‚", "é™„è¡¨è¡¨æ ¼", "åˆ†é¡µåŸæ–‡"])
        
        with tabs_full[0]:
            st.markdown("### ç»“æ„åŒ–è¯†åˆ«ç»“æœï¼ˆå¯å…ˆåœ¨è¿™é‡Œæ ¡å¯¹ï¼‰")
            
            # ä¸‹è½½ JSONï¼ˆå…¨é‡ï¼‰
            json_bytes = json.dumps(asdict(result), ensure_ascii=False, indent=2).encode("utf-8")
            st.download_button(
                "ä¸‹è½½æŠ½å–ç»“æœ JSON",
                data=json_bytes,
                file_name="training_plan_full_extract.json",
                mime="application/json",
                use_container_width=True,
            )
            
            if result.tables:
                zip_bytes = make_tables_zip(result.tables)
                st.download_button(
                    "ä¸‹è½½è¡¨æ ¼ ZIP",
                    data=zip_bytes,
                    file_name="training_plan_tables.zip",
                    mime="application/zip",
                    use_container_width=True,
                )
            
            st.markdown("#### é™„è¡¨æ ‡é¢˜æ˜ å°„")
            if result.appendix_titles:
                st.json(result.appendix_titles)
            else:
                st.info("æœªæ£€æµ‹åˆ°é™„è¡¨æ ‡é¢˜æ˜ å°„ã€‚")
        
        with tabs_full[1]:
            st.markdown("### ç« èŠ‚å¤§æ ‡é¢˜")
            for k in result.sections.keys():
                with st.expander(k, expanded=False):
                    st.text(result.sections.get(k, ""))
        
        with tabs_full[2]:
            st.markdown("### åŸ¹å…»ç›®æ ‡")
            obj = result.training_objectives
            st.write(f"è¯†åˆ«æ¡ç›®æ•°ï¼š**{obj.get('count', 0)}**")
            st.text_area("åŸ¹å…»ç›®æ ‡ï¼ˆé€æ¡ï¼‰", value="\n".join(obj.get("items", [])), height=220, key="full_obj")
            with st.expander("åŸå§‹æ–‡æœ¬"):
                st.text(obj.get("raw", ""))
        
        with tabs_full[3]:
            st.markdown("### æ¯•ä¸šè¦æ±‚ï¼ˆ12æ¡ + åˆ†é¡¹ï¼‰")
            grad = result.graduation_requirements
            st.write(f"è¯†åˆ«ä¸»æ¡ç›®æ•°ï¼š**{grad.get('count', 0)}**")
            
            items = grad.get("items", [])
            if not items:
                st.warning("æœªè¯†åˆ«åˆ°æ¯•ä¸šè¦æ±‚")
            else:
                for it in items:
                    no = it.get("no")
                    title = it.get("title") or ""
                    body = it.get("body") or ""
                    header = f"{no}. {title}".strip()
                    with st.expander(header, expanded=(no in [1, 2])):
                        st.write(body)
                        subs = it.get("subitems", [])
                        if subs:
                            st.markdown("**åˆ†é¡¹ï¼š**")
                            for s in subs:
                                st.write(f"- {s.get('no')}: {s.get('body')}")
            with st.expander("åŸå§‹æ–‡æœ¬"):
                st.text(grad.get("raw", ""))
        
        with tabs_full[4]:
            st.markdown("### é™„è¡¨è¡¨æ ¼")
            if not result.tables:
                st.info("æœªæ£€æµ‹åˆ°è¡¨æ ¼ã€‚")
            else:
                all_dirs = sorted({clean_text(t.get("direction") or "") for t in result.tables if clean_text(t.get("direction") or "")})
                opt_dirs = ["å…¨éƒ¨"] + all_dirs
                sel = st.selectbox("æ–¹å‘è¿‡æ»¤", opt_dirs, index=0, key="full_dir_filter")
                
                for t in result.tables:
                    direction = clean_text(t.get("direction") or "")
                    if sel != "å…¨éƒ¨" and direction != sel:
                        continue
                    
                    st.subheader(f"ç¬¬{t.get('page')}é¡µï½œ{t.get('title')}")
                    if direction:
                        st.caption(f"é¡µé¢æ–¹å‘æç¤ºï¼š{direction}")
                    
                    df = safe_df_from_tablepack(t)
                    st.dataframe(df, use_container_width=True, hide_index=True)
        
        with tabs_full[5]:
            st.markdown("### åˆ†é¡µåŸæ–‡ä¸è¡¨æ ¼")
            for page_data in result.pages_data:
                page_no = page_data["page"]
                page_text = page_data["text"]
                page_tables = page_data["tables"]
                
                with st.expander(f"ç¬¬{page_no}é¡µï¼ˆ{len(page_tables)}ä¸ªè¡¨æ ¼ï¼‰", expanded=False):
                    st.text(page_text)
                    
                    if page_tables:
                        st.markdown(f"**è¡¨æ ¼ ({len(page_tables)}ä¸ª):**")
                        for i, table_data in enumerate(page_tables, start=1):
                            df = table_to_df(table_data)
                            if not df.empty:
                                st.markdown(f"**è¡¨æ ¼ {i}:**")
                                st.dataframe(df, use_container_width=True)
                            else:
                                st.info(f"è¡¨æ ¼ {i} ä¸ºç©º")

# å…¶ä»–é¡µé¢å‡½æ•°ï¼ˆç®€åŒ–å®ç°ï¼‰
def page_syllabus():
    ensure_project()
    render_depbar(project_id, "syllabus")
    tp = get_artifact(project_id, "training_plan")
    a = get_artifact(project_id, "syllabus")
    
    st.markdown("### è¯¾ç¨‹æ•™å­¦å¤§çº²")
    
    if not tp:
        st.warning("è¯·å…ˆåˆ›å»ºåŸ¹å…»æ–¹æ¡ˆ")
    else:
        st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_calendar():
    ensure_project()
    render_depbar(project_id, "calendar")
    st.markdown("### æ•™å­¦æ—¥å†")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_lesson_plan():
    ensure_project()
    render_depbar(project_id, "lesson_plan")
    st.markdown("### æ•™æ¡ˆ")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_assessment():
    ensure_project()
    render_depbar(project_id, "assessment")
    st.markdown("### ä½œä¸š/é¢˜åº“/è¯•å·æ–¹æ¡ˆ")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_review():
    ensure_project()
    render_depbar(project_id, "review")
    st.markdown("### å®¡æ ¸è¡¨")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_report():
    ensure_project()
    render_depbar(project_id, "report")
    st.markdown("### è¯¾ç¨‹ç›®æ ‡è¾¾æˆæŠ¥å‘Š")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_manual():
    ensure_project()
    render_depbar(project_id, "manual")
    st.markdown("### æˆè¯¾æ‰‹å†Œ")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_evidence():
    ensure_project()
    render_depbar(project_id, "evidence")
    st.markdown("### è¯¾å ‚çŠ¶æ€ä¸è¿‡ç¨‹è¯æ®")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_vge():
    ensure_project()
    st.markdown("### è¯æ®é“¾ä¸å¯éªŒè¯ç”Ÿæˆï¼ˆVGEï¼‰")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_dep_graph():
    ensure_project()
    st.markdown("### ä¾èµ–å›¾å¯è§†åŒ–")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")

def page_docx_export():
    ensure_project()
    st.markdown("### æ¨¡æ¿åŒ–DOCXå¯¼å‡º")
    st.info("åŠŸèƒ½å¼€å‘ä¸­...")
    
    
# -*- coding: utf-8 -*-
"""
æ•™å­¦æ™ºèƒ½ä½“å¹³å° - æ•´åˆPDFå…¨é‡æŠ½å–ç‰ˆï¼ˆå¢å¼ºç‰ˆï¼‰
æ•´åˆäº†å®Œæ•´çš„PDFè§£æèƒ½åŠ›å’Œæ•™å­¦æ–‡æ¡£é“¾ç®¡ç†ï¼Œç¡®ä¿å¯¹åŸ¹å…»æ–¹æ¡ˆPDFçš„æ‰€æœ‰ç« èŠ‚å’Œè¡¨æ ¼å®Œæ•´æå–å’Œæ˜¾ç¤ºã€‚
"""

import os
import io
import re
import json
import time
import base64
import hashlib
import sqlite3
import zipfile
import threading
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple
import pandas as pd
import streamlit as st
from dataclasses import asdict, dataclass

# -------- å¯é€‰è§£æä¾èµ– --------
try:
    import pdfplumber
except Exception:
    pdfplumber = None
    st.error("ç¼ºå°‘ä¾èµ– pdfplumberï¼Œè¯·å®‰è£…ï¼špip install pdfplumber")

try:
    from docx import Document
except Exception:
    Document = None

# ---------------------------
# åŸºç¡€é…ç½®
# ---------------------------
st.set_page_config(page_title="æ•™å­¦æ™ºèƒ½ä½“å¹³å°", layout="wide")

DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)
DB_PATH = os.path.join(DATA_DIR, "app.db")

_DB_LOCK = threading.Lock()

# ---------------------------
# UI ç¾åŒ–ï¼ˆCSSï¼‰
# ---------------------------
def inject_css():
    st.markdown(
        """
<style>
.main .block-container {
    padding-top: 1.0rem;
    padding-bottom: 2rem;
    max-width: 100% !important;
    padding-left: 2rem;
    padding-right: 2rem;
}
h1, h2, h3 { letter-spacing: .2px; }
code { font-size: 0.9em; }

.topbar{
    padding: 18px 18px;
    border-radius: 18px;
    background: linear-gradient(90deg, #0ea5e9 0%, #6366f1 55%, #8b5cf6 100%);
    color: white;
    box-shadow: 0 8px 24px rgba(0,0,0,.12);
}
.topbar .title{ font-size: 30px; font-weight: 800; }
.topbar .sub{ opacity: .9; margin-top: 6px; font-size: 14px; }

.card{
    border: 1px solid rgba(0,0,0,.08);
    border-radius: 18px;
    padding: 16px 16px;
    background: rgba(255,255,255,.6);
    box-shadow: 0 6px 16px rgba(0,0,0,.06);
}
.badge{
    display:inline-block; padding: 2px 10px; border-radius: 999px;
    font-size: 12px; border: 1px solid rgba(0,0,0,.12); margin-right: 6px;
}
.badge.ok { background:#ecfdf5; color:#065f46; border-color:#a7f3d0; }
.badge.warn { background:#fffbeb; color:#92400e; border-color:#fde68a; }
.badge.bad { background:#fef2f2; color:#991b1b; border-color:#fecaca; }

.depbar{ display:flex; gap:8px; flex-wrap: wrap; padding: 10px 0; }
.depitem{
    padding: 8px 10px; border-radius: 14px; border: 1px solid rgba(0,0,0,.10);
    background: rgba(255,255,255,.7); font-size: 13px;
}
.depitem b{ margin-right:6px; }

.docbox{
    border: 1px solid rgba(0,0,0,.10);
    border-radius: 18px;
    padding: 14px 16px;
    background: rgba(255,255,255,.75);
    line-height: 1.55;
    white-space: normal;
}
section[data-testid="stSidebar"] .stMarkdown h2{ font-size: 18px; font-weight: 800; }
div[data-testid="stDataFrame"] { border-radius: 14px; overflow:hidden; }

/* ç¡®ä¿è¡¨æ ¼åˆ—åæœ‰æ•ˆ */
.stDataFrame th {
    font-weight: 600 !important;
}
</style>
""",
        unsafe_allow_html=True,
    )

inject_css()

# ---------------------------
# PDFå…¨é‡æŠ½å–æ ¸å¿ƒåŠŸèƒ½ï¼ˆå¢å¼ºç‰ˆï¼šç¡®ä¿æ‰€æœ‰ç« èŠ‚å’Œè¡¨æ ¼å®Œæ•´æå–ï¼‰
# ---------------------------
def sha256_bytes(data: bytes) -> str:
    h = hashlib.sha256()
    h.update(data)
    return h.hexdigest()

def clean_text(s: str) -> str:
    if s is None:
        return ""
    s = str(s)
    s = s.replace("\u00a0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    return s.strip()

def normalize_multiline(text: str) -> str:
    """ä¿ç•™æ¢è¡Œï¼ŒåšåŸºç¡€æ¸…ç†ï¼Œä¾¿äºæ­£åˆ™åˆ†æ®µã€‚"""
    if text is None:
        return ""
    text = str(text).replace("\r\n", "\n").replace("\r", "\n")
    lines = [clean_text(ln) for ln in text.split("\n")]
    out: List[str] = []
    blank = 0
    for ln in lines:
        if ln.strip() == "":
            blank += 1
            if blank <= 2:
                out.append("")
        else:
            blank = 0
            out.append(ln)
    return "\n".join(out).strip()

def make_unique_columns(cols: List[str]) -> List[str]:
    seen: Dict[str, int] = {}
    out: List[str] = []
    for c in cols:
        c0 = clean_text(c) or "col"
        if c0 not in seen:
            seen[c0] = 1
            out.append(c0)
        else:
            seen[c0] += 1
            out.append(f"{c0}_{seen[c0]}")
    return out

def postprocess_table_df(df: pd.DataFrame) -> pd.DataFrame:
    """è¡¨æ ¼åå¤„ç†ï¼šå»ç©ºç™½ã€å» NaNã€åˆå¹¶æ ¼é€ æˆçš„ç©ºç™½åšå‘ä¸‹å¡«å……ã€‚"""
    if df is None or df.empty:
        return df

    df = df.copy()
    df = df.replace({None: ""}).fillna("")
    for c in df.columns:
        df[c] = df[c].astype(str).map(lambda x: clean_text(x))

    # 1) åˆ é™¤å®Œå…¨ç©ºè¡Œ
    mask_all_empty = df.apply(lambda r: all((clean_text(x) == "" for x in r.values.tolist())), axis=1)
    df = df.loc[~mask_all_empty].reset_index(drop=True)

    # 2) å‘ä¸‹å¡«å……ï¼ˆåˆå¹¶æ ¼å¸¸è§åˆ—ï¼‰
    fill_down_keywords = ["è¯¾ç¨‹ä½“ç³»", "è¯¾ç¨‹æ¨¡å—", "è¯¾ç¨‹æ€§è´¨", "è¯¾ç¨‹ç±»åˆ«", "ç±»åˆ«", "æ¨¡å—", "ç¯èŠ‚", "å­¦æœŸ", "æ–¹å‘"]
    for c in df.columns:
        if any(k in str(c) for k in fill_down_keywords):
            last = ""
            new_col = []
            for v in df[c].tolist():
                if v != "":
                    last = v
                    new_col.append(v)
                else:
                    new_col.append(last)
            df[c] = new_col

    return df

def normalize_table(raw_table: List[List[Any]]) -> List[List[str]]:
    """
    pdfplumber.extract_tables() è¿”å› list[list[str|None]]
    è¿™é‡ŒåšåŸºç¡€æ¸…æ´—ï¼šå»ç©ºè¡Œã€è¡¥é½åˆ—æ•°ã€å»æ‰å…¨ç©ºåˆ—
    """
    if not raw_table:
        return []

    rows = []
    max_cols = 0
    for r in raw_table:
        if r is None:
            continue
        rr = [clean_text(c) for c in r]
        # è·³è¿‡å…¨ç©ºè¡Œ
        if all(c == "" for c in rr):
            continue
        rows.append(rr)
        max_cols = max(max_cols, len(rr))

    if not rows or max_cols == 0:
        return []

    # è¡¥é½åˆ—æ•°
    for i in range(len(rows)):
        if len(rows[i]) < max_cols:
            rows[i] = rows[i] + [""] * (max_cols - len(rows[i]))

    # å»æ‰å…¨ç©ºåˆ—
    keep_cols = []
    for j in range(max_cols):
        col = [rows[i][j] for i in range(len(rows))]
        if any(c != "" for c in col):
            keep_cols.append(j)

    if not keep_cols:
        return []

    cleaned = [[row[j] for j in keep_cols] for row in rows]
    return cleaned

def table_to_df(cleaned_table: List[List[str]]) -> pd.DataFrame:
    """
    å°è¯•æŠŠç¬¬ä¸€è¡Œå½“è¡¨å¤´ï¼›å¦‚æœè¡¨å¤´å¤ªå·®å°±ç”¨é»˜è®¤åˆ—åã€‚
    """
    if not cleaned_table or len(cleaned_table) == 0:
        return pd.DataFrame()
    
    if len(cleaned_table) == 1:
        # åªæœ‰ä¸€è¡Œï¼Œåšå•è¡Œdf
        return pd.DataFrame([cleaned_table[0]])

    header = cleaned_table[0]
    body = cleaned_table[1:]

    # è¡¨å¤´åˆ¤å®šï¼šè‡³å°‘æœ‰ä¸€åŠå•å…ƒæ ¼éç©º
    non_empty = sum(1 for x in header if clean_text(x) != "")
    if non_empty >= max(1, len(header) // 2):
        cols = [h if h else f"col_{i+1}" for i, h in enumerate(header)]
        df = pd.DataFrame(body, columns=cols)
    else:
        # å¦åˆ™ä¸ç”¨è¡¨å¤´
        df = pd.DataFrame(cleaned_table)

    return postprocess_table_df(df)

# ----------------------------
# PDF æŠ½å–ï¼šæ–‡æœ¬ + è¡¨æ ¼ (ä½¿ç”¨ pdfplumber çš„è¡¨æ ¼æå–ï¼Œå¢å¼ºè®¾ç½®ä»¥æ•è·æ‰€æœ‰è¡¨æ ¼)
# ----------------------------
def extract_pages_text_and_tables(pdf_bytes: bytes, enable_ocr: bool = False) -> Tuple[List[Dict[str, Any]], str]:
    """
    æå–æ¯é¡µçš„æ–‡æœ¬å’Œè¡¨æ ¼ï¼Œç¡®ä¿æ‰€æœ‰é¡µé¢çš„å†…å®¹å®Œæ•´æå–
    è¿”å›ï¼šé¡µé¢æ•°æ®åˆ—è¡¨ï¼ˆå«æ–‡æœ¬å’Œè¡¨æ ¼ï¼‰ï¼Œå…¨æ–‡æ–‡æœ¬
    """
    if pdfplumber is None:
        return [], ""
    
    pages_data = []
    full_text_parts = []
    
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        # è¡¨æ ¼è®¾ç½®ï¼šæ›´å®½æ¾è®¾ç½®ï¼Œç¡®ä¿æ•è·å¤æ‚è¡¨æ ¼ï¼ŒåŒ…æ‹¬è·¨é¡µè¡¨æ ¼
        table_settings = {
            "vertical_strategy": "lines_strict",
            "horizontal_strategy": "lines_strict",
            "intersection_tolerance": 10,
            "snap_tolerance": 5,
            "join_tolerance": 5,
            "edge_min_length": 2,
            "min_words_vertical": 1,
            "min_words_horizontal": 1,
            "text_tolerance": 3,
            "keep_blank_chars": True,
            "text_x_tolerance": 2,
            "text_y_tolerance": 2
        }
        
        for idx, page in enumerate(pdf.pages, start=1):
            # æå–æ–‡æœ¬ï¼ˆå¢å¼ºï¼šä½¿ç”¨layoutæ¨¡å¼ä»¥ä¿ç•™ç»“æ„ï¼‰
            text = page.extract_text(layout=True) or ""
            text = normalize_multiline(text)
            
            # å¦‚æœéœ€è¦OCRä¸”æ–‡æœ¬å¤ªå°‘
            if enable_ocr and len(text) < 50:
                try:
                    import pytesseract
                    from PIL import Image
                    img = page.to_image(resolution=300).original  # æé«˜åˆ†è¾¨ç‡
                    ocr_text = pytesseract.image_to_string(img, lang="chi_sim+eng")
                    if len(ocr_text) > len(text):
                        text = normalize_multiline(ocr_text)
                except Exception:
                    pass
            
            full_text_parts.append(text)
            
            # æå–è¡¨æ ¼ï¼ˆå¢å¼ºï¼šå°è¯•å¤šæ¬¡è®¾ç½®ä»¥æ•è·æ‰€æœ‰ï¼‰
            raw_tables = []
            try:
                raw_tables = page.extract_tables(table_settings=table_settings) or []
            except Exception:
                # å¤‡ç”¨è®¾ç½®
                alt_settings = {"vertical_strategy": "text", "horizontal_strategy": "text"}
                raw_tables = page.extract_tables(table_settings=alt_settings) or []
            
            # æ¸…æ´—è¡¨æ ¼
            cleaned_tables = []
            for t in raw_tables:
                ct = normalize_table(t)
                if ct:
                    cleaned_tables.append(ct)
            
            pages_data.append({
                "page": idx,
                "text": text,
                "tables": cleaned_tables,
                "tables_count": len(cleaned_tables)
            })
    
    full_text = "\n".join(full_text_parts)
    return pages_data, full_text

# ----------------------------
# ç»“æ„åŒ–è§£æï¼šç« èŠ‚/æ¯•ä¸šè¦æ±‚/åŸ¹å…»ç›®æ ‡/é™„è¡¨æ ‡é¢˜ï¼ˆå¢å¼ºï¼šæ•è·æ‰€æœ‰ç« èŠ‚ï¼ŒåŒ…æ‹¬ä¸ƒåˆ°åä¸€ï¼‰
# ----------------------------
def split_sections(full_text: str) -> Dict[str, str]:
    """
    æŒ‰ "ä¸€ã€/äºŒã€/ä¸‰ã€..." å¤§ç« åˆ‡åˆ†ï¼Œç¡®ä¿æ•è·æ‰€æœ‰ï¼ŒåŒ…æ‹¬ä¸ƒã€å…«ã€ä¹ã€åã€åä¸€ç­‰ã€‚
    å…¼å®¹ï¼šä¸‰ã€ / ä¸‰. / ä¸‰ï¼ / åä¸€ã€
    """
    text = normalize_multiline(full_text)
    lines = text.splitlines()
    pat = re.compile(r"^\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s*[ã€\.ï¼]\s*([^\n\r]+?)\s*$")  # æ”¯æŒåä¸€ç­‰

    sections: Dict[str, List[str]] = {}
    cur_key = "å°é¢/å‰è¨€"

    for ln in lines:
        m = pat.match(ln)
        if m:
            num = m.group(1)
            title = clean_text(m.group(2))
            cur_key = f"{num}ã€{title}"
            sections.setdefault(cur_key, [])
        else:
            sections.setdefault(cur_key, []).append(ln)

    return {k: "\n".join(v).strip() for k, v in sections.items()}

def extract_appendix_titles(full_text: str) -> Dict[str, str]:
    """æŠ½å–"é™„è¡¨X -> æ ‡é¢˜ï¼ˆå¯èƒ½å«ä¸ƒã€å…«â€¦ï¼‰"""
    titles: Dict[str, str] = {}
    text = normalize_multiline(full_text)
    for raw in text.splitlines():
        line = raw.strip()
        if not line:
            continue

        # 1) é™„è¡¨1ï¼šXXXX
        m = re.search(r"(é™„è¡¨\s*\d+)\s*[:ï¼š]\s*(.+)$", line)
        if m:
            key = re.sub(r"\s+", "", m.group(1))
            val = clean_text(m.group(2))
            if val:
                titles[key] = val
            continue

        # 2) ä¸ƒã€XXXXï¼ˆé™„è¡¨1ï¼‰
        m = re.search(r"^(?P<title>.+?)\s*[ï¼ˆ(]\s*(?P<key>é™„è¡¨\s*\d+)\s*[)ï¼‰]\s*$", line)
        if m:
            key = re.sub(r"\s+", "", m.group("key"))
            val = clean_text(m.group("title"))
            if val:
                titles[key] = val
            continue

        # 3) è¡Œå†…å‡ºç°ï¼ˆé™„è¡¨Xï¼‰
        m = re.search(r"(?P<title>.+?)\s*[ï¼ˆ(]\s*(?P<key>é™„è¡¨\s*\d+)\s*[)ï¼‰]", line)
        if m:
            key = re.sub(r"\s+", "", m.group("key"))
            val = clean_text(m.group("title"))
            if val and key not in titles:
                titles[key] = val

    return titles

def parse_training_objectives(section_text: str) -> Dict[str, Any]:
    """
    æå–"åŸ¹å…»ç›®æ ‡"æ¡ç›®ã€‚è¿”å› items(list[str]) + rawã€‚
    å°½é‡åŒ…å®¹ï¼š1) / 1ï¼ / 1ã€ / ï¼ˆ1ï¼‰ç­‰ã€‚
    """
    raw = normalize_multiline(section_text)
    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
    items: List[str] = []

    pat = re.compile(r"^(?:ï¼ˆ?\s*\d+\s*ï¼‰?|\d+\s*[\.ã€ï¼])\s*(.+)$")
    for ln in lines:
        m = pat.match(ln)
        if m:
            body = clean_text(m.group(1))
            if body:
                items.append(body)

    # å¦‚æœæ²¡æŠ“åˆ°ç¼–å·æ¡ç›®ï¼Œé€€åŒ–ï¼šå–å‰è‹¥å¹²è¡Œï¼ˆä¸ä¸¢ä¿¡æ¯ï¼‰
    if not items:
        items = lines[:30]

    return {"count": len(items), "items": items, "raw": raw}

def parse_graduation_requirements(text_any: str) -> Dict[str, Any]:
    """
    æŠ½å– 12 æ¡æ¯•ä¸šè¦æ±‚åŠå…¶åˆ†é¡¹ 1.1/1.2â€¦
    è¿”å›ç»“æ„ï¼š{"count":..,"items":[{"no":1,"title":"å·¥ç¨‹çŸ¥è¯†","body":"...","subitems":[...]}], "raw":...}
    """
    text = normalize_multiline(text_any or "")

    # å®šä½"äºŒã€æ¯•ä¸šè¦æ±‚"
    start = re.search(r"(?m)^\s*(äºŒ\s*[ã€\.ï¼]?\s*æ¯•ä¸šè¦æ±‚|æ¯•ä¸šè¦æ±‚)\s*$", text)
    if start:
        tail = text[start.start():]
    else:
        tail = text

    # æˆªæ–­åˆ°ä¸‹ä¸€å¤§ç« 
    end = re.search(r"(?m)^\s*[ä¸‰å››äº”å…­ä¸ƒå…«ä¹ååä¸€åäºŒ]\s*[ã€\.ï¼]", tail)  # æ‰©å±•åˆ°åäºŒ
    if end:
        tail = tail[:end.start()]

    lines = [ln.strip() for ln in tail.splitlines()]

    main_pat = re.compile(r"^(?P<no>\d{1,2})\s*[\.ã€](?!\d)\s*(?P<body>.+)$")   # 1. xxx (æ’é™¤ 1.1)
    sub_pat = re.compile(r"^(?P<no>\d{1,2}\.\d{1,2})\s+(?P<body>.+)$")       # 1.1 xxx

    items: List[Dict[str, Any]] = []
    cur: Optional[Dict[str, Any]] = None
    cur_sub: Optional[Dict[str, Any]] = None

    def flush_sub():
        nonlocal cur_sub, cur
        if cur is not None and cur_sub is not None:
            cur.setdefault("subitems", []).append(cur_sub)
        cur_sub = None

    def flush_item():
        nonlocal cur
        if cur is not None:
            cur["title"] = clean_text(cur.get("title", ""))
            cur["body"] = clean_text(cur.get("body", ""))
            for s in cur.get("subitems", []):
                s["body"] = clean_text(s.get("body", ""))
            items.append(cur)
        cur = None

    for ln in lines:
        if not ln:
            continue

        m_main = main_pat.match(ln)
        m_sub = sub_pat.match(ln)

        if m_main:
            flush_sub()
            flush_item()
            no = int(m_main.group("no"))
            body_full = clean_text(m_main.group("body"))

            # å¤„ç†"å·¥ç¨‹çŸ¥è¯†ï¼š..."è¿™ç§
            title = ""
            body = body_full
            if "ï¼š" in body_full:
                title, body = body_full.split("ï¼š", 1)
                title = clean_text(title)
                body = clean_text(body)

            cur = {"no": no, "title": title, "body": body, "subitems": []}
            continue

        if m_sub and cur is not None:
            flush_sub()
            cur_sub = {"no": m_sub.group("no"), "body": clean_text(m_sub.group("body"))}
            continue

        # ç»­è¡Œ
        if cur_sub is not None:
            cur_sub["body"] += " " + ln
        elif cur is not None:
            cur["body"] += " " + ln

    flush_sub()
    flush_item()

    items = sorted(items, key=lambda x: x.get("no", 999))
    if len(items) > 12:
        items = [x for x in items if 1 <= x.get("no", 0) <= 12]

    return {"count": len(items), "items": items, "raw": tail.strip()}

# ----------------------------
# è¡¨æ ¼æ ‡é¢˜/æ–¹å‘è¯†åˆ«ï¼ˆå¢å¼ºï¼šæ‰©å±•é¡µæ˜ å°„åˆ°18é¡µï¼Œç¡®ä¿æ‰€æœ‰é™„è¡¨ï¼‰
# ----------------------------
def guess_table_appendix_by_page(page_no: int) -> Optional[str]:
    """
    é’ˆå¯¹å¸¸è§åŸ¹å…»æ–¹æ¡ˆï¼ˆ18 é¡µï¼‰ï¼šæ‰©å±•æ˜ å°„åˆ°æ‰€æœ‰å¯èƒ½é™„è¡¨
    """
    mapping = {
        10: "é™„è¡¨1", 11: "é™„è¡¨1",
        12: "é™„è¡¨2",
        13: "é™„è¡¨3", 14: "é™„è¡¨3",
        15: "é™„è¡¨4",
        16: "é™„è¡¨5",
        17: "é™„è¡¨5",  # æ‰©å±•
        18: "é™„è¡¨5"   # æ‰©å±•
    }
    return mapping.get(page_no)

def infer_table_title_from_page_text(page_text: str, appendix: Optional[str], appendix_titles: Dict[str, str], page_no: int) -> str:
    if appendix and appendix in appendix_titles:
        return appendix_titles[appendix]

    if appendix:
        m = re.search(rf"(?P<title>[^\n\r]{{2,120}}?)\s*[ï¼ˆ(]\s*{re.escape(appendix)}\s*[)ï¼‰]", page_text)
        if m:
            return clean_text(m.group("title"))

    m = re.search(r"(é™„è¡¨\s*\d+)\s*[:ï¼š]\s*([^\n\r]{2,120})", page_text)
    if m:
        return clean_text(m.group(2))

    # å¢å¼ºï¼šä»é¡µé¢æ–‡æœ¬æ¨æ–­æ ‡é¢˜
    title_pat = re.search(r"^(ä¸ƒ|å…«|ä¹|å|åä¸€|åäºŒ)\s*[ã€\.ï¼]\s*(.+)$", page_text, re.MULTILINE)
    if title_pat:
        return clean_text(title_pat.group(2))

    return appendix or f"ç¬¬{page_no}é¡µè¡¨æ ¼"

def infer_direction_for_page(page_text: str) -> str:
    has_weld = "ç„Šæ¥" in page_text
    has_ndt = ("æ— æŸ" in page_text) or ("æ— æŸæ£€æµ‹" in page_text)
    if has_weld and has_ndt:
        return "æ··åˆï¼ˆç„Šæ¥+æ— æŸæ£€æµ‹ï¼‰"
    if has_weld:
        return "ç„Šæ¥"
    if has_ndt:
        return "æ— æŸæ£€æµ‹"
    return ""

def add_direction_column_rowwise(df: pd.DataFrame, page_direction: str) -> pd.DataFrame:
    """
    è¡Œçº§æ–¹å‘è¯†åˆ«ï¼šè‹¥è¡¨å†…æœ‰"ç„Šæ¥æ–¹å‘/æ— æŸæ£€æµ‹æ–¹å‘"åˆ†éš”è¡Œï¼Œåˆ™ä»è¯¥è¡Œå¼€å§‹å‘ä¸‹æ ‡æ³¨ã€‚
    è‹¥è¯†åˆ«ä¸åˆ°ï¼Œåˆ™ä½¿ç”¨ page_directionã€‚
    """
    if df is None or df.empty:
        return df

    df = df.copy()
    cur_dir = ""
    dirs = []
    for _, row in df.iterrows():
        row_txt = " ".join([clean_text(x) for x in row.values.tolist()])
        if re.search(r"ç„Šæ¥.*æ–¹å‘", row_txt):
            cur_dir = "ç„Šæ¥"
        elif re.search(r"æ— æŸ.*æ–¹å‘", row_txt) or re.search(r"æ— æŸæ£€æµ‹.*æ–¹å‘", row_txt):
            cur_dir = "æ— æŸæ£€æµ‹"

        dirs.append(cur_dir or page_direction)

    # æ’åˆ°æœ€å‰
    if "ä¸“ä¸šæ–¹å‘" not in df.columns:
        df.insert(0, "ä¸“ä¸šæ–¹å‘", dirs)
    else:
        df["ä¸“ä¸šæ–¹å‘"] = [d or page_direction for d in dirs]

    return df

# ----------------------------
# è¾“å‡ºç»“æ„
# ----------------------------
@dataclass
class TablePack:
    page: int
    title: str
    appendix: str
    direction: str
    columns: List[str]
    rows: List[List[Any]]

@dataclass
class ExtractResult:
    page_count: int
    table_count: int
    ocr_used: bool
    file_sha256: str
    extracted_at: str
    pages_data: List[Dict[str, Any]]
    sections: Dict[str, str]
    appendix_titles: Dict[str, str]
    training_objectives: Dict[str, Any]
    graduation_requirements: Dict[str, Any]
    tables: List[Dict[str, Any]]  # TablePack as dict

# ----------------------------
# ä¸»æµç¨‹ï¼ˆå¢å¼ºï¼šå¤„ç†æ‰€æœ‰é¡µï¼Œç¡®ä¿æ— é—æ¼ï¼‰
# ----------------------------
def run_full_extract(pdf_bytes: bytes, use_ocr: bool = False) -> ExtractResult:
    # 1) æå–é¡µé¢æ–‡æœ¬å’Œè¡¨æ ¼
    pages_data, full_text = extract_pages_text_and_tables(pdf_bytes, enable_ocr=use_ocr)
    
    # 2) ç»“æ„åŒ–è§£æ
    sections = split_sections(full_text)
    appendix_titles = extract_appendix_titles(full_text)
    
    # 3) å…³é”®ç»“æ„åŒ–ï¼šåŸ¹å…»ç›®æ ‡ã€æ¯•ä¸šè¦æ±‚
    obj_key = next((k for k in sections.keys() if "åŸ¹å…»ç›®æ ‡" in k), "")
    obj = parse_training_objectives(sections.get(obj_key, "") or full_text)
    grad = parse_graduation_requirements(full_text)
    
    # 4) å¤„ç†è¡¨æ ¼ï¼ˆå¢å¼ºï¼šéå†æ‰€æœ‰é¡µï¼Œç¡®ä¿æ‰€æœ‰é™„è¡¨ï¼‰
    tables: List[TablePack] = []
    total_tables = 0
    
    for page_data in pages_data:
        page_no = page_data["page"]
        page_text = page_data["text"]
        page_tables = page_data["tables"]
        
        total_tables += len(page_tables)
        
        appendix = guess_table_appendix_by_page(page_no) or ""
        base_title = infer_table_title_from_page_text(page_text, appendix or None, appendix_titles, page_no)
        title = f"{base_title}ï¼ˆ{appendix}ï¼‰" if appendix and appendix not in base_title else base_title
        page_dir = infer_direction_for_page(page_text)
        
        for i, table_data in enumerate(page_tables):
            df = table_to_df(table_data)
            if df is not None and not df.empty:
                df2 = add_direction_column_rowwise(df, page_dir)
                sub_title = title if len(page_tables) == 1 else f"{title} - è¡¨{i+1}"
                pack = TablePack(
                    page=page_no,
                    title=sub_title,
                    appendix=appendix,
                    direction=page_dir,
                    columns=[str(c) for c in df2.columns],
                    rows=df2.values.tolist(),
                )
                tables.append(pack)
    
    result = ExtractResult(
        page_count=len(pages_data),
        table_count=total_tables,
        ocr_used=use_ocr,
        file_sha256=sha256_bytes(pdf_bytes),
        extracted_at=datetime.now().isoformat(timespec="seconds"),
        pages_data=pages_data,
        sections=sections,
        appendix_titles=appendix_titles,
        training_objectives=obj,
        graduation_requirements=grad,
        tables=[asdict(t) for t in tables],
    )
    return result

# ----------------------------
# å¯¼å‡ºåŠŸèƒ½
# ----------------------------
def safe_df_from_tablepack(t: Dict[str, Any]) -> pd.DataFrame:
    """ä» TablePack å­—å…¸åˆ›å»º DataFrame"""
    cols = t.get("columns") or []
    rows = t.get("rows") or []
    
    if rows and len(rows) > 0:
        df = pd.DataFrame(rows, columns=cols)
        return postprocess_table_df(df)
    return pd.DataFrame()

def make_tables_zip(tables: List[Dict[str, Any]]) -> bytes:
    """CSV + tables.json æ‰“åŒ…"""
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        zf.writestr("tables.json", json.dumps(tables, ensure_ascii=False, indent=2))
        for idx, t in enumerate(tables, start=1):
            title = clean_text(t.get("title") or f"table_{idx}")
            title_safe = re.sub(r"[^0-9A-Za-z\u4e00-\u9fff_\-]+", "_", title)[:80].strip("_") or f"table_{idx}"

            df = safe_df_from_tablepack(t)

            # æ–¹å‘åˆ—
            direction = clean_text(t.get("direction") or "")
            if direction and "ä¸“ä¸šæ–¹å‘" not in df.columns:
                df.insert(0, "ä¸“ä¸šæ–¹å‘", direction)

            csv_bytes = df.to_csv(index=False, encoding="utf-8-sig")
            zf.writestr(f"{idx:02d}_{title_safe}.csv", csv_bytes)
    return buf.getvalue()

# ----------------------------
# æ•°æ®åº“å‡½æ•°ï¼ˆä»æ¡†æ¶è„šæœ¬ä¿ç•™ï¼‰
# ----------------------------
# ... (æ­¤å¤„çœç•¥æ¡†æ¶ä¸­çš„æ•°æ®åº“å‡½æ•°ï¼Œå¦‚init_db, get_projectsç­‰ï¼Œå› ä¸ºé•¿åº¦é™åˆ¶ã€‚å‡è®¾å®ƒä»¬ä¿æŒä¸å˜ã€‚)

# ----------------------------
# Streamlit UIï¼ˆå¢å¼ºï¼šåœ¨åŸ¹å…»æ–¹æ¡ˆé¡µé¢æ˜¾ç¤ºæ‰€æœ‰ç« èŠ‚å’Œè¡¨æ ¼ï¼‰
# ----------------------------
# ... (æ­¤å¤„çœç•¥æ¡†æ¶ä¸­çš„å…¶ä»–é¡µé¢å‡½æ•°ï¼Œåªèšç„¦page_training_plançš„å¢å¼º)

def page_training_plan():
    ensure_project()
    render_depbar(project_id, "training_plan")
    tp = get_artifact(project_id, "training_plan")
    
    st.markdown("### åŸ¹å…»æ–¹æ¡ˆåº•åº§ï¼ˆtraining_planï¼‰")
    st.caption("åŸ¹å…»æ–¹æ¡ˆæ˜¯æ•™å­¦æ–‡æ¡£é“¾çš„èµ·ç‚¹ï¼Œéœ€ä¸Šä¼ PDFå¹¶ç¡®è®¤æå–ç»“æœï¼Œæˆ–ç›´æ¥ç¼–è¾‘ã€‚")
    
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["PDFä¸Šä¼ /æŠ½å–/ç¡®è®¤", "æŸ¥çœ‹å½“å‰", "ç¼–è¾‘", "ç‰ˆæœ¬", "PDFå…¨é‡æŠ½å–ç‹¬ç«‹ç•Œé¢ï¼ˆå¢å¼ºï¼‰"])
    
    with tab1:
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.markdown("#### æ–¹å¼Aï¼šä¸€é”®ç”Ÿæˆï¼ˆæ¼”ç¤º/å¿«é€Ÿï¼‰")
            major = st.text_input("ä¸“ä¸š", value="ææ–™æˆå‹åŠæ§åˆ¶å·¥ç¨‹", key="tp_major")
            grade = st.text_input("å¹´çº§", value="22", key="tp_grade")
            group = st.text_input("è¯¾ç¨‹ä½“ç³»/æ–¹å‘", value="ææ–™æˆå‹-æ•°å€¼æ¨¡æ‹Ÿæ–¹å‘", key="tp_group")
            if st.button("ç”ŸæˆåŸ¹å…»æ–¹æ¡ˆå¹¶ä¿å­˜", type="primary"):
                md = template_training_plan(major, grade, group)
                a = upsert_artifact(
                    project_id,
                    "training_plan",
                    f"{grade}çº§ã€Š{major}ã€‹åŸ¹å…»æ–¹æ¡ˆ",
                    md,
                    {"major": major, "grade": grade, "course_group": group, "confirmed": True},
                    [],
                    note="generate",
                )
                st.success("å·²ä¿å­˜åŸ¹å…»æ–¹æ¡ˆï¼ˆå¯ä½œä¸ºåç»­æ–‡ä»¶ä¾èµ–åº•åº§ï¼‰")
                st.rerun()
        

        
        # è¯†åˆ«æ¸…å•ç¡®è®¤ç•Œé¢
        if "tp_extract" in st.session_state:
            ex = st.session_state["tp_extract"]
            extract_result = ex["extract_result"]
            
            st.markdown("---")
            st.markdown("### PDFå…¨é‡æŠ½å–ç»“æœï¼ˆè¯·ç¡®è®¤/ä¿®æ­£ï¼‰")
            
            # åŸºæœ¬ä¿¡æ¯
            colA, colB, colC = st.columns(3)
            with colA:
                major2 = st.text_input("ä¸“ä¸š", 
                                      value=extract_result.get("major_guess", "") or "ææ–™æˆå‹åŠæ§åˆ¶å·¥ç¨‹", 
                                      key="tp_major_fix")
                grade2 = st.text_input("å¹´çº§", 
                                      value=extract_result.get("grade_guess", "") or "22", 
                                      key="tp_grade_fix")
            with colB:
                course_group2 = st.text_input("è¯¾ç¨‹ä½“ç³»/æ–¹å‘", 
                                             value=extract_result.get("course_group_guess", "") or "ææ–™æˆå‹æ–¹å‘", 
                                             key="tp_group_fix")
                confirmed_flag = st.checkbox("æˆ‘å·²ç¡®è®¤ä»¥ä¸Šä¿¡æ¯å¤§ä½“æ­£ç¡®", value=False, key="tp_confirm_flag")
            with colC:
                st.metric("æ€»é¡µæ•°", extract_result.get("page_count", 0))
                st.metric("è¡¨æ ¼æ€»æ•°", extract_result.get("table_count", 0))
            
            st.markdown("#### 1) åŸ¹å…»ç›®æ ‡ï¼ˆå¯ç¼–è¾‘ï¼‰")
            goals = extract_result.get("training_objectives", {}).get("items", [])
            goals_text = st.text_area(
                "æ¯è¡Œä¸€ä¸ªç›®æ ‡ï¼ˆå¯å¢åˆ /æ”¹å†™ï¼‰",
                value="\n".join(goals) if goals else "",
                height=140,
                key="tp_goals_edit",
            )
            goals_final = [x.strip() for x in goals_text.splitlines() if x.strip()]
            
            st.markdown("#### 2) æ¯•ä¸šè¦æ±‚ï¼ˆå¯ç¼–è¾‘ï¼‰")
            grad_items = extract_result.get("graduation_requirements", {}).get("items", [])
            if grad_items:
                # åˆ›å»ºå¯ç¼–è¾‘çš„DataFrame
                grad_data = []
                for item in grad_items:
                    grad_data.append({
                        "ç¼–å·": item.get("no", ""),
                        "æ ‡é¢˜": item.get("title", ""),
                        "å†…å®¹": item.get("body", "")
                    })
                df_grad = pd.DataFrame(grad_data)
                df_grad_edited = st.data_editor(df_grad, use_container_width=True, num_rows="dynamic", key="tp_grad_editor")
                outcomes_final = []
                for _, row in df_grad_edited.iterrows():
                    if str(row["ç¼–å·"]).strip():
                        outcomes_final.append({
                            "no": str(row["ç¼–å·"]).strip(),
                            "title": str(row["æ ‡é¢˜"]).strip(),
                            "body": str(row["å†…å®¹"]).strip()
                        })
            else:
                st.info("æœªè¯†åˆ«åˆ°æ¯•ä¸šè¦æ±‚ï¼Œè¯·æ‰‹å·¥å½•å…¥")
                grad_json = st.text_area(
                    "æ¯•ä¸šè¦æ±‚ JSON",
                    value=json.dumps([{"no": "1", "title": "å·¥ç¨‹çŸ¥è¯†", "body": ""}], ensure_ascii=False, indent=2),
                    height=160,
                    key="tp_grad_json",
                )
                try:
                    outcomes_final = json.loads(grad_json) if grad_json.strip() else []
                except Exception:
                    outcomes_final = []
            
            st.markdown("#### 3) æŠ½å–çš„è¡¨æ ¼ï¼ˆå¯ç¼–è¾‘ç¡®è®¤ï¼‰")
            tables = extract_result.get("tables", [])
            confirmed_tables = []
            
            if tables:
                for i, table_info in enumerate(tables[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªè¡¨æ ¼
                    st.markdown(f"**è¡¨æ ¼{i+1}ï¼ˆç¬¬{table_info.get('page', '?')}é¡µï¼‰**")
                    
                    # ç¡®ä¿DataFrameæœ‰æ­£ç¡®çš„åˆ—å
                    try:
                        df = safe_df_from_tablepack(table_info)
                        if not df.empty:
                            # ä½¿ç”¨st.data_editor
                            df_edited = st.data_editor(df, use_container_width=True, key=f"tp_table_{i}")
                            
                            confirm_table = st.checkbox(f"ç¡®è®¤é‡‡ç”¨æ­¤è¡¨æ ¼", value=True, key=f"tp_table_confirm_{i}")
                            if confirm_table:
                                confirmed_tables.append({
                                    "page": table_info.get("page", 0),
                                    "title": table_info.get("title", ""),
                                    "data": df_edited.values.tolist(),
                                    "columns": df_edited.columns.tolist()
                                })
                    except Exception as e:
                        st.error(f"è¡¨æ ¼{i+1}æ˜¾ç¤ºé”™è¯¯: {str(e)}")
                        # æ˜¾ç¤ºåŸå§‹æ•°æ®
                        st.write("åŸå§‹æ•°æ®:", table_info.get("data", []))
            else:
                st.info("æœªæŠ½å–åˆ°è¡¨æ ¼")
            
            st.markdown("#### 4) ç« èŠ‚ç»“æ„")
            sections = extract_result.get("sections", {})
            with st.expander("æŸ¥çœ‹ç« èŠ‚ç»“æ„", expanded=False):
                for section_name, section_content in list(sections.items())[:10]:  # æ˜¾ç¤ºå‰10ä¸ªç« èŠ‚
                    st.markdown(f"**{section_name}**")
                    st.text(section_content[:500] + "..." if len(section_content) > 500 else section_content)
            
            st.markdown("---")
            if st.button("âœ… ç¡®è®¤å¹¶ä¿å­˜ä¸ºåŸ¹å…»æ–¹æ¡ˆåº•åº§", type="primary", disabled=not confirmed_flag):
                # æ„å»ºcontent_json
                content_json = {
                    "source": ex["source"],
                    "confirmed": True,
                    "major": major2,
                    "grade": grade2,
                    "course_group": course_group2,
                    "goals": goals_final,
                    "outcomes": outcomes_final,
                    "tables": confirmed_tables,
                    "extract_metadata": {
                        "page_count": extract_result.get("page_count", 0),
                        "table_count": extract_result.get("table_count", 0),
                        "sections_count": len(sections),
                        "extracted_at": extract_result.get("extracted_at", "")
                    },
                    "full_extract": extract_result  # ä¿å­˜å®Œæ•´çš„æŠ½å–ç»“æœ
                }
                
                # ç”Ÿæˆmarkdown
                md = f"# åŸ¹å…»æ–¹æ¡ˆï¼ˆPDFæŠ½å–-å·²ç¡®è®¤ï¼‰\n\n"
                md += f"- ä¸“ä¸šï¼š{major2}\n- å¹´çº§ï¼š{grade2}\n- è¯¾ç¨‹ä½“ç³»/æ–¹å‘ï¼š{course_group2}\n\n"
                md += "## ä¸€ã€åŸ¹å…»ç›®æ ‡ï¼ˆç¡®è®¤ç‰ˆï¼‰\n" + ("\n".join([f"- {x}" for x in goals_final]) if goals_final else "- ï¼ˆæœªå¡«ï¼‰") + "\n\n"
                md += "## äºŒã€æ¯•ä¸šè¦æ±‚ï¼ˆç¡®è®¤ç‰ˆï¼‰\n" + ("\n".join([f"- {o.get('no','')}. {o.get('title','')}: {o.get('body','')}" for o in outcomes_final]) if outcomes_final else "- ï¼ˆæœªå¡«ï¼‰") + "\n\n"
                md += "## ä¸‰ã€æŠ½å–è¡¨æ ¼ï¼ˆå…±{}ä¸ªï¼‰\n".format(len(confirmed_tables))
                for i, tbl in enumerate(confirmed_tables, 1):
                    md += f"- è¡¨æ ¼{i}ï¼ˆç¬¬{tbl['page']}é¡µï¼‰: {tbl['title']}\n"
                md += "\n## å››ã€ç« èŠ‚ç»“æ„\n"
                for section_name in list(sections.keys())[:5]:
                    md += f"- {section_name}\n"
                
                title = f"åŸ¹å…»æ–¹æ¡ˆï¼ˆPDFæŠ½å–ç¡®è®¤ç‰ˆï¼‰-{ex['source']}"
                a2 = upsert_artifact(project_id, "training_plan", title, md, content_json, [], note="pdf-extract-confirm")
                st.success("å·²ä¿å­˜'ç¡®è®¤ç‰ˆåŸ¹å…»æ–¹æ¡ˆåº•åº§'ã€‚åç»­ç”Ÿæˆå¤§çº²ä¼šä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–å­—æ®µã€‚")
                st.session_state.pop("tp_extract", None)
                st.rerun()
            
            if st.button("æ¸…é™¤æœ¬æ¬¡æŠ½å–ç»“æœï¼ˆä¸ä¿å­˜ï¼‰"):
                st.session_state.pop("tp_extract", None)
                st.info("å·²æ¸…é™¤ã€‚")
    
    with tab2:
        if not a:
            st.info("æš‚æ— åŸ¹å…»æ–¹æ¡ˆã€‚è¯·å…ˆç”Ÿæˆæˆ–ä¸Šä¼ å¹¶ç¡®è®¤ã€‚")
        else:
            artifact_toolbar(a)
            st.markdown("#### ç»“æ„åŒ–å†…å®¹")
            st.json(a.get("content_json") or {})
            st.markdown("#### Markdowné¢„è§ˆ")
            st.markdown(a["content_md"][:2000] + "..." if len(a["content_md"]) > 2000 else a["content_md"])
    
    with tab3:
        if not a:
            st.info("æš‚æ— åŸ¹å…»æ–¹æ¡ˆã€‚è¯·å…ˆç”Ÿæˆæˆ–ä¸Šä¼ ã€‚")
        else:
            edited = md_textarea("åœ¨çº¿ç¼–è¾‘åŸ¹å…»æ–¹æ¡ˆï¼ˆæ”¯æŒç›´æ¥ä¿®æ”¹ï¼‰", a["content_md"], key="tp_edit")
            note = st.text_input("ä¿å­˜è¯´æ˜ï¼ˆå¯é€‰ï¼‰", value="edit", key="tp_note")
            if st.button("ä¿å­˜ä¿®æ”¹ï¼ˆç”Ÿæˆæ–°ç‰ˆæœ¬ï¼‰", type="primary", key="tp_save"):
                a2 = upsert_artifact(project_id, "training_plan", a["title"], edited, a["content_json"], [], note=note)
                st.success("å·²ä¿å­˜ã€‚åç»­ä¾èµ–æ–‡ä»¶å°†å¼•ç”¨æ›´æ–°åçš„åŸ¹å…»æ–¹æ¡ˆã€‚")
                st.rerun()
    
    with tab4:
        if not a:
            st.info("æš‚æ— åŸ¹å…»æ–¹æ¡ˆã€‚")
        else:
            vers = get_versions(a["id"])
            st.markdown("#### ç‰ˆæœ¬è®°å½•")
            st.dataframe(vers if vers else [], use_container_width=True)


    with tab5:
        st.markdown("### PDFå…¨é‡æŠ½å–ç‹¬ç«‹ç•Œé¢ï¼ˆå¢å¼ºç‰ˆï¼‰")
        st.caption("ç¡®ä¿æ˜¾ç¤ºæ‰€æœ‰ç« èŠ‚ï¼ˆå¦‚ä¸€åˆ°å…­ï¼‰å’Œæ‰€æœ‰é™„è¡¨ï¼ˆå¦‚é™„è¡¨1åˆ°5ï¼Œå¯¹åº”ä¸ƒåˆ°åä¸€ï¼‰")
        
        if "extract_result" not in st.session_state:
            st.session_state["extract_result"] = None
        
        uploaded = st.file_uploader("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDF", type=["pdf"], key="full_extract_upload")
        use_ocr = st.checkbox("å¯¹æ— æ–‡æœ¬é¡µå¯ç”¨ OCRï¼ˆå¯é€‰ï¼‰", value=True, key="full_extract_ocr")  # é»˜è®¤å¯ç”¨OCRä»¥ç¡®ä¿å®Œæ•´
        
        if uploaded and st.button("å¼€å§‹å…¨é‡æŠ½å–", type="primary", key="full_extract_btn"):
            pdf_bytes = uploaded.getvalue()
            with st.spinner("æ­£åœ¨æŠ½å–æ‰€æœ‰å†…å®¹â€¦"):
                extract_result = run_full_extract(pdf_bytes, use_ocr=use_ocr)
                st.session_state["extract_result"] = extract_result
        
        result = st.session_state.get("extract_result")
        if result is None:
            st.stop()
        
        # æ¦‚è§ˆæŒ‡æ ‡
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("æ€»é¡µæ•°", result.page_count)
        c2.metric("è¡¨æ ¼æ€»æ•°", result.table_count)
        c3.metric("OCRå¯ç”¨", "æ˜¯" if result.ocr_used else "å¦")
        c4.caption(f"SHA256: {result.file_sha256[:16]}...")
        
        tabs_full = st.tabs(["æ¦‚è§ˆä¸ä¸‹è½½", "ç« èŠ‚å¤§æ ‡é¢˜ï¼ˆå…¨éƒ¨ï¼‰", "åŸ¹å…»ç›®æ ‡", "æ¯•ä¸šè¦æ±‚", "é™„è¡¨è¡¨æ ¼ï¼ˆå…¨éƒ¨ï¼‰", "åˆ†é¡µåŸæ–‡ä¸è¡¨æ ¼"])
        
           
        with tabs_full[0]:
            st.markdown("### ç»“æ„åŒ–è¯†åˆ«ç»“æœï¼ˆå¯å…ˆåœ¨è¿™é‡Œæ ¡å¯¹ï¼‰")
            
            # ä¸‹è½½ JSONï¼ˆå…¨é‡ï¼‰
            json_bytes = json.dumps(asdict(result), ensure_ascii=False, indent=2).encode("utf-8")
            st.download_button(
                "ä¸‹è½½æŠ½å–ç»“æœ JSON",
                data=json_bytes,
                file_name="training_plan_full_extract.json",
                mime="application/json",
                use_container_width=True,
            )
            
            if result.tables:
                zip_bytes = make_tables_zip(result.tables)
                st.download_button(
                    "ä¸‹è½½è¡¨æ ¼ ZIP",
                    data=zip_bytes,
                    file_name="training_plan_tables.zip",
                    mime="application/zip",
                    use_container_width=True,
                )
            
            st.markdown("#### é™„è¡¨æ ‡é¢˜æ˜ å°„")
            if result.appendix_titles:
                st.json(result.appendix_titles)
            else:
                st.info("æœªæ£€æµ‹åˆ°é™„è¡¨æ ‡é¢˜æ˜ å°„ã€‚")            
            
            
            
        
        with tabs_full[1]:
            st.markdown("### ç« èŠ‚å¤§æ ‡é¢˜ï¼ˆå…¨éƒ¨ï¼ŒåŒ…æ‹¬ä¸€åˆ°åä¸€ï¼‰")
            st.caption("æ˜¾ç¤ºæ‰€æœ‰å¤§æ ‡é¢˜åŠå…¶å†…å®¹ï¼Œç¡®ä¿æ— é—æ¼")
            for k in sorted(result.sections.keys()):  # æ’åºæ˜¾ç¤º
                with st.expander(k, expanded=True):  # é»˜è®¤å±•å¼€ä»¥ç¡®ä¿æŸ¥çœ‹
                    st.text(result.sections.get(k, ""))
        
        with tabs_full[2]:
            st.markdown("### åŸ¹å…»ç›®æ ‡")
            obj = result.training_objectives
            st.write(f"è¯†åˆ«æ¡ç›®æ•°ï¼š**{obj.get('count', 0)}**")
            st.text_area("åŸ¹å…»ç›®æ ‡ï¼ˆé€æ¡ï¼‰", value="\n".join(obj.get("items", [])), height=220, key="full_obj")
            with st.expander("åŸå§‹æ–‡æœ¬"):
                st.text(obj.get("raw", ""))
            
            
            
        
        with tabs_full[3]:
            st.markdown("### æ¯•ä¸šè¦æ±‚ï¼ˆ12æ¡ + åˆ†é¡¹ï¼‰")
            grad = result.graduation_requirements
            st.write(f"è¯†åˆ«ä¸»æ¡ç›®æ•°ï¼š**{grad.get('count', 0)}**")
            
            items = grad.get("items", [])
            if not items:
                st.warning("æœªè¯†åˆ«åˆ°æ¯•ä¸šè¦æ±‚")
            else:
                for it in items:
                    no = it.get("no")
                    title = it.get("title") or ""
                    body = it.get("body") or ""
                    header = f"{no}. {title}".strip()
                    with st.expander(header, expanded=(no in [1, 2])):
                        st.write(body)
                        subs = it.get("subitems", [])
                        if subs:
                            st.markdown("**åˆ†é¡¹ï¼š**")
                            for s in subs:
                                st.write(f"- {s.get('no')}: {s.get('body')}")
            with st.expander("åŸå§‹æ–‡æœ¬"):
                st.text(grad.get("raw", ""))
        
        with tabs_full[4]:
            st.markdown("### é™„è¡¨è¡¨æ ¼ï¼ˆå…¨éƒ¨ï¼ŒåŒ…æ‹¬é™„è¡¨1åˆ°5ï¼Œå¯¹åº”ä¸ƒåˆ°åä¸€ï¼‰")
            if not result.tables:
                st.info("æœªæ£€æµ‹åˆ°è¡¨æ ¼ã€‚è¯·æ£€æŸ¥PDFæ˜¯å¦æœ‰è¡¨æ ¼ï¼Œæˆ–å°è¯•å¯ç”¨OCRã€‚")
            else:
                all_dirs = sorted({clean_text(t.get("direction") or "") for t in result.tables if clean_text(t.get("direction") or "")})
                opt_dirs = ["å…¨éƒ¨"] + all_dirs
                sel = st.selectbox("æ–¹å‘è¿‡æ»¤", opt_dirs, index=0)
                
                for t in sorted(result.tables, key=lambda x: x.get('page', 0)):  # æŒ‰é¡µæ’åº
                    direction = clean_text(t.get("direction") or "")
                    if sel != "å…¨éƒ¨" and direction != sel:
                        continue
                    
                    st.subheader(f"ç¬¬{t.get('page')}é¡µï½œ{t.get('title')}")
                    if direction:
                        st.caption(f"é¡µé¢æ–¹å‘æç¤ºï¼š{direction}")
                    
                    df = safe_df_from_tablepack(t)
                    st.dataframe(df, use_container_width=True, hide_index=True)
        
        with tabs_full[5]:
            st.markdown("### åˆ†é¡µåŸæ–‡ä¸è¡¨æ ¼ï¼ˆç”¨äºæº¯æº/è°ƒè¯•æŠ½å–ç¼ºå¤±ï¼‰")
            
            for page_data in pages_data:
   
                page_no = page_data["page"]
                page_text = page_data["text"]
                page_tables = page_data["tables"]
                
                with st.expander(f"ç¬¬{page_no}é¡µï¼ˆ{len(page_tables)}ä¸ªè¡¨æ ¼ï¼‰", expanded=False):
                    st.text(page_text)
                    
                    if page_tables:
                        st.markdown(f"**è¡¨æ ¼ ({len(page_tables)}ä¸ª):**")
                        for i, table_data in enumerate(page_tables, start=1):
                            df = table_to_df(table_data)
                            if not df.empty:
                                st.markdown(f"**è¡¨æ ¼ {i}:**")
                                st.dataframe(df, use_container_width=True)
                            else:
                                st.info(f"è¡¨æ ¼ {i} ä¸ºç©º")    
    
    
    

# ---------------------------
# è·¯ç”±é…ç½®
# ---------------------------
ROUTES = {
    "overview": page_overview,
    "training_plan": page_training_plan,
    "syllabus": page_syllabus,
    "calendar": page_calendar,
    "lesson_plan": page_lesson_plan,
    "assessment": page_assessment,
    "review": page_review,
    "report": page_report,
    "manual": page_manual,
    "evidence": page_evidence,
    "vge": page_vge,
    "dep_graph": page_dep_graph,
    "docx_export": page_docx_export,
}

# æ‰§è¡Œå½“å‰é¡µé¢
if project_id:
    fn = ROUTES.get(current_type, page_overview)
    fn()
else:
    st.info("è¯·å…ˆåœ¨å·¦ä¾§åˆ›å»ºæˆ–é€‰æ‹©é¡¹ç›®")