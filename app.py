import io, json, time
import pandas as pd
import streamlit as st
import pdfplumber
import google.generativeai as genai
from google.api_core import exceptions

# ============================================================
# 1. æ ¸å¿ƒé€»è¾‘ï¼šå‡å°‘è¯·æ±‚æ¬¡æ•°ï¼Œå¢åŠ å•æ¬¡å®¹é‡
# ============================================================
def ai_call_with_throttle(model, prompt, generation_config):
    """å¼ºåˆ¶èŠ‚æµè°ƒç”¨ï¼šç¡®ä¿æ¯åˆ†é’Ÿè¯·æ±‚ä¸è¶…è¿‡ 12 æ¬¡ (ç•™ä½™é‡)"""
    try:
        # æ¯æ¬¡è°ƒç”¨å‰å¼ºåˆ¶å†·å´ï¼Œç¡®ä¿ç¬¦åˆ 15 RPM é™åˆ¶
        time.sleep(5) 
        return model.generate_content(prompt, generation_config=generation_config)
    except exceptions.ResourceExhausted:
        st.error("API é¢åº¦å·²è€—å°½ã€‚è¯·ç­‰å¾… 60 ç§’åå†æ¬¡ç‚¹å‡»ï¼Œæˆ–æ›´æ¢ API Keyã€‚")
        return None

def ai_process_large_chunks(model, data_list, prompt_template, chunk_size=100):
    """å¤§å¹…å¢åŠ  chunk_sizeï¼ˆä» 30 å¢åŠ åˆ° 100ï¼‰ï¼Œå‡å°‘è¯·æ±‚æ€»æ•°"""
    results = []
    # é™„è¡¨ 1 çº¦ 150 è¡Œï¼Œ100 è¡Œä¸€ç»„åªéœ€ 2 æ¬¡è¯·æ±‚ï¼ŒåŸæ¥éœ€è¦ 5-6 æ¬¡
    for i in range(0, len(data_list), chunk_size):
        chunk = data_list[i : i + chunk_size]
        full_prompt = f"{prompt_template}\næ•°æ®ï¼š{json.dumps(chunk, ensure_ascii=False)}"
        
        st.write(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1} è‡³ {i+len(chunk)} è¡Œ... (å®‰å…¨èŠ‚æµä¸­)")
        response = ai_call_with_throttle(
            model, 
            full_prompt, 
            {"response_mime_type": "application/json"}
        )
        
        if response:
            try:
                res = json.loads(response.text)
                if isinstance(res, list): results.extend(res)
            except: pass
    return results

# ============================================================
# 2. å¢å¼ºå‹è§£æå¼•æ“
# ============================================================
def final_stable_processor(api_key, pdf_bytes):
    genai.configure(api_key=api_key)
    # å¿…é¡»ä½¿ç”¨ flash æ‰èƒ½è·å¾— 15 RPMï¼ŒPro åªæœ‰ 2 RPM ä¼šç›´æ¥ç˜«ç—ª
    model = genai.GenerativeModel('gemini-2.5-flash')
    results = {"sections": {}, "tables": {"1": [], "2": [], "4": []}}
    
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        all_text = [p.extract_text() or "" for p in pdf.pages]
        
        # 1. æ­£æ–‡æŠ½å– (åˆå¹¶ 1-6 é¡¹ï¼Œä»… 1 æ¬¡è¯·æ±‚)
        st.info("æ­¥éª¤ 1/4: æ­£åœ¨æå– 1-6 é¡¹æ­£æ–‡...")
        sec_context = "\n".join(all_text[:6])
        res_sec = ai_call_with_throttle(model, f"æå– 1-6 é¡¹æ­£æ–‡ JSONã€‚å†…å®¹ï¼š{sec_context}", {"response_mime_type": "application/json"})
        if res_sec: results["sections"] = json.loads(res_sec.text)

        # 2. æœé›†åŸå§‹è¡Œ (é™„è¡¨ 1-4)
        raw_t1, raw_t4, text_t2 = [], [], ""
        for i, page in enumerate(pdf.pages):
            txt = all_text[i]
            if "é™„è¡¨1" in txt or "æ•™å­¦è®¡åˆ’è¡¨" in txt:
                tbl = page.extract_table()
                if tbl: raw_t1.extend(tbl[1:]) # 
            elif "é™„è¡¨2" in txt or "å­¦åˆ†ç»Ÿè®¡" in txt:
                text_t2 += f"\n{txt}" # [cite: 113, 119]
            elif "é™„è¡¨4" in txt or "æ”¯æ’‘å…³ç³»" in txt:
                tbl = page.extract_table()
                if tbl: raw_t4.extend(tbl[1:]) # 

        # 3. é™„è¡¨å¤„ç† (é€šè¿‡å¢åŠ  chunk_size æå¤§å‡å°‘è¯·æ±‚æ¬¡æ•°)
        if raw_t1:
            st.info("æ­¥éª¤ 2/4: æ­£åœ¨æ ¡å¯¹é™„è¡¨ 1 (æ•™å­¦è®¡åˆ’)...")
            results["tables"]["1"] = ai_process_large_chunks(model, raw_t1, "è½¬æ¢æ•™å­¦è®¡åˆ’è¡¨ã€‚å­—æ®µï¼š[è¯¾ç¨‹ä½“ç³», è¯¾ç¨‹ç¼–ç , è¯¾ç¨‹åç§°, å¼€è¯¾æ¨¡å¼, è€ƒæ ¸æ–¹å¼, å­¦åˆ†, æ€»å­¦æ—¶, å†…_è®²è¯¾, å†…_å®éªŒ, å†…_ä¸Šæœº, å†…_å®è·µ, å¤–_å­¦åˆ†, å¤–_å­¦æ—¶, ä¸Šè¯¾å­¦æœŸ, ä¸“ä¸šæ–¹å‘, å­¦ä½è¯¾, å¤‡æ³¨]", chunk_size=80)
        
        if text_t2:
            st.info("æ­¥éª¤ 3/4: æ­£åœ¨å¤„ç†é™„è¡¨ 2 (å­¦åˆ†ç»Ÿè®¡)...")
            res_t2 = ai_call_with_throttle(model, f"æå–å­¦åˆ†ç»Ÿè®¡ JSONã€‚æ–‡æœ¬ï¼š{text_t2}", {"response_mime_type": "application/json"})
            if res_t2: results["tables"]["2"] = json.loads(res_t2.text)

        if raw_t4:
            st.info("æ­¥éª¤ 4/4: æ­£åœ¨æ ¡å¯¹é™„è¡¨ 4 (æ”¯æ’‘çŸ©é˜µ)...")
            # é™„è¡¨ 4 å†…å®¹æå¤šï¼Œå¢åŠ  chunk_size åˆ° 100 å‡å°‘è¯·æ±‚
            results["tables"]["4"] = ai_process_large_chunks(model, raw_t4, "æå–æ”¯æ’‘çŸ©é˜µã€‚å­—æ®µï¼š[è¯¾ç¨‹åç§°, æŒ‡æ ‡ç‚¹, å¼ºåº¦]", chunk_size=100)

    return results

def main():
    st.set_page_config(layout="wide")
    with st.sidebar:
        api_key = st.text_input("Gemini API Key", type="password", key="safe_key")
    
    file = st.file_uploader("ä¸Šä¼  PDF", type="pdf")
    if file and api_key:
        if st.button("ğŸš€ æ‰§è¡Œå…¨é‡æŠ½å–", type="primary"):
            data = final_stable_processor(api_key, file.getvalue())
            st.session_state.final_v98 = data
            st.success("æŠ½å–å®Œæˆï¼")

    # ç»“æœæ¸²æŸ“é€»è¾‘... (åŒå‰)