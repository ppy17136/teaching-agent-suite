# app.py
# Teaching Agent Suite (single-file demo)
# - Base plan 1-11 extraction
# - Appendix tables (7-10) auto extraction + classification + page-anchored search
# - Streamlit keys fixed (no DuplicateElementKey / ValueAssignmentNotAllowedError)
# - Sidebar logo fixed (HTML render or upload image)
# - Download JSON fixed (no TypeError / non-serializable)

from __future__ import annotations

import io
import re
import json
import time
import hashlib
import base64
import datetime as _dt
from pathlib import Path
from decimal import Decimal
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Any

import pandas as pd
import streamlit as st
import pdfplumber
import google.generativeai as genai


def extract_with_gemini(api_key: str, raw_text: str, task_type: str):
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-1.5-pro') # å»ºè®®ä½¿ç”¨ pro ç‰ˆæœ¬å¤„ç†é•¿æ–‡æ¡£
    
    if task_type == "sections":
        prompt = f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–åŸ¹å…»æ–¹æ¡ˆçš„ 1-11 é¡¹å†…å®¹ï¼ŒæŒ‰ JSON æ ¼å¼è¿”å›ï¼š\n\n{raw_text}"
    elif task_type == "table_align":
        prompt = f"è¯·å°†ä»¥ä¸‹åŸå§‹è¡¨æ ¼æ•°æ®å¯¹é½åˆ°æ ‡å‡†æ•™å­¦è®¡åˆ’è¡¨æ¨¡ç‰ˆï¼š\n\n{raw_text}"
        
    response = model.generate_content(
        prompt,
        generation_config={"response_mime_type": "application/json"} # å¼ºåˆ¶è¿”å› JSON
    )
    return json.loads(response.text)

# ============================================================
# JSON serialization helper
# ============================================================
def payload_to_jsonable(obj):
    """é€’å½’æŠŠå„ç§å¸¸è§ä¸å¯ JSON åºåˆ—åŒ–å¯¹è±¡è½¬æˆå¯åºåˆ—åŒ–ç»“æ„ã€‚"""
    # pandas
    try:
        import pandas as pd

        if isinstance(obj, pd.DataFrame):
            df = obj.copy().fillna("")
            return {
                "__type__": "dataframe",
                "columns": [str(c) for c in df.columns.tolist()],
                "data": df.astype(str).values.tolist(),
            }
        if hasattr(pd, "Timestamp") and isinstance(obj, pd.Timestamp):
            return obj.isoformat()
    except Exception:
        pass

    # numpy
    try:
        import numpy as np

        if isinstance(obj, (np.integer, np.floating, np.bool_)):
            return obj.item()
        if isinstance(obj, np.ndarray):
            return obj.tolist()
    except Exception:
        pass

    # bytesï¼ˆæ¯”å¦‚ pdf_bytesï¼‰
    if isinstance(obj, (bytes, bytearray)):
        return {
            "__type__": "bytes_base64",
            "data": base64.b64encode(bytes(obj)).decode("ascii"),
        }

    # datetime / date
    if isinstance(obj, (_dt.datetime, _dt.date)):
        return obj.isoformat()

    # Path / Decimal
    if isinstance(obj, Path):
        return str(obj)
    if isinstance(obj, Decimal):
        return float(obj)

    # set/tuple
    if isinstance(obj, (set, tuple)):
        return [payload_to_jsonable(x) for x in obj]

    # dict / list
    if isinstance(obj, dict):
        return {str(k): payload_to_jsonable(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [payload_to_jsonable(x) for x in obj]

    # å…¶å®ƒï¼šå°½é‡åŸæ ·è¿”å›ï¼Œå¿…è¦æ—¶è½¬å­—ç¬¦ä¸²
    try:
        json.dumps(obj)  # probe
        return obj
    except Exception:
        return str(obj)


# ============================================================
# Helpers
# ============================================================
def _now_str() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())


def _short_id(s: str) -> str:
    return hashlib.md5(s.encode("utf-8")).hexdigest()[:10]


def _safe_text(x: Any) -> str:
    if x is None:
        return ""
    return str(x).strip()


def _compact_lines(s: str) -> str:
    s = (s or "").replace("\u00a0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()


def _join_pages(pages_text: List[str]) -> str:
    return _compact_lines("\n\n".join([t or "" for t in pages_text]))


def _read_pdf_pages_text(pdf_bytes: bytes) -> List[str]:
    pages = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for p in pdf.pages:
            txt = p.extract_text() or ""
            pages.append(_compact_lines(txt))
    return pages


# ============================================================
# Base plan (sections 1-11) text extraction (regex best-effort)
# å…³é”®æ”¹è¿›ï¼šé¿å…ç›®å½•(Toc)å¹²æ‰° -> æ¯ä¸ªæ ‡é¢˜å–â€œæœ€åä¸€æ¬¡å‡ºç°â€çš„ä½ç½®
# ============================================================
_SECTION_PATTERNS: List[Tuple[str, List[str]]] = [
    ("1", [r"ä¸€[ã€\.\s]*åŸ¹å…»ç›®æ ‡", r"1[ã€\.\s]*åŸ¹å…»ç›®æ ‡"]),
    ("2", [r"äºŒ[ã€\.\s]*æ¯•ä¸šè¦æ±‚", r"2[ã€\.\s]*æ¯•ä¸šè¦æ±‚"]),
    ("3", [r"ä¸‰[ã€\.\s]*ä¸“ä¸šå®šä½ä¸ç‰¹è‰²", r"3[ã€\.\s]*ä¸“ä¸šå®šä½ä¸ç‰¹è‰²"]),
    ("4", [r"å››[ã€\.\s]*ä¸»å¹²å­¦ç§‘", r"4[ã€\.\s]*ä¸»å¹²å­¦ç§‘"]),
    ("5", [r"äº”[ã€\.\s]*æ ‡å‡†å­¦åˆ¶ä¸æˆäºˆå­¦ä½", r"5[ã€\.\s]*æ ‡å‡†å­¦åˆ¶"]),
    ("6", [r"å…­[ã€\.\s]*æ¯•ä¸šæ¡ä»¶", r"6[ã€\.\s]*æ¯•ä¸šæ¡ä»¶"]),
    ("7", [r"ä¸ƒ[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨", r"7[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨"]),
    ("8", [r"å…«[ã€\.\s]*å­¦åˆ†ç»Ÿè®¡è¡¨", r"8[ã€\.\s]*å­¦åˆ†ç»Ÿè®¡è¡¨"]),
    ("9", [r"ä¹[ã€\.\s]*æ•™å­¦è¿›ç¨‹è¡¨", r"9[ã€\.\s]*æ•™å­¦è¿›ç¨‹è¡¨"]),
    ("10", [r"å[ã€\.\s]*è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚æ”¯æ’‘å…³ç³»è¡¨", r"10[ã€\.\s]*è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚æ”¯æ’‘å…³ç³»è¡¨"]),
    ("11", [r"åä¸€[ã€\.\s]*è¯¾ç¨‹è®¾ç½®é€»è¾‘æ€ç»´å¯¼å›¾", r"11[ã€\.\s]*è¯¾ç¨‹è®¾ç½®é€»è¾‘æ€ç»´å¯¼å›¾"]),
]


def _find_last_heading_pos(full_text: str, patterns: List[str]) -> Optional[int]:
    """è¿”å›è¯¥æ ‡é¢˜åœ¨å…¨æ–‡ä¸­æœ€åä¸€æ¬¡å‡ºç°çš„ä½ç½®ï¼Œå°½é‡ç»•å¼€å‰é¢çš„ç›®å½•ã€‚"""
    last_pos = None
    for pat in patterns:
        for m in re.finditer(pat, full_text):
            last_pos = m.start()
    return last_pos


def _build_section_spans(full_text: str) -> Dict[str, Tuple[int, int]]:
    """
    Find each section heading position (prefer last occurrence); return char spans [start,end) for each section.
    """
    hits: List[Tuple[str, int]] = []
    for sec_id, pats in _SECTION_PATTERNS:
        pos = _find_last_heading_pos(full_text, pats)
        if pos is not None:
            hits.append((sec_id, pos))

    hits.sort(key=lambda x: x[1])
    spans: Dict[str, Tuple[int, int]] = {}
    for i, (sec_id, start) in enumerate(hits):
        end = hits[i + 1][1] if i + 1 < len(hits) else len(full_text)
        spans[sec_id] = (start, end)
    return spans


def _extract_section_text(full_text: str, spans: Dict[str, Tuple[int, int]], sec_id: str) -> str:
    if sec_id not in spans:
        return ""
    s, e = spans[sec_id]
    chunk = full_text[s:e].strip()

    # å»æ‰æ ‡é¢˜è¡Œè‡ªèº«ï¼ˆå°½é‡ï¼‰
    chunk = re.sub(
        r"^\s*(ä¸€|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹|å|åä¸€|\d+)[ã€\.\s]*[^\n]{0,30}\n",
        "",
        chunk,
    )
    return _compact_lines(chunk)


# ============================================================
# Appendix table extraction (pdfplumber) + classification
# å…³é”®æ”¹è¿›ï¼š
# 1) å…ˆç”¨ pages_text é”šå®šâ€œé™„è¡¨1/2/3/4â€æ‰€åœ¨é¡µï¼Œå†åœ¨é™„è¿‘é¡µæŠ½è¡¨
# 2) æŠ½è¡¨è¿”å›å¸¦ page_idxï¼Œé¿å…ä¸åŒé™„è¡¨äº’ç›¸ä¸²
# 3) æ¯ä¸ªé™„è¡¨å–â€œæœ€åŒ¹é…(é«˜åˆ†)+æ›´å¤§(é¢ç§¯)â€çš„é‚£å¼ 
# ============================================================
def _valid_table_settings_lines() -> dict:
    """Safe pdfplumber settings (avoid TableSettings.resolve TypeError)."""
    return dict(
        vertical_strategy="lines",
        horizontal_strategy="lines",
        snap_tolerance=3,
        join_tolerance=3,
        edge_min_length=3,
        intersection_tolerance=3,
        text_tolerance=3,
    )


def _drop_repeated_header_row(df: pd.DataFrame) -> pd.DataFrame:
    """å¦‚æœæ•°æ®ç¬¬ä¸€è¡Œå°±æ˜¯é‡å¤è¡¨å¤´ï¼ˆå€¼â‰ˆåˆ—åï¼‰ï¼Œå°±åˆ æ‰ã€‚"""
    if df is None or df.empty:
        return df
    first = [str(x).strip() for x in df.iloc[0].tolist()]
    cols = [str(c).strip() for c in df.columns.tolist()]

    # â€œç¬¬ä¸€è¡Œä¸åˆ—åé«˜åº¦ä¸€è‡´â€å°±è®¤ä¸ºæ˜¯é‡å¤è¡¨å¤´
    if len(first) == len(cols):
        same = sum(1 for a, b in zip(first, cols) if a == b and a != "")
        if same >= max(2, int(0.6 * len(cols))):
            return df.iloc[1:].reset_index(drop=True)
    return df


def _align_to_canonical_cols(df: pd.DataFrame, canonical_cols: List[str]) -> pd.DataFrame:
    """æŠŠ df å¯¹é½åˆ° canonical_colsï¼šåŒåˆ—æ•°åˆ™ç›´æ¥æŒ‰ä½ç½®æ”¹åï¼›ä¸åŒåˆ—æ•°åˆ™æŒ‰ä½ç½®å¡«å……ã€‚"""
    if df is None:
        return pd.DataFrame(columns=canonical_cols)
    df = df.copy()

    if df.empty:
        return pd.DataFrame(columns=canonical_cols)

    # åŒåˆ—æ•°ï¼šç›´æ¥æŒ‰ä½ç½®å¯¹é½åˆ—å
    if len(df.columns) == len(canonical_cols):
        df.columns = canonical_cols
        return df

    # ä¸åŒåˆ—æ•°ï¼šåˆ›å»ºæ–°è¡¨ï¼ŒæŒ‰ä½ç½®å¡«å……
    new_df = pd.DataFrame(columns=canonical_cols)
    m = min(len(df.columns), len(canonical_cols))
    for i in range(m):
        new_df[canonical_cols[i]] = df.iloc[:, i].astype(str)
    # å‰©ä½™ canonical åˆ—ä¿æŒç©º
    return new_df


def _merge_table_fragments(fragments: List[pd.DataFrame]) -> pd.DataFrame:
    """
    çºµå‘åˆå¹¶å¤šä¸ªç‰‡æ®µï¼šåˆ—å¯¹é½ + å»é‡å¤è¡¨å¤´ + concat
    """
    fragments = [f for f in fragments if f is not None and not f.empty]
    if not fragments:
        return pd.DataFrame()

    # é€‰â€œåˆ—æœ€å¤šâ€çš„é‚£å¼ ä½œä¸º canonicalï¼ˆé€šå¸¸ç¬¬ä¸€é¡µæœ€å®Œæ•´ï¼‰
    canonical = max(fragments, key=lambda d: len(d.columns))
    canonical_cols = [str(c) for c in canonical.columns.tolist()]

    merged_parts = []
    for i, df in enumerate(fragments):
        df2 = _align_to_canonical_cols(df, canonical_cols)
        df2 = _clean_df(df2)
        # ç¬¬äºŒé¡µå¼€å§‹ç»å¸¸ä¼šé‡å¤è¡¨å¤´ï¼Œåˆ æ‰
        if i > 0:
            df2 = _drop_repeated_header_row(df2)
        merged_parts.append(df2)

    merged = pd.concat(merged_parts, axis=0, ignore_index=True)
    merged = _clean_df(merged)
    return merged

def _extract_tables_from_pages(pdf_bytes: bytes, page_idx_list: List[int]) -> List[Dict[str, Any]]:
    """
    Return: list of {"page": page_idx, "order": table_order_in_page, "rows": table_rows}
    """
    out: List[Dict[str, Any]] = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for idx in page_idx_list:
            if idx < 0 or idx >= len(pdf.pages):
                continue
            page = pdf.pages[idx]

            try:
                tables = page.extract_tables(table_settings=_valid_table_settings_lines()) or []
            except TypeError:
                tables = page.extract_tables() or []
            except Exception:
                try:
                    tables = page.extract_tables() or []
                except Exception:
                    tables = []

            for t_i, t in enumerate(tables):
                norm = []
                for row in t:
                    norm.append([_safe_text(c) for c in row])
                out.append({"page": idx, "order": t_i, "rows": norm})
    return out



def _dedup_cols(cols: List[str]) -> List[str]:
    seen = {}
    out = []
    for c in cols:
        c0 = c.strip() or "åˆ—"
        if c0 not in seen:
            seen[c0] = 1
            out.append(c0)
        else:
            seen[c0] += 1
            out.append(f"{c0}_{seen[c0]}")
    return out


def _clean_df(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame()

    df = df.copy()
    df.replace({None: ""}, inplace=True)

    # æŠŠ "nan" æ–‡æœ¬ä¹Ÿæ¸…æ‰
    df = df.applymap(lambda x: "" if str(x).strip().lower() == "nan" else str(x).strip())

    # drop all-empty rows/cols
    df = df.loc[~df.apply(lambda r: all((str(x).strip() == "") for x in r), axis=1)]
    df = df.loc[:, ~df.apply(lambda c: all((str(x).strip() == "") for x in c), axis=0)]
    df = df.reset_index(drop=True)

    # åˆ é™¤â€œå­¦æœŸä¸­æ–‡æ•°å­—è¡Œâ€å™ªå£°ï¼ˆå›› äº” å…­ ä¸ƒ å…«â€¦ï¼‰
    def _looks_like_semester_row(row: pd.Series) -> bool:
        tokens = [str(x).strip() for x in row.tolist() if str(x).strip()]
        if len(tokens) < 3:
            return False
        cn_nums = set(list("ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å"))
        hits = sum(1 for t in tokens if (len(t) == 1 and t in cn_nums))
        return hits >= 3

    if not df.empty:
        df = df.loc[~df.apply(_looks_like_semester_row, axis=1)].reset_index(drop=True)

    return df


def _table_to_df(table_rows: List[List[str]]) -> pd.DataFrame:
    rows = [r for r in table_rows if any(_safe_text(x) for x in r)]
    if not rows:
        return pd.DataFrame()

    max_cols = max(len(r) for r in rows)
    rows = [r + [""] * (max_cols - len(r)) for r in rows]

    header = rows[0]
    header_join = " ".join(header)
    header_like = any(
        k in header_join
        for k in ["è¯¾ç¨‹", "å­¦åˆ†", "å‘¨æ¬¡", "æŒ‡æ ‡", "æ”¯æ’‘", "åˆè®¡", "è¯¾ç¨‹ç¼–ç ", "è¯¾ç¨‹åç§°", "æ¯•ä¸šè¦æ±‚"]
    )
    if header_like:
        cols = [c if c else f"åˆ—{i+1}" for i, c in enumerate(header)]
        df = pd.DataFrame(rows[1:], columns=_dedup_cols(cols))
    else:
        cols = [f"åˆ—{i+1}" for i in range(max_cols)]
        df = pd.DataFrame(rows, columns=cols)

    return _clean_df(df)


def _table_signature_text(df: pd.DataFrame) -> str:
    if df is None or df.empty:
        return ""
    head = " ".join([str(c) for c in df.columns.tolist()])
    top_rows = []
    for i in range(min(3, len(df))):
        top_rows.append(" ".join([str(x) for x in df.iloc[i].tolist()]))
    return (head + " " + " ".join(top_rows)).lower()


def _classify_table(df: pd.DataFrame) -> Tuple[str, int]:
    """
    Return (section_id, score). section_id in {"7","8","9","10"} or ("",0)
    """
    s = _table_signature_text(df)

    score7 = 0
    for k in ["è¯¾ç¨‹ç¼–ç ", "è¯¾ç¨‹ä»£ç ", "è¯¾ç¨‹åç§°", "å­¦åˆ†", "æ€»å­¦æ—¶", "è€ƒæ ¸", "å¼€è¯¾"]:
        if k in s:
            score7 += 3

    score8 = 0
    for k in ["å­¦åˆ†ç»Ÿè®¡", "å¿…ä¿®", "é€‰ä¿®", "é€šè¯†", "ä¸“ä¸š", "å®è·µ", "åˆè®¡", "å°è®¡"]:
        if k in s:
            score8 += 3

    score9 = 0
    for k in ["å‘¨æ¬¡", "æ•™å­¦å†…å®¹", "è¿›åº¦", "ç« èŠ‚", "å­¦æ—¶", "å®éªŒ"]:
        if k in s:
            score9 += 3

    score10 = 0
    for k in ["æ¯•ä¸šè¦æ±‚", "æŒ‡æ ‡ç‚¹", "æ”¯æ’‘", "è¾¾æˆ", "å¯¹åº”", "è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚"]:
        if k in s:
            score10 += 3

    scores = [("7", score7), ("8", score8), ("9", score9), ("10", score10)]
    best = max(scores, key=lambda x: x[1])
    if best[1] >= 6:
        return best
    return ("", 0)


def _find_appendix_anchor_pages(pages_text: List[str]) -> Dict[str, List[int]]:
    """
    åœ¨ pages_text ä¸­å¯»æ‰¾é™„è¡¨1~4 çš„é”šç‚¹é¡µï¼ˆå¯èƒ½å†™æˆâ€œé™„è¡¨ 1â€â€œé™„è¡¨1â€â€œï¼ˆé™„è¡¨1ï¼‰â€ç­‰ï¼‰ã€‚
    è¿”å›: {"7":[...], "8":[...], "9":[...], "10":[...]} çš„é¡µå·åˆ—è¡¨(0-based)
    """
    pats = {
        "7": [r"é™„è¡¨\s*1", r"ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨", r"ä¸ƒ[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨"],
        "8": [r"é™„è¡¨\s*2", r"å­¦åˆ†ç»Ÿè®¡è¡¨", r"å…«[ã€\.\s]*å­¦åˆ†ç»Ÿè®¡è¡¨"],
        "9": [r"é™„è¡¨\s*3", r"æ•™å­¦è¿›ç¨‹è¡¨", r"ä¹[ã€\.\s]*æ•™å­¦è¿›ç¨‹è¡¨"],
        "10": [r"é™„è¡¨\s*4", r"æ”¯æ’‘å…³ç³»è¡¨", r"è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚æ”¯æ’‘å…³ç³»è¡¨", r"å[ã€\.\s]*è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚æ”¯æ’‘å…³ç³»è¡¨"],
    }
    anchors: Dict[str, List[int]] = {k: [] for k in pats.keys()}
    for i, t in enumerate(pages_text):
        tt = t or ""
        for sec, ps in pats.items():
            for p in ps:
                if re.search(p, tt):
                    anchors[sec].append(i)
                    break
    # å»é‡ã€æ’åº
    for k in anchors:
        anchors[k] = sorted(list(set(anchors[k])))
    return anchors


def extract_appendix_tables_best_effort(pdf_bytes: bytes, pages_text: List[str]) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Any]]:
    """
    ä» PDF æœ«å°¾é¡µé¢æŠ½å–è¡¨æ ¼ï¼Œè‡ªåŠ¨åˆ†ç±»åˆ†é…åˆ° 7-10ã€‚
    âœ… æ”¯æŒåŒä¸€é™„è¡¨è·¨å¤šé¡µï¼šæŒ‰é¡µåºåˆå¹¶ï¼ˆç‰¹åˆ«æ˜¯é™„è¡¨1/é™„è¡¨4ï¼‰
    """
    n = len(pages_text)
    tail_pages = list(range(max(0, n - 18), n))  # æœ«å°¾å¤šæŠ“ä¸€ç‚¹é¡µï¼Œè·¨é¡µæ›´ç¨³
    raw_tables = _extract_tables_from_pages(pdf_bytes, tail_pages)

    dfs_info: List[Tuple[int, int, pd.DataFrame, str, int]] = []
    # (page, order, df, sec, score)

    for item in raw_tables:
        page_idx = item["page"]
        order = item["order"]
        df = _table_to_df(item["rows"])
        if df is None or df.empty:
            continue
        if df.shape[0] < 2 and df.shape[1] < 3:
            continue

        sec, score = _classify_table(df)
        if sec:
            dfs_info.append((page_idx, order, df, sec, score))

    # åˆ†ç»„ï¼šåŒä¸€ä¸ª sec æ”¶é›†æ‰€æœ‰ç‰‡æ®µ
    frags: Dict[str, List[Tuple[int, int, int, pd.DataFrame]]] = {"7": [], "8": [], "9": [], "10": []}
    for page_idx, order, df, sec, score in dfs_info:
        if sec in frags:
            frags[sec].append((page_idx, order, score, df))

    assigned: Dict[str, pd.DataFrame] = {}
    debug_sec = {}

    for sec, lst in frags.items():
        if not lst:
            continue

        # è¿‡æ»¤æ‰æ˜æ˜¾è¯¯åˆ¤ï¼šåªä¿ç•™æ¥è¿‘è¯¥ sec â€œæœ€é«˜åˆ†â€çš„ç‰‡æ®µ
        max_score = max(x[2] for x in lst)
        kept = [x for x in lst if x[2] >= max(6, max_score - 3)]  # >=6 æˆ–æ¥è¿‘æœ€é«˜åˆ†
        kept.sort(key=lambda x: (x[0], x[1]))  # æŒ‰é¡µåº/è¡¨åº

        merged = _merge_table_fragments([x[3] for x in kept])
        if merged is not None and not merged.empty:
            assigned[sec] = merged

        debug_sec[sec] = {
            "fragments_total": len(lst),
            "fragments_kept": len(kept),
            "max_score": max_score,
            "pages": [x[0] for x in kept],
            "shape_merged": list(merged.shape) if merged is not None else None,
        }

    debug = {
        "tail_pages": tail_pages,
        "raw_tables_count": len(raw_tables),
        "classified_tables_count": len(dfs_info),
        "assigned": {k: list(v.shape) for k, v in assigned.items()},
        "merge_debug": debug_sec,
    }
    return assigned, debug



def base_plan_from_pdf(pdf_bytes: bytes) -> Dict[str, Any]:
    pages = _read_pdf_pages_text(pdf_bytes)
    full = _join_pages(pages)
    spans = _build_section_spans(full)

    base = {}
    for sec_id, _ in _SECTION_PATTERNS:
        base[sec_id] = _extract_section_text(full, spans, sec_id)

    # 7-11 æ­£æ–‡å¯èƒ½åªæœ‰æ ‡é¢˜ï¼šæç¤º
    for sec_id in ["7", "8", "9", "10", "11"]:
        if not base.get(sec_id, "").strip():
            base[sec_id] = f"{sec_id}ï¼šæ­£æ–‡å¯èƒ½ä»…æœ‰æ ‡é¢˜ï¼›è¯·å°è¯•ä» PDF æœ«å°¾é™„è¡¨è‡ªåŠ¨æŠ½å–ã€‚"

    auto_tables, debug_meta = extract_appendix_tables_best_effort(pdf_bytes, pages)

    return dict(
        pages=pages,
        full_text=full,
        sections=base,              # 1-11 text
        tables=auto_tables,         # 7-10 tables
        debug=debug_meta,
    )


# ============================================================
# UI
# ============================================================
@dataclass
class Project:
    project_id: str
    name: str
    updated_at: str


def _init_state():
    if "projects" not in st.session_state:
        pid = _short_id(_now_str())
        st.session_state.projects = [
            Project(project_id=pid, name=f"é»˜è®¤é¡¹ç›®-{time.strftime('%Y%m%d-%H%M')}", updated_at=_now_str())
        ]
        st.session_state.active_project_id = pid

    if "project_data" not in st.session_state:
        st.session_state.project_data = {}

    if "logo_bytes" not in st.session_state:
        st.session_state.logo_bytes = None


def ui_sidebar_brand():
    with st.sidebar:
        col1, col2 = st.columns([1, 4])
        with col1:
            if st.session_state.logo_bytes:
                st.image(st.session_state.logo_bytes, width=44)
            else:
                # âœ… ä¸å†ç”¨ components.htmlï¼ˆæœ‰æ—¶ä¼šæ˜¾ç¤ºæˆæ–‡æœ¬/æˆ–è§¦å‘ sidebar.components ç›¸å…³é—®é¢˜ï¼‰
                # âœ… ç”¨ markdown + unsafe_allow_html 100%ç¨³
                svg = """
                <div style="width:44px;height:44px;border-radius:50%;
                            background:#2f6fed;display:flex;align-items:center;justify-content:center;
                            color:white;font-weight:800;font-family:Arial;">
                  TA
                </div>
                """
                st.markdown(svg, unsafe_allow_html=True)

        with col2:
            st.markdown("**Teaching Agent Suite**")
            st.caption("v0.6 (base 1â€“11 + appendix tables + logo fixed)")

        up = st.file_uploader("ä¸Šä¼  Logoï¼ˆå¯é€‰ï¼Œpng/jpgï¼‰", type=["png", "jpg", "jpeg"], key="logo_uploader")
        if up is not None:
            st.session_state.logo_bytes = up.getvalue()


def ui_project_sidebar() -> Project:
    ui_sidebar_brand()

    with st.sidebar:
        st.divider()
        st.markdown("### é¡¹ç›®")
        options = {p.project_id: p for p in st.session_state.projects}
        labels = {p.project_id: f"{p.name} ({p.project_id})" for p in st.session_state.projects}

        pid = st.selectbox(
            "é€‰æ‹©é¡¹ç›®",
            options=list(labels.keys()),
            format_func=lambda x: labels[x],
            index=list(labels.keys()).index(st.session_state.active_project_id),
            key="project_select",
        )
        st.session_state.active_project_id = pid
        return options[pid]


def _render_top_header(project: Project):
    # âœ… å¿…é¡» unsafe_allow_html=Trueï¼Œå¦åˆ™ä¼šæŠŠ HTML å½“çº¯æ–‡æœ¬æ˜¾ç¤º
    html = f"""
    <div style="border:1px solid #e7eefc; background:#f6f9ff; padding:18px 18px; border-radius:14px;">
      <div style="font-weight:900; font-size:28px;">æ•™å­¦æ–‡ä»¶å·¥ä½œå°</div>
      <div style="color:#666; margin-top:4px; font-size:14px;">
        é¡¹ç›®ï¼š <b>{project.name}</b>ï¼ˆ{project.project_id}ï¼‰ Â· æœ€åæ›´æ–°ï¼š {project.updated_at}
      </div>
    </div>
    """
    st.markdown(html, unsafe_allow_html=True)


def ui_base_training_plan(project: Project):
    st.subheader("åŸ¹å…»æ–¹æ¡ˆåŸºåº§ï¼ˆå…¨é‡å†…å®¹åº“ï¼‰")
    st.caption("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDF â†’ æŠ½å–å¡«å…… 1â€“11 â†’ å¹¶å°è¯•ä»æœ«å°¾é™„è¡¨è‡ªåŠ¨æŠ½è¡¨å¡«å…… 7â€“10ã€‚")

    left, right = st.columns([1, 1.4], gap="large")

    with left:
        pdf = st.file_uploader("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDFï¼ˆå¯é€‰ï¼‰", type=["pdf"], key=f"pdf_{project.project_id}")

        if st.button("æŠ½å–å¹¶å†™å…¥åŸºåº§", use_container_width=True, type="primary", key=f"extract_btn_{project.project_id}"):
            if not pdf:
                st.warning("è¯·å…ˆä¸Šä¼  PDFã€‚")
            else:
                pdf_bytes = pdf.getvalue()
                payload = base_plan_from_pdf(pdf_bytes)
                st.session_state.project_data[project.project_id] = payload

                # æ›´æ–°æ—¶é—´
                for i, p in enumerate(st.session_state.projects):
                    if p.project_id == project.project_id:
                        st.session_state.projects[i] = Project(
                            project_id=p.project_id,
                            name=p.name,
                            updated_at=_now_str(),
                        )
                        break

                st.success("å·²æŠ½å–å¹¶å†™å…¥åŸºåº§ã€‚å³ä¾§å·²è”åŠ¨å¡«å……ã€‚")

        # ä¸‹è½½ JSONï¼ˆâœ… ä¿®å¤ï¼šä¸èƒ½åœ¨ download_button å‚æ•°é‡Œä¹±å†™èµ‹å€¼ï¼›åŒæ—¶å…ˆåš jsonableï¼‰
        payload = st.session_state.project_data.get(project.project_id)
        if payload:
            json_payload = payload_to_jsonable(payload)
            st.download_button(
                "ä¸‹è½½åŸºåº§ JSON",
                data=json.dumps(json_payload, ensure_ascii=False, indent=2).encode("utf-8"),
                file_name=f"base_{project.project_id}.json",
                mime="application/json",
                use_container_width=True,
                key=f"dl_{project.project_id}",
            )

        st.divider()
        if payload:
            missing = [k for k in [str(i) for i in range(1, 12)] if not payload["sections"].get(k, "").strip()]
            if missing:
                st.warning(f"æ£€æŸ¥ï¼šç¼ºå°‘æ ç›® {missing}")
            else:
                st.success("1â€“11 æ ç›®å‡å·²å­˜åœ¨ï¼ˆä»å»ºè®®äººå·¥å¿«é€Ÿæ‰«è¯»ï¼‰ã€‚")

        with st.expander("è°ƒè¯•ï¼šåˆ†é¡µåŸæ–‡ (raw_pages_text)"):
            if payload:
                st.write(payload["pages"])
            else:
                st.info("å…ˆæŠ½å–åå¯è§ã€‚")

        with st.expander("è°ƒè¯•ï¼šé™„è¡¨æŠ½å–ä¿¡æ¯ (appendix_debug)"):
            if payload:
                st.json(payload["debug"])
            else:
                st.info("å…ˆæŠ½å–åå¯è§ã€‚")

    with right:
        st.markdown("#### åŸ¹å…»æ–¹æ¡ˆå†…å®¹ï¼ˆæŒ‰æ ç›®å±•ç¤ºï¼Œå¯ç¼–è¾‘ï¼‰")

        payload = st.session_state.project_data.get(project.project_id)
        if not payload:
            st.info("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼  PDF å¹¶ç‚¹å‡»â€œæŠ½å–å¹¶å†™å…¥åŸºåº§â€ã€‚")
            return

        sections = payload["sections"]
        tables = payload.get("tables", {})

        toc = [
            ("1", "åŸ¹å…»ç›®æ ‡"),
            ("2", "æ¯•ä¸šè¦æ±‚"),
            ("3", "ä¸“ä¸šå®šä½ä¸ç‰¹è‰²"),
            ("4", "ä¸»å¹²å­¦ç§‘/æ ¸å¿ƒè¯¾ç¨‹/å®è·µç¯èŠ‚"),
            ("5", "æ ‡å‡†å­¦åˆ¶ä¸æˆäºˆå­¦ä½"),
            ("6", "æ¯•ä¸šæ¡ä»¶"),
            ("7", "ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨ï¼ˆé™„è¡¨1ï¼‰"),
            ("8", "å­¦åˆ†ç»Ÿè®¡è¡¨ï¼ˆé™„è¡¨2ï¼‰"),
            ("9", "æ•™å­¦è¿›ç¨‹è¡¨ï¼ˆé™„è¡¨3ï¼‰"),
            ("10", "æ”¯æ’‘å…³ç³»è¡¨ï¼ˆé™„è¡¨4ï¼‰"),
            ("11", "é€»è¾‘æ€ç»´å¯¼å›¾ï¼ˆé™„è¡¨5ï¼‰"),
        ]
        title_map = dict(toc)

        sec_pick = st.radio(
            "æ ç›®",
            options=[x[0] for x in toc],
            format_func=lambda x: title_map[x],
            horizontal=True,
            key=f"sec_radio_{project.project_id}",
        )

        st.markdown(f"##### {sec_pick}ã€{title_map[sec_pick]}")

        # 6 å†…å®¹è¿‡é•¿å…œåº•æˆªæ–­ï¼šé‡åˆ° â€œä¸ƒã€ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨â€ å°±æˆªæ–­
        def _truncate_at_next_heading(txt: str, next_sec_id: str) -> str:
            if not txt:
                return ""
            next_title = title_map.get(next_sec_id, "")
            if not next_title:
                return txt
            # å…¼å®¹ â€œä¸ƒã€ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨â€ æˆ– â€œ7 ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨â€
            pat = rf"(\n\s*ä¸ƒ[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨|\n\s*7[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨)"
            m = re.search(pat, "\n" + txt)
            if m:
                return _compact_lines(txt[: m.start()])
            return txt

        txt = sections.get(sec_pick, "")
        if sec_pick == "6":
            txt = _truncate_at_next_heading(txt, "7")

        st.text_area(
            f"{sec_pick} æ–‡æœ¬æŠ½å–ç»“æœ",
            value=txt,
            height=220,
            key=f"sec_text_{project.project_id}_{sec_pick}",
        )

        # 7-10ï¼šè¡¨æ ¼åŒºï¼ˆè‡ªåŠ¨æŠ½å–ï¼‰
        if sec_pick in ["7", "8", "9", "10"]:
            st.markdown("###### è¡¨æ ¼åŒºï¼ˆå¯ç¼–è¾‘ï¼Œè¡Œå¯å¢åˆ ï¼‰")

            df0 = tables.get(sec_pick)
            if df0 is None or (isinstance(df0, pd.DataFrame) and df0.empty):
                st.info("æœªè‡ªåŠ¨æŠ½å–åˆ°è¯¥é™„è¡¨ï¼ˆå¯èƒ½ PDF è¡¨æ ¼æ˜¯å›¾ç‰‡/çº¿æ¡ä¸è§„åˆ™/æˆ–é™„è¡¨å¸ƒå±€ç‰¹æ®Šï¼‰ã€‚ä½ å¯ä»¥æ‰‹å·¥è¡¥å…¨ã€‚")
                df0 = pd.DataFrame()

            editor_key = f"tbl_editor_{project.project_id}_{sec_pick}"
            edited = st.data_editor(
                df0,
                num_rows="dynamic",
                use_container_width=True,
                key=editor_key,
            )
            # âœ… ä¸è¦†ç›– widget keyï¼Œå¦å­˜ä¸€ä»½
            st.session_state[f"{editor_key}__value"] = edited

        if sec_pick == "11":
            st.info("é€»è¾‘æ€ç»´å¯¼å›¾ï¼ˆé™„è¡¨5ï¼‰é€šå¸¸æ˜¯å›¾ç‰‡/æµç¨‹å›¾ï¼Œpdfplumber çš„è¡¨æ ¼æŠ½å–ä¸ä¸€å®šæœ‰æ•ˆã€‚å¯åç»­åŠ â€œæœ«é¡µå›¾ç‰‡æŠ½å–â€ã€‚")


# def main():
    # st.set_page_config(page_title="Teaching Agent Suite", page_icon="ğŸ§ ", layout="wide")
    # _init_state()

    # prj = ui_project_sidebar()
    # _render_top_header(prj)

    # tab1, tab2, tab3 = st.tabs(["åŸ¹å…»æ–¹æ¡ˆåŸºåº§", "æ¨¡æ¿åŒ–æ•™å­¦æ–‡ä»¶", "é¡¹ç›®æ¦‚è§ˆ"])
    # with tab1:
        # ui_base_training_plan(prj)
    # with tab2:
        # st.info("è¿™é‡Œç•™ç»™ä½ çš„â€œæ¨¡æ¿åŒ–æ•™å­¦æ–‡ä»¶â€æ¨¡å—ï¼ˆä½ åŸæ¥çš„ç”Ÿæˆ/æ ¡å¯¹/å¯¼å‡ºæµç¨‹å¯ä»¥æ”¾å›è¿™é‡Œï¼‰ã€‚")
    # with tab3:
        # st.write("é¡¹ç›®IDï¼š", prj.project_id)
        # st.write("æœ€åæ›´æ–°ï¼š", prj.updated_at)
        # payload = st.session_state.project_data.get(prj.project_id)
        # if payload:
            # st.write("å·²å†™å…¥åŸºåº§ï¼šâœ…")
            # st.write("å·²æŠ½å–é™„è¡¨ï¼š", payload.get("debug", {}).get("assigned", {}))
        # else:
            # st.write("å·²å†™å…¥åŸºåº§ï¼šâŒ")

# app.py


# ============================================================
# LLM æ ¸å¿ƒå¤„ç†æ¨¡å—
# ============================================================
def call_gemini_ai(api_key: str, prompt: str, system_instruction: str = "") -> Any:
    """è°ƒç”¨ Gemini 1.5 Pro å¹¶è¿”å›ç»“æ„åŒ–æ•°æ®"""
    try:
        genai.configure(api_key=api_key)
        # ä½¿ç”¨ 1.5 Flash æˆ– Pro å‡å¯ï¼ŒPro å¯¹é•¿è¡¨æ ¼ç†è§£æ›´ä½³
        model = genai.GenerativeModel(
            model_name="gemini-1.5-flash",
            system_instruction=system_instruction
        )
        
        response = model.generate_content(
            prompt,
            generation_config={"response_mime_type": "application/json"}
        )
        return json.loads(response.text)
    except Exception as e:
        st.error(f"AI æŠ½å–å¤±è´¥: {str(e)}")
        return None

def ai_extract_sections(api_key: str, full_text: str) -> Dict[str, str]:
    """ä½¿ç”¨ AI æå– 1-11 é¡¹æ­£æ–‡"""
    sys_msg = "ä½ æ˜¯ä¸€ä¸ªé«˜æ ¡æ•™åŠ¡ä¸“å®¶ï¼Œè´Ÿè´£ä»åŸ¹å…»æ–¹æ¡ˆä¸­å‡†ç¡®æå–ä¿¡æ¯ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ 1-11 çš„é”®å€¼è¿”å› JSONã€‚"
    prompt = f"""
    è¯·ä»ä»¥ä¸‹åŸ¹å…»æ–¹æ¡ˆåŸå§‹æ–‡æœ¬ä¸­ï¼Œæå–å‡ºå¯¹åº”çš„ 11 ä¸ªæ ç›®å†…å®¹ã€‚
    1: åŸ¹å…»ç›®æ ‡
    2: æ¯•ä¸šè¦æ±‚
    3: ä¸“ä¸šå®šä½ä¸ç‰¹è‰²
    4: ä¸»å¹²å­¦ç§‘/æ ¸å¿ƒè¯¾ç¨‹/å®è·µç¯èŠ‚
    5: æ ‡å‡†å­¦åˆ¶ä¸æˆäºˆå­¦ä½
    6: æ¯•ä¸šæ¡ä»¶
    7-11: ä»…æå–è¿™äº›ç« èŠ‚çš„æ ‡é¢˜å’Œç®€çŸ­æè¿°ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚
    
    åŸå§‹æ–‡æœ¬ï¼š
    {full_text[:15000]} # æˆªå–å‰ 15000 å­—é¿å…è¶…å‡º Token é™åˆ¶
    """
    return call_gemini_ai(api_key, prompt, sys_msg)

def ai_align_table(api_key: str, raw_table_data: List[List[str]], table_type: str) -> pd.DataFrame:
    """ä½¿ç”¨ AI å°†éç»“æ„åŒ–è¡¨æ ¼è¡Œå¯¹é½åˆ°æ ‡å‡†æ¨¡ç‰ˆåˆ—"""
    cols_map = {
        "7": ["è¯¾ç¨‹ä½“ç³»", "è¯¾ç¨‹ç¼–ç ", "è¯¾ç¨‹åç§°", "å¼€è¯¾æ¨¡å¼", "è€ƒæ ¸æ–¹å¼", "å­¦åˆ†", "æ€»å­¦æ—¶", "ä¸Šè¯¾å­¦æœŸ"],
        "8": ["è¯¾ç¨‹ä½“ç³»", "å¿…ä¿®å­¦åˆ†", "é€‰ä¿®å­¦åˆ†", "åˆè®¡", "å­¦åˆ†å æ¯”"],
        "10": ["è¯¾ç¨‹åç§°", "æŒ‡æ ‡ç‚¹1.1", "æŒ‡æ ‡ç‚¹1.2", "æŒ‡æ ‡ç‚¹2.1", "ä»¥æ­¤ç±»æ¨..."]
    }
    target_cols = cols_map.get(table_type, [])
    
    sys_msg = f"ä½ è´Ÿè´£å°†æ··ä¹±çš„ PDF è¡¨æ ¼è¡Œè½¬æ¢æˆæ ‡å‡†çš„ {target_cols} æ ¼å¼ã€‚è¿”å›æ ¼å¼ä¸º [{{...}}, {{...}}]"
    prompt = f"""
    ä»¥ä¸‹æ˜¯ä» PDF é™„è¡¨{table_type}ä¸­æŠ½å–çš„åŸå§‹è¡Œæ•°æ®ã€‚è¯·æ ¹æ®è¯­ä¹‰å°†å…¶æ˜ å°„åˆ°æ ‡å‡†åˆ—ï¼š{target_cols}ã€‚
    å¦‚æœåŸå§‹æ•°æ®è·¨è¡Œæˆ–é”™ä½ï¼Œè¯·æ ¹æ®è¯¾ç¨‹åç§°è¿›è¡Œåˆå¹¶ã€‚
    åŸå§‹æ•°æ®ï¼š{json.dumps(raw_table_data, ensure_ascii=False)}
    """
    result = call_gemini_ai(api_key, prompt, sys_msg)
    if result and isinstance(result, list):
        return pd.DataFrame(result)
    return pd.DataFrame(columns=target_cols)

# ============================================================
# åŸæœ‰ Helper ä¸ JSON åºåˆ—åŒ–ï¼ˆä¿æŒä¸å˜ï¼Œç”¨äºå…¼å®¹æ€§ï¼‰
# ============================================================
def payload_to_jsonable(obj):
    if isinstance(obj, pd.DataFrame):
        return obj.fillna("").to_dict(orient="records")
    if isinstance(obj, (bytes, bytearray)):
        return base64.b64encode(bytes(obj)).decode("ascii")
    if isinstance(obj, (_dt.datetime, _dt.date)):
        return obj.isoformat()
    if isinstance(obj, dict):
        return {str(k): payload_to_jsonable(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [payload_to_jsonable(x) for x in obj]
    return obj

def _compact_lines(s: str) -> str:
    s = (s or "").replace("\u00a0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def _read_pdf_pages_text(pdf_bytes: bytes) -> List[str]:
    pages = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for p in pdf.pages:
            pages.append(_compact_lines(p.extract_text() or ""))
    return pages

# ============================================================
# UI ä¸ ä¸»é€»è¾‘
# ============================================================
def main():
    st.set_page_config(page_title="Teaching Agent Suite AI", layout="wide")
    
    # ä¾§è¾¹æ ï¼šAPI Key é…ç½®
    with st.sidebar:
        st.title("âš™ï¸ è®¾ç½®")
        api_key = st.text_input("Gemini API Key", type="password", help="ä» Google AI Studio è·å–")
        st.divider()
        st.caption("v0.7 (AI Powered)")

    # é¡¹ç›®åˆå§‹åŒ–
    if "project_data" not in st.session_state:
        st.session_state.project_data = {}

    st.header("ğŸ§  æ•™å­¦æ–‡ä»¶æ™ºèƒ½å·¥ä½œå°")
    
    tab1, tab2 = st.tabs(["åŸ¹å…»æ–¹æ¡ˆåŸºåº§ (AI æŠ½å–)", "é¡¹ç›®æ¦‚è§ˆ"])
    
    with tab1:
        col_l, col_r = st.columns([1, 1.5])
        
        with col_l:
            pdf = st.file_uploader("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDF", type=["pdf"])
            use_ai = st.toggle("å¯ç”¨ Gemini AI å¢å¼ºæŠ½å–", value=True)
            
            if st.button("å¼€å§‹æ™ºèƒ½æŠ½å–", type="primary", use_container_width=True):
                if not pdf:
                    st.warning("è¯·ä¸Šä¼  PDF")
                elif use_ai and not api_key:
                    st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® API Key")
                else:
                    with st.spinner("æ­£åœ¨è§£æ PDF å¹¶è¯·æ±‚ AI å¤„ç†..."):
                        pdf_bytes = pdf.getvalue()
                        pages = _read_pdf_pages_text(pdf_bytes)
                        full_text = "\n".join(pages)
                        
                        # 1. åŸºç¡€æ–‡å­—å¤„ç†
                        sections = {}
                        if use_ai:
                            sections = ai_extract_sections(api_key, full_text)
                        
                        # 2. è¡¨æ ¼å¤„ç† (é™„è¡¨ 1 ç¤ºä¾‹)
                        tables = {}
                        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf_obj:
                            # å‡è®¾é™„è¡¨1åœ¨åé¢å‡ é¡µï¼Œé€‰å–æœ‰è¡¨æ ¼çš„é¡µé¢
                            raw_rows = []
                            for p in pdf_obj.pages[-12:]: # æ‰«æå12é¡µæ‰¾è¡¨æ ¼
                                tbl = p.extract_table()
                                if tbl: raw_rows.extend(tbl)
                            
                            if use_ai and raw_rows:
                                tables["7"] = ai_align_table(api_key, raw_rows[:100], "7") # å–å‰100è¡Œæµ‹è¯•
                        
                        st.session_state.project_data = {
                            "sections": sections or {},
                            "tables": tables,
                            "raw_text": full_text
                        }
                        st.success("æŠ½å–å®Œæˆï¼")

        with col_r:
            data = st.session_state.project_data
            if not data:
                st.info("å¾…æŠ½å–æ•°æ®...")
            else:
                sec_list = ["1", "2", "3", "4", "5", "6"]
                choice = st.selectbox("æŸ¥çœ‹æ ç›®", sec_list, format_func=lambda x: f"æ ç›® {x}")
                st.text_area("å†…å®¹", value=data["sections"].get(choice, ""), height=300)
                
                if "7" in data["tables"]:
                    st.markdown("### è‡ªåŠ¨ç”Ÿæˆçš„ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨ (é™„è¡¨1)")
                    st.data_editor(data["tables"]["7"], use_container_width=True)

if __name__ == "__main__":
    main()

