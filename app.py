# app.py
# Teaching Agent Suite (single-file demo)
# - Base plan 1-11 extraction
# - Appendix tables (7-10) auto extraction + classification + page-anchored search
# - Streamlit keys fixed (no DuplicateElementKey / ValueAssignmentNotAllowedError)
# - Sidebar logo fixed (HTML render or upload image)
# - Download JSON fixed (no TypeError / non-serializable)

from __future__ import annotations

import io
import re
import json
import time
import hashlib
import base64
import datetime as _dt
from pathlib import Path
from decimal import Decimal
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional, Any

import pandas as pd
import streamlit as st
import pdfplumber


# ============================================================
# JSON serialization helper
# ============================================================
def payload_to_jsonable(obj):
    """é€’å½’æŠŠå„ç§å¸¸è§ä¸å¯ JSON åºåˆ—åŒ–å¯¹è±¡è½¬æˆå¯åºåˆ—åŒ–ç»“æ„ã€‚"""
    # pandas
    try:
        import pandas as pd

        if isinstance(obj, pd.DataFrame):
            df = obj.copy().fillna("")
            return {
                "__type__": "dataframe",
                "columns": [str(c) for c in df.columns.tolist()],
                "data": df.astype(str).values.tolist(),
            }
        if hasattr(pd, "Timestamp") and isinstance(obj, pd.Timestamp):
            return obj.isoformat()
    except Exception:
        pass

    # numpy
    try:
        import numpy as np

        if isinstance(obj, (np.integer, np.floating, np.bool_)):
            return obj.item()
        if isinstance(obj, np.ndarray):
            return obj.tolist()
    except Exception:
        pass

    # bytesï¼ˆæ¯”å¦‚ pdf_bytesï¼‰
    if isinstance(obj, (bytes, bytearray)):
        return {
            "__type__": "bytes_base64",
            "data": base64.b64encode(bytes(obj)).decode("ascii"),
        }

    # datetime / date
    if isinstance(obj, (_dt.datetime, _dt.date)):
        return obj.isoformat()

    # Path / Decimal
    if isinstance(obj, Path):
        return str(obj)
    if isinstance(obj, Decimal):
        return float(obj)

    # set/tuple
    if isinstance(obj, (set, tuple)):
        return [payload_to_jsonable(x) for x in obj]

    # dict / list
    if isinstance(obj, dict):
        return {str(k): payload_to_jsonable(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [payload_to_jsonable(x) for x in obj]

    # å…¶å®ƒï¼šå°½é‡åŸæ ·è¿”å›ï¼Œå¿…è¦æ—¶è½¬å­—ç¬¦ä¸²
    try:
        json.dumps(obj)  # probe
        return obj
    except Exception:
        return str(obj)


# ============================================================
# Helpers
# ============================================================
def _now_str() -> str:
    return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())


def _short_id(s: str) -> str:
    return hashlib.md5(s.encode("utf-8")).hexdigest()[:10]


def _safe_text(x: Any) -> str:
    if x is None:
        return ""
    return str(x).strip()


def _compact_lines(s: str) -> str:
    s = (s or "").replace("\u00a0", " ")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()


def _join_pages(pages_text: List[str]) -> str:
    return _compact_lines("\n\n".join([t or "" for t in pages_text]))


def _read_pdf_pages_text(pdf_bytes: bytes) -> List[str]:
    pages = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for p in pdf.pages:
            txt = p.extract_text() or ""
            pages.append(_compact_lines(txt))
    return pages


# ============================================================
# Base plan (sections 1-11) text extraction (regex best-effort)
# å…³é”®æ”¹è¿›ï¼šé¿å…ç›®å½•(Toc)å¹²æ‰° -> æ¯ä¸ªæ ‡é¢˜å–â€œæœ€åä¸€æ¬¡å‡ºç°â€çš„ä½ç½®
# ============================================================
_SECTION_PATTERNS: List[Tuple[str, List[str]]] = [
    ("1", [r"ä¸€[ã€\.\s]*åŸ¹å…»ç›®æ ‡", r"1[ã€\.\s]*åŸ¹å…»ç›®æ ‡"]),
    ("2", [r"äºŒ[ã€\.\s]*æ¯•ä¸šè¦æ±‚", r"2[ã€\.\s]*æ¯•ä¸šè¦æ±‚"]),
    ("3", [r"ä¸‰[ã€\.\s]*ä¸“ä¸šå®šä½ä¸ç‰¹è‰²", r"3[ã€\.\s]*ä¸“ä¸šå®šä½ä¸ç‰¹è‰²"]),
    ("4", [r"å››[ã€\.\s]*ä¸»å¹²å­¦ç§‘", r"4[ã€\.\s]*ä¸»å¹²å­¦ç§‘"]),
    ("5", [r"äº”[ã€\.\s]*æ ‡å‡†å­¦åˆ¶ä¸æˆäºˆå­¦ä½", r"5[ã€\.\s]*æ ‡å‡†å­¦åˆ¶"]),
    ("6", [r"å…­[ã€\.\s]*æ¯•ä¸šæ¡ä»¶", r"6[ã€\.\s]*æ¯•ä¸šæ¡ä»¶"]),
    ("7", [r"ä¸ƒ[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨", r"7[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨"]),
    ("8", [r"å…«[ã€\.\s]*å­¦åˆ†ç»Ÿè®¡è¡¨", r"8[ã€\.\s]*å­¦åˆ†ç»Ÿè®¡è¡¨"]),
    ("9", [r"ä¹[ã€\.\s]*æ•™å­¦è¿›ç¨‹è¡¨", r"9[ã€\.\s]*æ•™å­¦è¿›ç¨‹è¡¨"]),
    ("10", [r"å[ã€\.\s]*è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚æ”¯æ’‘å…³ç³»è¡¨", r"10[ã€\.\s]*è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚æ”¯æ’‘å…³ç³»è¡¨"]),
    ("11", [r"åä¸€[ã€\.\s]*è¯¾ç¨‹è®¾ç½®é€»è¾‘æ€ç»´å¯¼å›¾", r"11[ã€\.\s]*è¯¾ç¨‹è®¾ç½®é€»è¾‘æ€ç»´å¯¼å›¾"]),
]


def _find_last_heading_pos(full_text: str, patterns: List[str]) -> Optional[int]:
    """è¿”å›è¯¥æ ‡é¢˜åœ¨å…¨æ–‡ä¸­æœ€åä¸€æ¬¡å‡ºç°çš„ä½ç½®ï¼Œå°½é‡ç»•å¼€å‰é¢çš„ç›®å½•ã€‚"""
    last_pos = None
    for pat in patterns:
        for m in re.finditer(pat, full_text):
            last_pos = m.start()
    return last_pos


def _build_section_spans(full_text: str) -> Dict[str, Tuple[int, int]]:
    """
    Find each section heading position (prefer last occurrence); return char spans [start,end) for each section.
    """
    hits: List[Tuple[str, int]] = []
    for sec_id, pats in _SECTION_PATTERNS:
        pos = _find_last_heading_pos(full_text, pats)
        if pos is not None:
            hits.append((sec_id, pos))

    hits.sort(key=lambda x: x[1])
    spans: Dict[str, Tuple[int, int]] = {}
    for i, (sec_id, start) in enumerate(hits):
        end = hits[i + 1][1] if i + 1 < len(hits) else len(full_text)
        spans[sec_id] = (start, end)
    return spans


def _extract_section_text(full_text: str, spans: Dict[str, Tuple[int, int]], sec_id: str) -> str:
    if sec_id not in spans:
        return ""
    s, e = spans[sec_id]
    chunk = full_text[s:e].strip()

    # å»æ‰æ ‡é¢˜è¡Œè‡ªèº«ï¼ˆå°½é‡ï¼‰
    chunk = re.sub(
        r"^\s*(ä¸€|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹|å|åä¸€|\d+)[ã€\.\s]*[^\n]{0,30}\n",
        "",
        chunk,
    )
    return _compact_lines(chunk)


# ============================================================
# Appendix table extraction (pdfplumber) + classification
# å…³é”®æ”¹è¿›ï¼š
# 1) å…ˆç”¨ pages_text é”šå®šâ€œé™„è¡¨1/2/3/4â€æ‰€åœ¨é¡µï¼Œå†åœ¨é™„è¿‘é¡µæŠ½è¡¨
# 2) æŠ½è¡¨è¿”å›å¸¦ page_idxï¼Œé¿å…ä¸åŒé™„è¡¨äº’ç›¸ä¸²
# 3) æ¯ä¸ªé™„è¡¨å–â€œæœ€åŒ¹é…(é«˜åˆ†)+æ›´å¤§(é¢ç§¯)â€çš„é‚£å¼ 
# ============================================================
def _valid_table_settings_lines() -> dict:
    """Safe pdfplumber settings (avoid TableSettings.resolve TypeError)."""
    return dict(
        vertical_strategy="lines",
        horizontal_strategy="lines",
        snap_tolerance=3,
        join_tolerance=3,
        edge_min_length=3,
        intersection_tolerance=3,
        text_tolerance=3,
    )


def _extract_tables_from_pages(
    pdf_bytes: bytes, page_idx_list: List[int]
) -> List[Tuple[int, List[List[str]]]]:
    """
    Return: list of (page_idx, table_rows)
      table_rows: list of rows; row: list of cell strings
    """
    out: List[Tuple[int, List[List[str]]]] = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for idx in page_idx_list:
            if idx < 0 or idx >= len(pdf.pages):
                continue
            page = pdf.pages[idx]

            tables = []
            try:
                tables = page.extract_tables(table_settings=_valid_table_settings_lines()) or []
            except TypeError:
                tables = page.extract_tables() or []
            except Exception:
                try:
                    tables = page.extract_tables() or []
                except Exception:
                    tables = []

            for t in tables:
                norm = []
                for row in t:
                    norm.append([_safe_text(c) for c in row])
                out.append((idx, norm))
    return out


def _dedup_cols(cols: List[str]) -> List[str]:
    seen = {}
    out = []
    for c in cols:
        c0 = c.strip() or "åˆ—"
        if c0 not in seen:
            seen[c0] = 1
            out.append(c0)
        else:
            seen[c0] += 1
            out.append(f"{c0}_{seen[c0]}")
    return out


def _clean_df(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame()

    df = df.copy()
    df.replace({None: ""}, inplace=True)

    # æŠŠ "nan" æ–‡æœ¬ä¹Ÿæ¸…æ‰
    df = df.applymap(lambda x: "" if str(x).strip().lower() == "nan" else str(x).strip())

    # drop all-empty rows/cols
    df = df.loc[~df.apply(lambda r: all((str(x).strip() == "") for x in r), axis=1)]
    df = df.loc[:, ~df.apply(lambda c: all((str(x).strip() == "") for x in c), axis=0)]
    df = df.reset_index(drop=True)

    # åˆ é™¤â€œå­¦æœŸä¸­æ–‡æ•°å­—è¡Œâ€å™ªå£°ï¼ˆå›› äº” å…­ ä¸ƒ å…«â€¦ï¼‰
    def _looks_like_semester_row(row: pd.Series) -> bool:
        tokens = [str(x).strip() for x in row.tolist() if str(x).strip()]
        if len(tokens) < 3:
            return False
        cn_nums = set(list("ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å"))
        hits = sum(1 for t in tokens if (len(t) == 1 and t in cn_nums))
        return hits >= 3

    if not df.empty:
        df = df.loc[~df.apply(_looks_like_semester_row, axis=1)].reset_index(drop=True)

    return df


def _table_to_df(table_rows: List[List[str]]) -> pd.DataFrame:
    rows = [r for r in table_rows if any(_safe_text(x) for x in r)]
    if not rows:
        return pd.DataFrame()

    max_cols = max(len(r) for r in rows)
    rows = [r + [""] * (max_cols - len(r)) for r in rows]

    header = rows[0]
    header_join = " ".join(header)
    header_like = any(
        k in header_join
        for k in ["è¯¾ç¨‹", "å­¦åˆ†", "å‘¨æ¬¡", "æŒ‡æ ‡", "æ”¯æ’‘", "åˆè®¡", "è¯¾ç¨‹ç¼–ç ", "è¯¾ç¨‹åç§°", "æ¯•ä¸šè¦æ±‚"]
    )
    if header_like:
        cols = [c if c else f"åˆ—{i+1}" for i, c in enumerate(header)]
        df = pd.DataFrame(rows[1:], columns=_dedup_cols(cols))
    else:
        cols = [f"åˆ—{i+1}" for i in range(max_cols)]
        df = pd.DataFrame(rows, columns=cols)

    return _clean_df(df)


def _table_signature_text(df: pd.DataFrame) -> str:
    if df is None or df.empty:
        return ""
    head = " ".join([str(c) for c in df.columns.tolist()])
    top_rows = []
    for i in range(min(3, len(df))):
        top_rows.append(" ".join([str(x) for x in df.iloc[i].tolist()]))
    return (head + " " + " ".join(top_rows)).lower()


def _classify_table(df: pd.DataFrame) -> Tuple[str, int]:
    """
    Return (section_id, score). section_id in {"7","8","9","10"} or ("",0)
    """
    s = _table_signature_text(df)

    score7 = 0
    for k in ["è¯¾ç¨‹ç¼–ç ", "è¯¾ç¨‹ä»£ç ", "è¯¾ç¨‹åç§°", "å­¦åˆ†", "æ€»å­¦æ—¶", "è€ƒæ ¸", "å¼€è¯¾"]:
        if k in s:
            score7 += 3

    score8 = 0
    for k in ["å­¦åˆ†ç»Ÿè®¡", "å¿…ä¿®", "é€‰ä¿®", "é€šè¯†", "ä¸“ä¸š", "å®è·µ", "åˆè®¡", "å°è®¡"]:
        if k in s:
            score8 += 3

    score9 = 0
    for k in ["å‘¨æ¬¡", "æ•™å­¦å†…å®¹", "è¿›åº¦", "ç« èŠ‚", "å­¦æ—¶", "å®éªŒ"]:
        if k in s:
            score9 += 3

    score10 = 0
    for k in ["æ¯•ä¸šè¦æ±‚", "æŒ‡æ ‡ç‚¹", "æ”¯æ’‘", "è¾¾æˆ", "å¯¹åº”", "è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚"]:
        if k in s:
            score10 += 3

    scores = [("7", score7), ("8", score8), ("9", score9), ("10", score10)]
    best = max(scores, key=lambda x: x[1])
    if best[1] >= 6:
        return best
    return ("", 0)


def _find_appendix_anchor_pages(pages_text: List[str]) -> Dict[str, List[int]]:
    """
    åœ¨ pages_text ä¸­å¯»æ‰¾é™„è¡¨1~4 çš„é”šç‚¹é¡µï¼ˆå¯èƒ½å†™æˆâ€œé™„è¡¨ 1â€â€œé™„è¡¨1â€â€œï¼ˆé™„è¡¨1ï¼‰â€ç­‰ï¼‰ã€‚
    è¿”å›: {"7":[...], "8":[...], "9":[...], "10":[...]} çš„é¡µå·åˆ—è¡¨(0-based)
    """
    pats = {
        "7": [r"é™„è¡¨\s*1", r"ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨", r"ä¸ƒ[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨"],
        "8": [r"é™„è¡¨\s*2", r"å­¦åˆ†ç»Ÿè®¡è¡¨", r"å…«[ã€\.\s]*å­¦åˆ†ç»Ÿè®¡è¡¨"],
        "9": [r"é™„è¡¨\s*3", r"æ•™å­¦è¿›ç¨‹è¡¨", r"ä¹[ã€\.\s]*æ•™å­¦è¿›ç¨‹è¡¨"],
        "10": [r"é™„è¡¨\s*4", r"æ”¯æ’‘å…³ç³»è¡¨", r"è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚æ”¯æ’‘å…³ç³»è¡¨", r"å[ã€\.\s]*è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚æ”¯æ’‘å…³ç³»è¡¨"],
    }
    anchors: Dict[str, List[int]] = {k: [] for k in pats.keys()}
    for i, t in enumerate(pages_text):
        tt = t or ""
        for sec, ps in pats.items():
            for p in ps:
                if re.search(p, tt):
                    anchors[sec].append(i)
                    break
    # å»é‡ã€æ’åº
    for k in anchors:
        anchors[k] = sorted(list(set(anchors[k])))
    return anchors


def extract_appendix_tables_best_effort(
    pdf_bytes: bytes, pages_text: List[str]
) -> Tuple[Dict[str, pd.DataFrame], Dict[str, Any]]:
    """
    ä» PDF æœ«å°¾é¡µé¢ + é™„è¡¨é”šç‚¹é¡µé¢æŠ½å–è¡¨æ ¼ï¼Œè‡ªåŠ¨åˆ†ç±»åˆ†é…åˆ° 7-10ã€‚
    Return:
      tables_map: {"7":df, "8":df, "9":df, "10":df}
      debug_meta: helpful debug info
    """
    n = len(pages_text)

    # 1) å…ˆæ‰¾é”šç‚¹é¡µ
    anchors = _find_appendix_anchor_pages(pages_text)

    # 2) ç”Ÿæˆå€™é€‰æŠ½å–é¡µï¼šé”šç‚¹é™„è¿‘ + æœ«å°¾å…œåº•
    page_set = set()
    for sec, idxs in anchors.items():
        for idx in idxs[-3:]:  # åªå–æœ€åå‡ ä¸ªé”šç‚¹ï¼ˆé¿å…ç›®å½•é¡µï¼‰
            for d in [-2, -1, 0, 1, 2, 3]:
                j = idx + d
                if 0 <= j < n:
                    page_set.add(j)

    # å…œåº•ï¼šæœ€å 12 é¡µ
    for j in range(max(0, n - 12), n):
        page_set.add(j)

    page_idx_list = sorted(page_set)

    # 3) æŠ½è¡¨ï¼ˆå¸¦é¡µå·ï¼‰
    raw_tables = _extract_tables_from_pages(pdf_bytes, page_idx_list)

    # 4) å˜æˆ dfï¼Œå¹¶è®°å½•æ¥æºé¡µ
    dfs: List[Tuple[int, pd.DataFrame]] = []
    for page_idx, t in raw_tables:
        df = _table_to_df(t)
        if df is None or df.empty:
            continue
        # è¿‡æ»¤æå°å™ªå£°è¡¨
        if df.shape[0] < 2 and df.shape[1] < 3:
            continue
        dfs.append((page_idx, df))

    # 5) è¯„åˆ†+åˆ†ç±»
    candidates: List[Dict[str, Any]] = []
    for i, (pidx, df) in enumerate(dfs):
        sec, score = _classify_table(df)
        area = int(df.shape[0] * df.shape[1])
        candidates.append(
            dict(
                i=i,
                page=pidx,
                sec=sec,
                score=score,
                area=area,
                shape=list(df.shape),
            )
        )

    # 6) åˆ†é…ï¼šä¼˜å…ˆ â€œscoreé«˜â€ï¼Œå†çœ‹â€œåŒåˆ†æ›´å¤§é¢ç§¯â€ï¼Œå†åå‘â€œé è¿‘è¯¥é™„è¡¨é”šç‚¹é¡µâ€
    assigned: Dict[str, pd.DataFrame] = {}
    chosen_idx = set()

    def _anchor_distance(sec: str, page: int) -> int:
        idxs = anchors.get(sec, [])
        if not idxs:
            return 9999
        # è·ç¦»æœ€è¿‘çš„é”šç‚¹
        return min(abs(page - a) for a in idxs)

    # ä¸ºæ¯ä¸ªå€™é€‰ç”Ÿæˆä¸€ä¸ªå…¨å±€æ’åºé”®
    ranked = []
    for c in candidates:
        sec = c["sec"]
        if not sec:
            continue
        dist = _anchor_distance(sec, c["page"])
        # æ’åºï¼šscore desc, area desc, dist asc
        ranked.append((c["sec"], c["i"], c["score"], c["area"], dist, c["page"]))
    ranked.sort(key=lambda x: (x[2], x[3], -x[4] * 0), reverse=True)  # å…ˆç²—æ’
    # æ›´ä¸¥æ ¼ï¼šæˆ‘ä»¬è‡ªå·±å†æŒ‰è§„åˆ™æŒ‘ï¼ˆæ¯ä¸ª sec é€‰æœ€ä¼˜ï¼‰
    for sec in ["7", "8", "9", "10"]:
        # sec ä¸“å±å€™é€‰
        sec_cands = []
        for c in candidates:
            if c["sec"] != sec:
                continue
            dist = _anchor_distance(sec, c["page"])
            sec_cands.append((c["score"], c["area"], -dist, c["page"], c["i"]))
        # scoreé«˜ã€é¢ç§¯å¤§ã€è·ç¦»è¿‘ä¼˜å…ˆ
        sec_cands.sort(reverse=True)
        for score, area, neg_dist, page, i in sec_cands:
            if i in chosen_idx:
                continue
            assigned[sec] = dfs[i][1].copy(deep=True)
            chosen_idx.add(i)
            break

    # 7) å¦‚æœæŸäº› sec æ²¡åˆ†åˆ°ï¼Œä½†æœ‰é”šç‚¹é¡µï¼šå°±ä»é”šç‚¹é¡µé™„è¿‘çš„æ‰€æœ‰è¡¨é‡ŒæŒ‘â€œæœ€å¤§â€çš„é‚£å¼ ä½œä¸ºå…œåº•
    for sec in ["7", "8", "9", "10"]:
        if sec in assigned:
            continue
        idxs = anchors.get(sec, [])
        if not idxs:
            continue
        near_pages = set()
        for idx in idxs[-2:]:
            for d in [-2, -1, 0, 1, 2, 3]:
                j = idx + d
                if 0 <= j < n:
                    near_pages.add(j)
        best_i = None
        best_area = -1
        for i, (pidx, df) in enumerate(dfs):
            if i in chosen_idx:
                continue
            if pidx not in near_pages:
                continue
            area = int(df.shape[0] * df.shape[1])
            if area > best_area:
                best_area = area
                best_i = i
        if best_i is not None:
            assigned[sec] = dfs[best_i][1].copy(deep=True)
            chosen_idx.add(best_i)

    debug = {
        "candidate_pages": page_idx_list,
        "anchors": anchors,
        "raw_tables_count": len(raw_tables),
        "dfs_count": len(dfs),
        "candidates_top": sorted(candidates, key=lambda x: (x["score"], x["area"]), reverse=True)[:30],
        "assigned": {k: list(v.shape) for k, v in assigned.items()},
    }
    return assigned, debug


def base_plan_from_pdf(pdf_bytes: bytes) -> Dict[str, Any]:
    pages = _read_pdf_pages_text(pdf_bytes)
    full = _join_pages(pages)
    spans = _build_section_spans(full)

    base = {}
    for sec_id, _ in _SECTION_PATTERNS:
        base[sec_id] = _extract_section_text(full, spans, sec_id)

    # 7-11 æ­£æ–‡å¯èƒ½åªæœ‰æ ‡é¢˜ï¼šæç¤º
    for sec_id in ["7", "8", "9", "10", "11"]:
        if not base.get(sec_id, "").strip():
            base[sec_id] = f"{sec_id}ï¼šæ­£æ–‡å¯èƒ½ä»…æœ‰æ ‡é¢˜ï¼›è¯·å°è¯•ä» PDF æœ«å°¾é™„è¡¨è‡ªåŠ¨æŠ½å–ã€‚"

    auto_tables, debug_meta = extract_appendix_tables_best_effort(pdf_bytes, pages)

    return dict(
        pages=pages,
        full_text=full,
        sections=base,              # 1-11 text
        tables=auto_tables,         # 7-10 tables
        debug=debug_meta,
    )


# ============================================================
# UI
# ============================================================
@dataclass
class Project:
    project_id: str
    name: str
    updated_at: str


def _init_state():
    if "projects" not in st.session_state:
        pid = _short_id(_now_str())
        st.session_state.projects = [
            Project(project_id=pid, name=f"é»˜è®¤é¡¹ç›®-{time.strftime('%Y%m%d-%H%M')}", updated_at=_now_str())
        ]
        st.session_state.active_project_id = pid

    if "project_data" not in st.session_state:
        st.session_state.project_data = {}

    if "logo_bytes" not in st.session_state:
        st.session_state.logo_bytes = None


def ui_sidebar_brand():
    with st.sidebar:
        col1, col2 = st.columns([1, 4])
        with col1:
            if st.session_state.logo_bytes:
                st.image(st.session_state.logo_bytes, width=44)
            else:
                # âœ… ä¸å†ç”¨ components.htmlï¼ˆæœ‰æ—¶ä¼šæ˜¾ç¤ºæˆæ–‡æœ¬/æˆ–è§¦å‘ sidebar.components ç›¸å…³é—®é¢˜ï¼‰
                # âœ… ç”¨ markdown + unsafe_allow_html 100%ç¨³
                svg = """
                <div style="width:44px;height:44px;border-radius:50%;
                            background:#2f6fed;display:flex;align-items:center;justify-content:center;
                            color:white;font-weight:800;font-family:Arial;">
                  TA
                </div>
                """
                st.markdown(svg, unsafe_allow_html=True)

        with col2:
            st.markdown("**Teaching Agent Suite**")
            st.caption("v0.6 (base 1â€“11 + appendix tables + logo fixed)")

        up = st.file_uploader("ä¸Šä¼  Logoï¼ˆå¯é€‰ï¼Œpng/jpgï¼‰", type=["png", "jpg", "jpeg"], key="logo_uploader")
        if up is not None:
            st.session_state.logo_bytes = up.getvalue()


def ui_project_sidebar() -> Project:
    ui_sidebar_brand()

    with st.sidebar:
        st.divider()
        st.markdown("### é¡¹ç›®")
        options = {p.project_id: p for p in st.session_state.projects}
        labels = {p.project_id: f"{p.name} ({p.project_id})" for p in st.session_state.projects}

        pid = st.selectbox(
            "é€‰æ‹©é¡¹ç›®",
            options=list(labels.keys()),
            format_func=lambda x: labels[x],
            index=list(labels.keys()).index(st.session_state.active_project_id),
            key="project_select",
        )
        st.session_state.active_project_id = pid
        return options[pid]


def _render_top_header(project: Project):
    # âœ… å¿…é¡» unsafe_allow_html=Trueï¼Œå¦åˆ™ä¼šæŠŠ HTML å½“çº¯æ–‡æœ¬æ˜¾ç¤º
    html = f"""
    <div style="border:1px solid #e7eefc; background:#f6f9ff; padding:18px 18px; border-radius:14px;">
      <div style="font-weight:900; font-size:28px;">æ•™å­¦æ–‡ä»¶å·¥ä½œå°</div>
      <div style="color:#666; margin-top:4px; font-size:14px;">
        é¡¹ç›®ï¼š <b>{project.name}</b>ï¼ˆ{project.project_id}ï¼‰ Â· æœ€åæ›´æ–°ï¼š {project.updated_at}
      </div>
    </div>
    """
    st.markdown(html, unsafe_allow_html=True)


def ui_base_training_plan(project: Project):
    st.subheader("åŸ¹å…»æ–¹æ¡ˆåŸºåº§ï¼ˆå…¨é‡å†…å®¹åº“ï¼‰")
    st.caption("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDF â†’ æŠ½å–å¡«å…… 1â€“11 â†’ å¹¶å°è¯•ä»æœ«å°¾é™„è¡¨è‡ªåŠ¨æŠ½è¡¨å¡«å…… 7â€“10ã€‚")

    left, right = st.columns([1, 1.4], gap="large")

    with left:
        pdf = st.file_uploader("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDFï¼ˆå¯é€‰ï¼‰", type=["pdf"], key=f"pdf_{project.project_id}")

        if st.button("æŠ½å–å¹¶å†™å…¥åŸºåº§", use_container_width=True, type="primary", key=f"extract_btn_{project.project_id}"):
            if not pdf:
                st.warning("è¯·å…ˆä¸Šä¼  PDFã€‚")
            else:
                pdf_bytes = pdf.getvalue()
                payload = base_plan_from_pdf(pdf_bytes)
                st.session_state.project_data[project.project_id] = payload

                # æ›´æ–°æ—¶é—´
                for i, p in enumerate(st.session_state.projects):
                    if p.project_id == project.project_id:
                        st.session_state.projects[i] = Project(
                            project_id=p.project_id,
                            name=p.name,
                            updated_at=_now_str(),
                        )
                        break

                st.success("å·²æŠ½å–å¹¶å†™å…¥åŸºåº§ã€‚å³ä¾§å·²è”åŠ¨å¡«å……ã€‚")

        # ä¸‹è½½ JSONï¼ˆâœ… ä¿®å¤ï¼šä¸èƒ½åœ¨ download_button å‚æ•°é‡Œä¹±å†™èµ‹å€¼ï¼›åŒæ—¶å…ˆåš jsonableï¼‰
        payload = st.session_state.project_data.get(project.project_id)
        if payload:
            json_payload = payload_to_jsonable(payload)
            st.download_button(
                "ä¸‹è½½åŸºåº§ JSON",
                data=json.dumps(json_payload, ensure_ascii=False, indent=2).encode("utf-8"),
                file_name=f"base_{project.project_id}.json",
                mime="application/json",
                use_container_width=True,
                key=f"dl_{project.project_id}",
            )

        st.divider()
        if payload:
            missing = [k for k in [str(i) for i in range(1, 12)] if not payload["sections"].get(k, "").strip()]
            if missing:
                st.warning(f"æ£€æŸ¥ï¼šç¼ºå°‘æ ç›® {missing}")
            else:
                st.success("1â€“11 æ ç›®å‡å·²å­˜åœ¨ï¼ˆä»å»ºè®®äººå·¥å¿«é€Ÿæ‰«è¯»ï¼‰ã€‚")

        with st.expander("è°ƒè¯•ï¼šåˆ†é¡µåŸæ–‡ (raw_pages_text)"):
            if payload:
                st.write(payload["pages"])
            else:
                st.info("å…ˆæŠ½å–åå¯è§ã€‚")

        with st.expander("è°ƒè¯•ï¼šé™„è¡¨æŠ½å–ä¿¡æ¯ (appendix_debug)"):
            if payload:
                st.json(payload["debug"])
            else:
                st.info("å…ˆæŠ½å–åå¯è§ã€‚")

    with right:
        st.markdown("#### åŸ¹å…»æ–¹æ¡ˆå†…å®¹ï¼ˆæŒ‰æ ç›®å±•ç¤ºï¼Œå¯ç¼–è¾‘ï¼‰")

        payload = st.session_state.project_data.get(project.project_id)
        if not payload:
            st.info("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼  PDF å¹¶ç‚¹å‡»â€œæŠ½å–å¹¶å†™å…¥åŸºåº§â€ã€‚")
            return

        sections = payload["sections"]
        tables = payload.get("tables", {})

        toc = [
            ("1", "åŸ¹å…»ç›®æ ‡"),
            ("2", "æ¯•ä¸šè¦æ±‚"),
            ("3", "ä¸“ä¸šå®šä½ä¸ç‰¹è‰²"),
            ("4", "ä¸»å¹²å­¦ç§‘/æ ¸å¿ƒè¯¾ç¨‹/å®è·µç¯èŠ‚"),
            ("5", "æ ‡å‡†å­¦åˆ¶ä¸æˆäºˆå­¦ä½"),
            ("6", "æ¯•ä¸šæ¡ä»¶"),
            ("7", "ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨ï¼ˆé™„è¡¨1ï¼‰"),
            ("8", "å­¦åˆ†ç»Ÿè®¡è¡¨ï¼ˆé™„è¡¨2ï¼‰"),
            ("9", "æ•™å­¦è¿›ç¨‹è¡¨ï¼ˆé™„è¡¨3ï¼‰"),
            ("10", "æ”¯æ’‘å…³ç³»è¡¨ï¼ˆé™„è¡¨4ï¼‰"),
            ("11", "é€»è¾‘æ€ç»´å¯¼å›¾ï¼ˆé™„è¡¨5ï¼‰"),
        ]
        title_map = dict(toc)

        sec_pick = st.radio(
            "æ ç›®",
            options=[x[0] for x in toc],
            format_func=lambda x: title_map[x],
            horizontal=True,
            key=f"sec_radio_{project.project_id}",
        )

        st.markdown(f"##### {sec_pick}ã€{title_map[sec_pick]}")

        # 6 å†…å®¹è¿‡é•¿å…œåº•æˆªæ–­ï¼šé‡åˆ° â€œä¸ƒã€ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨â€ å°±æˆªæ–­
        def _truncate_at_next_heading(txt: str, next_sec_id: str) -> str:
            if not txt:
                return ""
            next_title = title_map.get(next_sec_id, "")
            if not next_title:
                return txt
            # å…¼å®¹ â€œä¸ƒã€ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨â€ æˆ– â€œ7 ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨â€
            pat = rf"(\n\s*ä¸ƒ[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨|\n\s*7[ã€\.\s]*ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨)"
            m = re.search(pat, "\n" + txt)
            if m:
                return _compact_lines(txt[: m.start()])
            return txt

        txt = sections.get(sec_pick, "")
        if sec_pick == "6":
            txt = _truncate_at_next_heading(txt, "7")

        st.text_area(
            f"{sec_pick} æ–‡æœ¬æŠ½å–ç»“æœ",
            value=txt,
            height=220,
            key=f"sec_text_{project.project_id}_{sec_pick}",
        )

        # 7-10ï¼šè¡¨æ ¼åŒºï¼ˆè‡ªåŠ¨æŠ½å–ï¼‰
        if sec_pick in ["7", "8", "9", "10"]:
            st.markdown("###### è¡¨æ ¼åŒºï¼ˆå¯ç¼–è¾‘ï¼Œè¡Œå¯å¢åˆ ï¼‰")

            df0 = tables.get(sec_pick)
            if df0 is None or (isinstance(df0, pd.DataFrame) and df0.empty):
                st.info("æœªè‡ªåŠ¨æŠ½å–åˆ°è¯¥é™„è¡¨ï¼ˆå¯èƒ½ PDF è¡¨æ ¼æ˜¯å›¾ç‰‡/çº¿æ¡ä¸è§„åˆ™/æˆ–é™„è¡¨å¸ƒå±€ç‰¹æ®Šï¼‰ã€‚ä½ å¯ä»¥æ‰‹å·¥è¡¥å…¨ã€‚")
                df0 = pd.DataFrame()

            editor_key = f"tbl_editor_{project.project_id}_{sec_pick}"
            edited = st.data_editor(
                df0,
                num_rows="dynamic",
                use_container_width=True,
                key=editor_key,
            )
            # âœ… ä¸è¦†ç›– widget keyï¼Œå¦å­˜ä¸€ä»½
            st.session_state[f"{editor_key}__value"] = edited

        if sec_pick == "11":
            st.info("é€»è¾‘æ€ç»´å¯¼å›¾ï¼ˆé™„è¡¨5ï¼‰é€šå¸¸æ˜¯å›¾ç‰‡/æµç¨‹å›¾ï¼Œpdfplumber çš„è¡¨æ ¼æŠ½å–ä¸ä¸€å®šæœ‰æ•ˆã€‚å¯åç»­åŠ â€œæœ«é¡µå›¾ç‰‡æŠ½å–â€ã€‚")


def main():
    st.set_page_config(page_title="Teaching Agent Suite", page_icon="ğŸ§ ", layout="wide")
    _init_state()

    prj = ui_project_sidebar()
    _render_top_header(prj)

    tab1, tab2, tab3 = st.tabs(["åŸ¹å…»æ–¹æ¡ˆåŸºåº§", "æ¨¡æ¿åŒ–æ•™å­¦æ–‡ä»¶", "é¡¹ç›®æ¦‚è§ˆ"])
    with tab1:
        ui_base_training_plan(prj)
    with tab2:
        st.info("è¿™é‡Œç•™ç»™ä½ çš„â€œæ¨¡æ¿åŒ–æ•™å­¦æ–‡ä»¶â€æ¨¡å—ï¼ˆä½ åŸæ¥çš„ç”Ÿæˆ/æ ¡å¯¹/å¯¼å‡ºæµç¨‹å¯ä»¥æ”¾å›è¿™é‡Œï¼‰ã€‚")
    with tab3:
        st.write("é¡¹ç›®IDï¼š", prj.project_id)
        st.write("æœ€åæ›´æ–°ï¼š", prj.updated_at)
        payload = st.session_state.project_data.get(prj.project_id)
        if payload:
            st.write("å·²å†™å…¥åŸºåº§ï¼šâœ…")
            st.write("å·²æŠ½å–é™„è¡¨ï¼š", payload.get("debug", {}).get("assigned", {}))
        else:
            st.write("å·²å†™å…¥åŸºåº§ï¼šâŒ")


if __name__ == "__main__":
    main()
