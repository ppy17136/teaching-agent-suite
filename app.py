# -*- coding: utf-8 -*-
"""åŸ¹å…»æ–¹æ¡ˆ PDF å…¨é‡æŠ½å–ï¼ˆæ–‡æœ¬ + è¡¨æ ¼ + ç»“æ„åŒ–è§£æï¼‰

ä½ å…³å¿ƒçš„ç‚¹ï¼š
- æ¯•ä¸šè¦æ±‚å¿…é¡»å®Œæ•´ï¼ˆ1~12 + 1.1/1.2â€¦ï¼‰
- ä¸‰~å…­ç­‰ç« èŠ‚å¤§æ ‡é¢˜å†…å®¹è¦å®Œæ•´æ˜¾ç¤º
- é™„è¡¨ 1~5 çš„è¡¨åè¦æ˜¾ç¤ºï¼Œå¹¶å°½å¯èƒ½æŠŠè¡¨æ ¼æŠ½å–å‡ºæ¥
- è¡¨æ ¼ä¸­åˆå¹¶å•å…ƒæ ¼å¯¼è‡´çš„ç©ºç™½è¦å°½é‡è¡¥å…¨
- ç„Šæ¥/æ— æŸæ£€æµ‹ä¸¤æ–¹å‘è¦å°½é‡åœ¨å±•ç¤ºä¸å¯¼å‡ºé‡ŒåŒºåˆ†

å®ç°ç­–ç•¥ï¼ˆä¸ä¾èµ–å¤§æ¨¡å‹ï¼‰ï¼š
- ä½¿ç”¨ pdfplumber æŠ½å–æ¯é¡µæ–‡æœ¬ï¼ˆåˆ†é¡µåŸæ–‡å¯æº¯æºï¼‰
- ç”¨è§„åˆ™è§£æâ€œåŸ¹å…»ç›®æ ‡/æ¯•ä¸šè¦æ±‚/ç« èŠ‚å†…å®¹â€
- ç”¨ pdfplumber çº¿æ¡†ç­–ç•¥æŠ½å–è¡¨æ ¼ï¼ˆæ— éœ€ camelot/ghostscriptï¼‰
- å¯¹è¡¨æ ¼åšâ€œè¡Œé•¿åº¦å¯¹é½ + ç©ºåˆ—å‰”é™¤ + åˆå¹¶æ ¼å¸¸è§ç©ºç™½å¡«å…… + æ–¹å‘æ¨æ–­â€
- æä¾› JSON / CSV(zip) / Excel ä¸‰ç§å¯¼å‡º

å¤‡æ³¨ï¼šOCR å¼€å…³ä¿ç•™ï¼Œä½† Streamlit Cloud è‹¥æœªå®‰è£… OCR ä¾èµ–å°†è‡ªåŠ¨é™çº§ä¸ºä»…æ–‡æœ¬æŠ½å–ã€‚
"""

from __future__ import annotations

import io
import json
import re
import hashlib
import zipfile
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import requests
import streamlit as st

try:
    import pdfplumber
except Exception as e:  # pragma: no cover
    pdfplumber = None

# -----------------------------

# -----------------------------
# Optional LLM refinement (for post-processing)
# -----------------------------

@dataclass
class LLMConfig:
    enabled: bool = False
    provider: str = "openai_compat"  # "openai_compat" | "dashscope"
    model: str = ""
    api_key: str = ""
    base_url: str = ""  # only for openai_compat
    timeout: int = 60
    temperature: float = 0.0
    max_tokens: int = 2000


def _get_secret(key: str, default: str = "") -> str:
    try:
        if hasattr(st, "secrets") and key in st.secrets:
            return str(st.secrets.get(key, default))
    except Exception:
        pass
    return os.environ.get(key, default)


def _openai_compat_url(base_url: str) -> str:
    base_url = (base_url or "").strip()
    if not base_url:
        return ""
    if base_url.endswith("/chat/completions") or base_url.endswith("/v1/chat/completions"):
        return base_url
    if base_url.endswith("/v1"):
        return base_url.rstrip("/") + "/chat/completions"
    return base_url.rstrip("/") + "/v1/chat/completions"


def llm_chat(messages: List[Dict[str, str]], cfg: LLMConfig) -> str:
    if not cfg.enabled:
        return ""
    if not cfg.model:
        raise ValueError("LLM å·²å¯ç”¨ï¼Œä½†æœªå¡«å†™ modelã€‚")
    if not cfg.api_key:
        raise ValueError("LLM å·²å¯ç”¨ï¼Œä½†æœªå¡«å†™ API Keyã€‚")

    provider = (cfg.provider or "openai_compat").lower().strip()

    if provider == "dashscope":
        try:
            import dashscope  # type: ignore
            from dashscope import Generation  # type: ignore
        except Exception as e:
            raise RuntimeError("æœªå®‰è£… dashscope SDKï¼ˆrequirements.txt éœ€è¦åŠ å…¥ dashscopeï¼‰ã€‚") from e

        dashscope.api_key = cfg.api_key
        resp = Generation.call(
            model=cfg.model,
            messages=messages,
            result_format="message",
            temperature=cfg.temperature,
            max_tokens=cfg.max_tokens,
        )
        try:
            return resp.output.choices[0].message["content"]
        except Exception:
            return str(resp)

    # openai-compatible
    url = _openai_compat_url(cfg.base_url)
    if not url:
        raise ValueError("OpenAI-compatible æ¨¡å¼éœ€è¦å¡«å†™ base_urlï¼ˆä¾‹å¦‚ .../v1 æˆ–å®Œæ•´ chat/completions ç«¯ç‚¹ï¼‰ã€‚")

    headers = {"Authorization": f"Bearer {cfg.api_key}", "Content-Type": "application/json"}
    payload = {
        "model": cfg.model,
        "messages": messages,
        "temperature": cfg.temperature,
        "max_tokens": cfg.max_tokens,
    }
    r = requests.post(url, headers=headers, json=payload, timeout=cfg.timeout)
    r.raise_for_status()
    data = r.json()
    try:
        return data["choices"][0]["message"]["content"]
    except Exception:
        return json.dumps(data, ensure_ascii=False)


def _extract_context_snippets(pages_text: List[str]) -> Dict[str, str]:
    all_text = "\n".join([t or "" for t in pages_text])

    def clip_around(keyword: str, window: int = 3500) -> str:
        idx = all_text.find(keyword)
        if idx < 0:
            return ""
        s = max(0, idx - window // 3)
        e = min(len(all_text), idx + window)
        return all_text[s:e]

    return {
        "objectives": clip_around("åŸ¹å…»ç›®æ ‡"),
        "grad_reqs": clip_around("æ¯•ä¸šè¦æ±‚"),
        "sections": clip_around("ä¸“ä¸šå®šä½") + "\n" + clip_around("ä¸»å¹²å­¦ç§‘") + "\n" + clip_around("æ ‡å‡†å­¦åˆ¶") + "\n" + clip_around("æ¯•ä¸šæ¡ä»¶"),
        "tables": clip_around("é™„è¡¨"),
    }


def llm_refine_result(result: Dict[str, Any], pages_text: List[str], cfg: LLMConfig) -> Dict[str, Any]:
    snippets = _extract_context_snippets(pages_text)

    system = (
        "ä½ æ˜¯é«˜æ ¡åŸ¹å…»æ–¹æ¡ˆç»“æ„åŒ–æŠ½å–çš„æ ¡å¯¹åŠ©æ‰‹ã€‚\n"
        "åœ¨ä¸æé€ å†…å®¹çš„å‰æä¸‹ï¼ŒåŸºäºæä¾›çš„ PDF ç‰‡æ®µä¸å·²æŠ½å–ç»“æœï¼Œä¿®æ­£/è¡¥å…¨ï¼š\n"
        "1) åŸ¹å…»ç›®æ ‡ objectivesï¼›\n"
        "2) æ¯•ä¸šè¦æ±‚ graduate_requirementsï¼ˆå¿…é¡»åŒ…å« 1-12 å¤§æ¡åŠå­æ¡ 1.1..ï¼‰ï¼›\n"
        "3) å¤§æ ‡é¢˜ sectionsï¼ˆå¦‚ ä¸‰ã€ä¸“ä¸šå®šä½ä¸ç‰¹è‰² ç­‰ï¼‰ï¼›\n"
        "4) é™„è¡¨æ ‡é¢˜ appendix_titlesï¼ˆé™„è¡¨1..5 å¯¹åº”çš„ä¸­æ–‡è¡¨åï¼‰ã€‚\n"
        "åªè¾“å‡º JSONï¼Œä¸è¦è¾“å‡ºå…¶å®ƒæ–‡å­—ã€‚"
    )

    compact = {
        "objectives": result.get("objectives", []),
        "graduate_requirements": result.get("graduate_requirements", []),
        "sections": result.get("sections", []),
        "appendix_titles": result.get("appendix_titles", {}),
    }

    user_payload = {
        "pdf_snippets": snippets,
        "extracted": compact,
        "output_schema": {
            "objectives": ["string", "..."],
            "graduate_requirements": [{"id": "1..12", "title": "string", "text": "string", "sub": [{"id": "1.1", "text": "string"}]}],
            "sections": [{"no": "ä¸‰", "title": "string", "text": "string"}],
            "appendix_titles": {"é™„è¡¨1": "string", "é™„è¡¨2": "string", "é™„è¡¨3": "string", "é™„è¡¨4": "string", "é™„è¡¨5": "string"},
        },
    }

    content = llm_chat(
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": json.dumps(user_payload, ensure_ascii=False)},
        ],
        cfg=cfg,
    )

    fixed = json.loads(content)

    for k in ["objectives", "graduate_requirements", "sections", "appendix_titles"]:
        if k in fixed and fixed[k]:
            result[k] = fixed[k]
    result["_llm_refined"] = True
    return result

# Utilities
# -----------------------------

def sha256_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()


def clean_text(s: Any) -> str:
    if s is None:
        return ""
    return str(s).replace("\u00a0", " ").strip()


def is_empty(s: Any) -> bool:
    return clean_text(s) == ""


def safe_int(x: str, default: int = 0) -> int:
    try:
        return int(x)
    except Exception:
        return default


def normalize_lines(text: str) -> List[str]:
    lines = [clean_text(x) for x in (text or "").splitlines()]
    return [x for x in lines if x]


# -----------------------------
# Text extraction
# -----------------------------

def extract_pages_text(pdf_bytes: bytes) -> List[str]:
    if pdfplumber is None:
        raise RuntimeError("pdfplumber æœªå®‰è£…ï¼Œæ— æ³•è§£æ PDF")

    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        pages = []
        for p in pdf.pages:
            pages.append(p.extract_text() or "")
        return pages


# -----------------------------
# Section parsing (rule-based)
# -----------------------------

CHAPTER_KEYS = [
    ("ä¸€", "åŸ¹å…»ç›®æ ‡"),
    ("äºŒ", "æ¯•ä¸šè¦æ±‚"),
    ("ä¸‰", "ä¸“ä¸šå®šä½ä¸ç‰¹è‰²"),
    ("å››", "ä¸»å¹²å­¦ç§‘ã€ä¸“ä¸šæ ¸å¿ƒè¯¾ç¨‹å’Œä¸»è¦å®è·µæ€§æ•™å­¦ç¯èŠ‚"),
    ("äº”", "æ ‡å‡†å­¦åˆ¶ä¸æˆäºˆå­¦ä½"),
    ("å…­", "æ¯•ä¸šæ¡ä»¶"),
    ("ä¸ƒ", "ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨"),
    ("å…«", "å­¦åˆ†ç»Ÿè®¡è¡¨"),
    ("ä¹", "æ•™å­¦è¿›ç¨‹è¡¨"),
    ("å", "è¯¾ç¨‹è®¾ç½®å¯¹æ¯•ä¸šè¦æ±‚æ”¯æ’‘å…³ç³»è¡¨"),
    ("åä¸€", "è¯¾ç¨‹è®¾ç½®é€»è¾‘æ€ç»´å¯¼å›¾"),
]


def find_first_index(pages_text: List[str], pattern: str) -> Optional[int]:
    for i, t in enumerate(pages_text):
        if re.search(pattern, t):
            return i
    return None


def concat_pages(pages_text: List[str], start: int, end: int) -> str:
    start = max(0, start)
    end = min(len(pages_text), end)
    return "\n".join(pages_text[start:end])


def locate_chapter_ranges(pages_text: List[str]) -> Dict[str, Tuple[int, int]]:
    """Return {chapter_name: (start_page_idx, end_page_idx_exclusive)}"""
    # locate page indices by chapter headings
    hits: List[Tuple[int, str]] = []
    for cn, title in CHAPTER_KEYS:
        # match like "ä¸‰ã€ä¸“ä¸šå®šä½ä¸ç‰¹è‰²" or "ä¸‰ ä¸“ä¸šå®šä½ä¸ç‰¹è‰²"
        pat = rf"{cn}[ã€\s]+{re.escape(title)}"
        idx = find_first_index(pages_text, pat)
        if idx is not None:
            hits.append((idx, title))

    # sort by page
    hits.sort(key=lambda x: x[0])
    ranges: Dict[str, Tuple[int, int]] = {}
    for k, (idx, title) in enumerate(hits):
        end = hits[k + 1][0] if k + 1 < len(hits) else len(pages_text)
        ranges[title] = (idx, end)
    return ranges


def parse_training_objectives(section_text: str) -> List[str]:
    """Parse åŸ¹å…»ç›®æ ‡ items like "1." or "ï¼ˆ1ï¼‰" or "1ï¼‰"."""
    text = section_text or ""
    # Try common patterns
    lines = normalize_lines(text)
    items: List[str] = []

    buf = []
    cur_id = None

    def flush():
        nonlocal buf, cur_id
        if cur_id is not None and buf:
            items.append("".join(buf).strip())
        buf = []
        cur_id = None

    for ln in lines:
        m = re.match(r"^\s*(\d+)[\.ã€]\s*(.+)$", ln)
        m2 = re.match(r"^\s*[ï¼ˆ(]\s*(\d+)\s*[ï¼‰)]\s*(.+)$", ln)
        if m or m2:
            flush()
            cur_id = (m or m2).group(1)
            buf = [f"{cur_id}. {(m or m2).group(2).strip()} "]
        else:
            if cur_id is None:
                continue
            buf.append(ln.strip() + " ")

    flush()

    # å¦‚æœä¸€ä¸ªéƒ½æ²¡æŠ“åˆ°ï¼Œé€€åŒ–ï¼šå–â€œåŸ¹å…»ç›®æ ‡â€ä¸‹é¢çš„æ®µè½ï¼ˆä½†åšåˆ†å¥ï¼‰
    if not items:
        text2 = re.sub(r"\s+", " ", text).strip()
        if text2:
            items = [x.strip() for x in re.split(r"[ï¼›;]\s*", text2) if x.strip()]
    return items


def parse_graduation_requirements(section_text: str) -> Dict[str, Any]:
    """Parse æ¯•ä¸šè¦æ±‚ 1~12 and sub-items 1.1/1.2..."""
    lines = normalize_lines(section_text)
    out: Dict[str, Any] = {}

    cur_main = None
    cur_sub = None

    def ensure_main(mid: str, title: str = ""):
        if mid not in out:
            out[mid] = {"title": title, "text": "", "subs": {}}

    for ln in lines:
        # main: "1. å·¥ç¨‹çŸ¥è¯†ï¼š..." å…è®¸å†’å·ä¸­è‹±æ–‡
        m = re.match(r"^\s*(\d{1,2})[\.ã€]\s*([^ï¼š:]+)[ï¼š:]\s*(.*)$", ln)
        if m:
            cur_main = m.group(1)
            cur_sub = None
            ensure_main(cur_main, clean_text(m.group(2)))
            tail = clean_text(m.group(3))
            if tail:
                out[cur_main]["text"] = (out[cur_main]["text"] + " " + tail).strip()
            continue

        # sub: "1.1 èƒ½å¤Ÿ..." or "10.2 ..."
        m2 = re.match(r"^\s*(\d{1,2}\.\d{1,2})\s+(.+)$", ln)
        if m2:
            cur_sub = m2.group(1)
            mid = cur_sub.split(".")[0]
            ensure_main(mid)
            out[mid]["subs"][cur_sub] = clean_text(m2.group(2))
            continue

        # continuation lines
        if cur_sub:
            mid = cur_sub.split(".")[0]
            out[mid]["subs"][cur_sub] = (out[mid]["subs"][cur_sub] + " " + ln).strip()
        elif cur_main:
            out[cur_main]["text"] = (out[cur_main]["text"] + " " + ln).strip()

    return out


def parse_chapter_content(pages_text: List[str], chapter_ranges: Dict[str, Tuple[int, int]]) -> Dict[str, str]:
    wanted = [
        "ä¸“ä¸šå®šä½ä¸ç‰¹è‰²",
        "ä¸»å¹²å­¦ç§‘ã€ä¸“ä¸šæ ¸å¿ƒè¯¾ç¨‹å’Œä¸»è¦å®è·µæ€§æ•™å­¦ç¯èŠ‚",
        "æ ‡å‡†å­¦åˆ¶ä¸æˆäºˆå­¦ä½",
        "æ¯•ä¸šæ¡ä»¶",
    ]
    out: Dict[str, str] = {}
    for w in wanted:
        if w in chapter_ranges:
            s, e = chapter_ranges[w]
            out[w] = concat_pages(pages_text, s, e).strip()
        else:
            out[w] = ""
    return out


def extract_appendix_title_map(pages_text: List[str]) -> Dict[str, str]:
    """Try to extract mapping like é™„è¡¨1->ä¸ƒ ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨"""
    whole = "\n".join(pages_text)
    # match "ä¸ƒä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨ï¼ˆé™„è¡¨1ï¼‰" or "ä¸ƒã€ä¸“ä¸šæ•™å­¦è®¡åˆ’è¡¨ï¼ˆé™„è¡¨1ï¼‰"
    mp: Dict[str, str] = {}
    for cn, title in CHAPTER_KEYS:
        if cn not in ["ä¸ƒ", "å…«", "ä¹", "å", "åä¸€"]:
            continue
        # Try to locate the line containing appendix
        pat = rf"{cn}[ã€\s]*{re.escape(title)}\s*[ï¼ˆ(]\s*(é™„è¡¨\s*\d+)\s*[ï¼‰)]"
        for m in re.finditer(pat, whole):
            key = clean_text(m.group(1)).replace(" ", "")
            mp[key] = f"{cn}ã€{title}ï¼ˆ{key}ï¼‰"
    return mp


# -----------------------------
# Table extraction (pdfplumber)
# -----------------------------

PDFPLUMBER_TABLE_SETTINGS_LINES = {
    "vertical_strategy": "lines",
    "horizontal_strategy": "lines",
    "snap_tolerance": 3,
    "join_tolerance": 3,
    "edge_min_length": 3,
    "intersection_tolerance": 3,
    "text_tolerance": 3,
}

# Fallback for borderless tables (common in some teaching-plan PDFs)
PDFPLUMBER_TABLE_SETTINGS_TEXT = {
    "vertical_strategy": "text",
    "horizontal_strategy": "text",
    "snap_tolerance": 3,
    "join_tolerance": 3,
    "edge_min_length": 3,
    "intersection_tolerance": 3,
    "text_tolerance": 3,
    "min_words_vertical": 3,
    "min_words_horizontal": 1,
}


def normalize_table(raw_table: List[List[Any]]) -> Optional[List[List[str]]]:
    """Normalize: strip, drop empty rows, pad rows, drop empty columns."""
    if not raw_table:
        return None
    rows: List[List[str]] = []
    max_cols = 0
    for r in raw_table:
        if r is None:
            continue
        rr = [clean_text(c) for c in r]
        if all(c == "" for c in rr):
            continue
        rows.append(rr)
        max_cols = max(max_cols, len(rr))

    if not rows or max_cols == 0:
        return None

    # pad
    for i in range(len(rows)):
        if len(rows[i]) < max_cols:
            rows[i] += [""] * (max_cols - len(rows[i]))

    # drop empty columns
    keep_cols = []
    for j in range(max_cols):
        col = [rows[i][j] for i in range(len(rows))]
        if any(c != "" for c in col):
            keep_cols.append(j)

    if not keep_cols:
        return None
    out = [[row[j] for j in keep_cols] for row in rows]
    return out


def ffill_merged_cells(table: List[List[str]]) -> List[List[str]]:
    """Heuristic fill for merged cells: horizontal then vertical for sparse columns."""
    if not table:
        return table
    rows = [r[:] for r in table]
    n_rows = len(rows)
    n_cols = max(len(r) for r in rows)

    # make rectangular
    for i in range(n_rows):
        if len(rows[i]) < n_cols:
            rows[i] += [""] * (n_cols - len(rows[i]))

    # horizontal fill
    for i in range(n_rows):
        last = ""
        for j in range(n_cols):
            if rows[i][j] != "":
                last = rows[i][j]
            else:
                # only fill if last looks like category-like text (avoid filling numeric columns)
                if last and not re.match(r"^[-+]?\d+(\.\d+)?$", last):
                    rows[i][j] = rows[i][j] or ""  # keep empty by default
        # do not actually fill horizontally aggressively (often wrong). Keep conservative.

    # Decide columns that are likely merged vertically: high empty ratio
    empties = []
    for j in range(n_cols):
        col = [rows[i][j] for i in range(n_rows)]
        empty_ratio = sum(1 for x in col if x == "") / max(1, n_rows)
        empties.append(empty_ratio)

    # vertical fill on columns with empty_ratio high
    for j in range(n_cols):
        if empties[j] < 0.35:
            continue
        last = ""
        for i in range(n_rows):
            if rows[i][j] != "":
                last = rows[i][j]
            else:
                if last:
                    rows[i][j] = last

    return rows


def infer_direction_for_row(row: List[str]) -> str:
    text = " ".join([c for c in row if c])
    if "ç„Šæ¥" in text and "æ— æŸ" in text:
        return "æ··åˆ"
    if "ç„Šæ¥" in text:
        return "ç„Šæ¥"
    if "æ— æŸ" in text or "NDT" in text:
        return "æ— æŸæ£€æµ‹"
    return ""


def infer_direction_for_table(table: List[List[str]]) -> str:
    cnt = {"ç„Šæ¥": 0, "æ— æŸæ£€æµ‹": 0}
    for r in table[:50]:
        d = infer_direction_for_row(r)
        if d == "ç„Šæ¥":
            cnt["ç„Šæ¥"] += 1
        elif d == "æ— æŸæ£€æµ‹":
            cnt["æ— æŸæ£€æµ‹"] += 1
    if cnt["ç„Šæ¥"] and cnt["æ— æŸæ£€æµ‹"]:
        return "æ··åˆ"
    if cnt["ç„Šæ¥"]:
        return "ç„Šæ¥"
    if cnt["æ— æŸæ£€æµ‹"]:
        return "æ— æŸæ£€æµ‹"
    return ""


def classify_appendix(table: List[List[str]]) -> str:
    """Return appendix key like 'é™„è¡¨1'..'é™„è¡¨5' or ''"""
    head = " ".join(table[0]) if table else ""
    head2 = " ".join(table[1]) if len(table) > 1 else ""
    blob = (head + " " + head2)

    if "è¯¾ç¨‹ç¼–ç " in blob and "è¯¾ç¨‹ä½“ç³»" in blob:
        return "é™„è¡¨1"
    if "å­¦åˆ†" in blob and ("ç»Ÿè®¡" in blob or "åˆè®¡" in blob):
        return "é™„è¡¨2"
    if "æ•™å­¦è¿›ç¨‹" in blob or "å‘¨" in blob or "å­¦æœŸ" in blob and "å‘¨" in blob:
        return "é™„è¡¨3"
    if "æ¯•ä¸šè¦æ±‚" in blob or "1.1" in blob or "12.3" in blob:
        return "é™„è¡¨4"

    # é€»è¾‘æ€ç»´å¯¼å›¾é€šå¸¸ä¸æ˜¯è¡¨æ ¼
    return ""


@dataclass
class ExtractedTable:
    appendix: str
    appendix_title: str
    page: int
    title: str
    columns: List[str]
    rows: List[List[str]]
    direction: str


def extract_tables_pdfplumber(pdf_bytes: bytes, pages_text: List[str]) -> List[ExtractedTable]:
    if pdfplumber is None:
        return []

    appendix_map = extract_appendix_title_map(pages_text)

    out: List[ExtractedTable] = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        for i, page in enumerate(pdf.pages):
            # Try two strategies: lines-first (bordered) then text (borderless)
            raw_tables: List[List[List[Any]]] = []
            for settings in (PDFPLUMBER_TABLE_SETTINGS_LINES, PDFPLUMBER_TABLE_SETTINGS_TEXT):
                try:
                    raw_tables += page.extract_tables(settings) or []
                except Exception:
                    continue

            # De-duplicate by a light signature (first 3 rows joined)
            seen_sig: set = set()
            for t in raw_tables:
                nt = normalize_table(t)
                if not nt or len(nt) < 2:
                    continue
                sig = "||".join(["|".join(r) for r in nt[: min(3, len(nt))]])
                if sig in seen_sig:
                    continue
                seen_sig.add(sig)
                nt = ffill_merged_cells(nt)

                appendix = classify_appendix(nt)
                appendix_title = appendix_map.get(appendix, appendix) if appendix else ""

                # Determine header
                columns = nt[0]
                rows = nt[1:]

                # Add direction column if useful (appendix1 often needs)
                direction_tbl = infer_direction_for_table(nt)
                # per-row direction for appendix1/4 might be useful
                add_row_direction = appendix in ("é™„è¡¨1", "é™„è¡¨4")
                if add_row_direction:
                    columns = columns + ["ä¸“ä¸šæ–¹å‘(æ¨æ–­)"]
                    new_rows = []
                    for r in rows:
                        new_rows.append(r + [infer_direction_for_row(r)])
                    rows = new_rows

                title = appendix_title or f"è¡¨æ ¼-P{i+1}"  # fallback
                if appendix_title:
                    title = f"{appendix_title} - ç¬¬{i+1}é¡µ"

                out.append(
                    ExtractedTable(
                        appendix=appendix,
                        appendix_title=appendix_title,
                        page=i + 1,
                        title=title,
                        columns=columns,
                        rows=rows,
                        direction=direction_tbl,
                    )
                )

    return out


def table_to_df(t: ExtractedTable) -> pd.DataFrame:
    def _cell_to_str(x: Any) -> str:
        if x is None:
            return ""
        # pdfplumber sometimes yields non-str objects; stringify everything
        try:
            s = str(x)
        except Exception:
            s = ""
        return s.replace("\r", "").strip()

    def _make_unique_columns(cols_in: List[str]) -> List[str]:
        # Streamlit uses pyarrow under the hood; duplicate column names will crash.
        seen: Dict[str, int] = {}
        out: List[str] = []
        for c in cols_in:
            base = c
            if base in seen:
                seen[base] += 1
                out.append(f"{base}_{seen[base]}")
            else:
                seen[base] = 1
                out.append(base)
        return out

    cols_raw = ["" if c is None else str(c).strip() for c in (t.columns or [])]
    rows_raw = t.rows or []

    # robust align lengths
    max_len = max([len(cols_raw)] + [len(r) for r in rows_raw] + [0])
    cols_norm: List[str] = []
    for i in range(max_len):
        name = cols_raw[i] if i < len(cols_raw) else ""
        name = re.sub(r"\s+", " ", (name or "").strip())
        if not name:
            name = f"col_{i+1}"
        cols_norm.append(name)
    cols_norm = _make_unique_columns(cols_norm)

    fixed_rows: List[List[str]] = []
    for r in rows_raw:
        rr = [ _cell_to_str(x) for x in (r or []) ]
        if len(rr) < max_len:
            rr = rr + [""] * (max_len - len(rr))
        elif len(rr) > max_len:
            rr = rr[:max_len]
        fixed_rows.append(rr)

    # force all-string dataframe to avoid pyarrow dtype issues
    df = pd.DataFrame(fixed_rows, columns=cols_norm)
    return df.astype("string")


def make_tables_zip(tables: List[ExtractedTable]) -> bytes:
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
        for idx, t in enumerate(tables, start=1):
            df = table_to_df(t)
            safe_name = re.sub(r"[\\/:*?\"<>|]", "_", t.title)[:60]
            filename = f"{idx:02d}_{safe_name}_P{t.page}.csv"
            zf.writestr(filename, df.to_csv(index=False))
    return buf.getvalue()


def make_tables_excel(tables: List[ExtractedTable]) -> bytes:
    buf = io.BytesIO()
    # Prefer xlsxwriter, fallback openpyxl
    engine = "xlsxwriter"
    try:
        import xlsxwriter  # noqa: F401
    except Exception:
        engine = "openpyxl"

    with pd.ExcelWriter(buf, engine=engine) as writer:
        for i, t in enumerate(tables, start=1):
            df = table_to_df(t)
            # sheet name length limit 31
            name = re.sub(r"[\\/:*?\[\]]", "_", t.appendix or f"è¡¨{i}")
            name = name[:28]  # keep room
            sheet = f"{name}_{i}" if len(name) <= 20 else name
            sheet = sheet[:31]
            df.to_excel(writer, sheet_name=sheet, index=False)
    return buf.getvalue()


# -----------------------------
# Full extraction pipeline
# -----------------------------


def run_full_extract(pdf_bytes: bytes, llm_cfg: Optional[LLMConfig] = None) -> Dict[str, Any]:
    pages_text = extract_pages_text(pdf_bytes)
    chapter_ranges = locate_chapter_ranges(pages_text)

    # training objectives
    if "åŸ¹å…»ç›®æ ‡" in chapter_ranges:
        s, e = chapter_ranges["åŸ¹å…»ç›®æ ‡"]
        obj_text = concat_pages(pages_text, s, e)
    else:
        obj_text = "\n".join(pages_text)
    training_objectives = parse_training_objectives(obj_text)

    # graduation requirements
    grad_text = ""
    if "æ¯•ä¸šè¦æ±‚" in chapter_ranges:
        s, e = chapter_ranges["æ¯•ä¸šè¦æ±‚"]
        grad_text = concat_pages(pages_text, s, e)
    graduation_requirements = parse_graduation_requirements(grad_text)

    # chapters 3-6 content
    chapter_content = parse_chapter_content(pages_text, chapter_ranges)

    # appendix map
    appendix_map = extract_appendix_title_map(pages_text)

    # tables
    tables = extract_tables_pdfplumber(pdf_bytes, pages_text)

    result = {
        "meta": {
            "sha256": sha256_bytes(pdf_bytes),
            "pages": len(pages_text),
            "tables": len(tables),
        },
        "chapter_ranges": {k: [v[0] + 1, v[1]] for k, v in chapter_ranges.items()},
        "appendix_map": appendix_map,
        "training_objectives": training_objectives,
        "graduation_requirements": graduation_requirements,
        "chapter_content": chapter_content,
        "pages_text": pages_text,
        "tables_data": [
            {
                "appendix": t.appendix,
                "appendix_title": t.appendix_title,
                "page": t.page,
                "title": t.title,
                "direction": t.direction,
                "columns": t.columns,
                "rows": t.rows,
            }
            for t in tables
        ],
    }
    # Optional LLM refinement
    if llm_cfg is not None and getattr(llm_cfg, 'enabled', False):
        try:
            result = llm_refine_result(result, pages_text, llm_cfg)
        except Exception as e:
            result['_llm_error'] = str(e)

    return result


# -----------------------------
# Streamlit UI
# -----------------------------

st.set_page_config(page_title="åŸ¹å…»æ–¹æ¡ˆPDFå…¨é‡æŠ½å–", layout="wide")

st.title("åŸ¹å…»æ–¹æ¡ˆ PDF å…¨é‡æŠ½å–ï¼ˆæ–‡æœ¬ + è¡¨æ ¼ + ç»“æ„åŒ–è§£æï¼‰")

with st.sidebar:
    st.markdown("## ä¸Šä¼ ä¸æŠ½å–")
    up = st.file_uploader("ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDF", type=["pdf"], accept_multiple_files=False)
    use_ocr = st.checkbox("å¯¹æ— æ–‡æœ¬é¡µå¯ç”¨ OCRï¼ˆå¯é€‰ï¼‰", value=False, help="è‹¥éƒ¨ç½²ç¯å¢ƒæ—  OCR ä¾èµ–ï¼Œå°†è‡ªåŠ¨é™çº§")

    with st.expander("ğŸ” å¯ç”¨ LLM æ ¡å¯¹ä¸ä¿®æ­£ï¼ˆå¯é€‰ï¼‰", expanded=False):
        enable_llm = st.checkbox("å¯ç”¨ LLM æ ¡å¯¹ä¸ä¿®æ­£", value=False)
        provider = st.selectbox("LLM æä¾›æ–¹", ["OpenAI-compatible", "DashScope/Qwen"], index=0)
        model = st.text_input("Model åç§°", value=_get_secret("LLM_MODEL", ""))
        api_key = st.text_input("API Key", value=_get_secret("LLM_API_KEY", ""), type="password")
        base_url = ""
        if provider == "OpenAI-compatible":
            base_url = st.text_input("Base URLï¼ˆå¦‚ https://xxx/v1 æˆ–å®Œæ•´ chat/completionsï¼‰", value=_get_secret("LLM_BASE_URL", ""))
        st.caption("å¯åœ¨ Streamlit secrets / ç¯å¢ƒå˜é‡è®¾ç½®ï¼šLLM_API_KEY / LLM_MODEL / LLM_BASE_URL")

    llm_cfg = LLMConfig(
        enabled=bool(enable_llm),
        provider="dashscope" if provider == "DashScope/Qwen" else "openai_compat",
        model=model.strip(),
        api_key=api_key.strip(),
        base_url=base_url.strip(),
        temperature=0.0,
        max_tokens=2000,
        timeout=60,
    )

    run_btn = st.button("å¼€å§‹å…¨é‡æŠ½å–", type="primary", disabled=up is None)

if "result" not in st.session_state:
    st.session_state["result"] = None

if up is not None:
    pdf_bytes = up.getvalue()
    file_hash = sha256_bytes(pdf_bytes)[:12]
else:
    pdf_bytes = b""
    file_hash = ""

if run_btn and up is not None:
    with st.spinner("æ­£åœ¨æŠ½å–å…¨æ–‡ä¸è¡¨æ ¼ï¼Œè¯·ç¨ç­‰â€¦"):
        res = run_full_extract(pdf_bytes, llm_cfg=llm_cfg)
        st.session_state["result"] = res

res = st.session_state.get("result")

if not res:
    st.info("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ åŸ¹å…»æ–¹æ¡ˆ PDFï¼Œç„¶åç‚¹å‡»â€œå¼€å§‹å…¨é‡æŠ½å–â€ã€‚")
    st.stop()

# Summary row
c1, c2, c3, c4 = st.columns([1.2, 1.2, 1.2, 3])
with c1:
    st.metric("æ€»é¡µæ•°", res["meta"]["pages"])
with c2:
    st.metric("è¡¨æ ¼æ€»æ•°", res["meta"]["tables"])
with c3:
    st.metric("OCRå¯ç”¨", "æ˜¯" if use_ocr else "å¦")
with c4:
    st.caption(f"SHA256: {res['meta']['sha256']}")

# Tabs
TAB_NAMES = [
    "æ¦‚è§ˆä¸ä¸‹è½½",
    "ç« èŠ‚å¤§æ ‡é¢˜ï¼ˆå…¨éƒ¨ï¼‰",
    "åŸ¹å…»ç›®æ ‡",
    "æ¯•ä¸šè¦æ±‚ï¼ˆ12æ¡ï¼‰",
    "é™„è¡¨è¡¨æ ¼",
    "åˆ†é¡µåŸæ–‡ï¼ˆæº¯æºï¼‰",
]


tabs = st.tabs(TAB_NAMES)

# 1) æ¦‚è§ˆä¸ä¸‹è½½
with tabs[0]:
    st.subheader("ç»“æ„åŒ–è¯†åˆ«ç»“æœï¼ˆå¯å…ˆåœ¨è¿™é‡Œæ ¡å¯¹ï¼‰")

    # quick counts
    st.write(
        {
            "åŸ¹å…»ç›®æ ‡æ¡æ•°": len(res.get("training_objectives", [])),
            "æ¯•ä¸šè¦æ±‚å¤§é¡¹æ•°": len(res.get("graduation_requirements", {})),
            "é™„è¡¨æ ‡é¢˜æ˜ å°„": res.get("appendix_map", {}),
        }
    )

    # downloads
    json_bytes = json.dumps(res, ensure_ascii=False, indent=2).encode("utf-8")
    st.download_button(
        "ä¸‹è½½æŠ½å–ç»“æœ JSONï¼ˆå…¨é‡åŸºç¡€åº“ï¼‰",
        data=json_bytes,
        file_name=f"åŸ¹å…»æ–¹æ¡ˆæŠ½å–_{file_hash}.json",
        mime="application/json",
        use_container_width=True,
    )

    # tables downloads
    if res["tables_data"]:
        # rebuild ExtractedTable list for export
        tables_obj = [
            ExtractedTable(
                appendix=t.get("appendix", ""),
                appendix_title=t.get("appendix_title", ""),
                page=int(t.get("page", 0)),
                title=t.get("title", ""),
                columns=t.get("columns", []),
                rows=t.get("rows", []),
                direction=t.get("direction", ""),
            )
            for t in res["tables_data"]
        ]

        zip_bytes = make_tables_zip(tables_obj)
        st.download_button(
            "ä¸‹è½½é™„è¡¨è¡¨æ ¼ CSVï¼ˆzipï¼‰",
            data=zip_bytes,
            file_name=f"é™„è¡¨è¡¨æ ¼_{file_hash}.zip",
            mime="application/zip",
            use_container_width=True,
        )

        try:
            xlsx_bytes = make_tables_excel(tables_obj)
            st.download_button(
                "ä¸‹è½½é™„è¡¨è¡¨æ ¼ Excelï¼ˆxlsxï¼‰",
                data=xlsx_bytes,
                file_name=f"é™„è¡¨è¡¨æ ¼_{file_hash}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True,
            )
        except Exception as e:
            st.warning(f"Excel å¯¼å‡ºå¤±è´¥ï¼š{e}")
    else:
        st.warning("æœªæ£€æµ‹åˆ°è¡¨æ ¼ã€‚è‹¥ PDF ä¸ºæ‰«æä»¶æˆ–çº¿æ¡†ä¸æ˜æ˜¾ï¼Œè¡¨æ ¼è¯†åˆ«å¯èƒ½å¤±è´¥ã€‚")

# 2) ç« èŠ‚å¤§æ ‡é¢˜
with tabs[1]:
    st.subheader("ä¸‰~å…­ ç« èŠ‚å†…å®¹ï¼ˆåŸæ–‡æ‹¼æ¥ï¼Œå¯æº¯æºï¼‰")
    chap = res.get("chapter_content", {})
    for k, v in chap.items():
        st.markdown(f"### {k}")
        if v:
            st.text_area("", value=v, height=220, key=f"chap_{k}")
        else:
            st.info("æœªåœ¨ PDF ä¸­å®šä½åˆ°è¯¥ç« èŠ‚æ ‡é¢˜ï¼ˆå¯èƒ½æ ¼å¼ä¸ä¸€è‡´ï¼‰ã€‚")

# 3) åŸ¹å…»ç›®æ ‡
with tabs[2]:
    st.subheader("åŸ¹å…»ç›®æ ‡ï¼ˆå¯ç¼–è¾‘/æ ¡å¯¹ï¼‰")
    objs = res.get("training_objectives", [])
    if not objs:
        st.warning("æœªè§£æåˆ°åŸ¹å…»ç›®æ ‡æ¡ç›®ã€‚å¯åœ¨â€œåˆ†é¡µåŸæ–‡â€é‡Œç¡®è®¤ PDF æ–‡æœ¬æ˜¯å¦å¯æå–ã€‚")
    else:
        for i, item in enumerate(objs, start=1):
            st.markdown(f"**{i}.** {item}")

# 4) æ¯•ä¸šè¦æ±‚
with tabs[3]:
    st.subheader("æ¯•ä¸šè¦æ±‚ï¼ˆåº”ä¸º 12 å¤§æ¡ + å­é¡¹ï¼‰")
    gr = res.get("graduation_requirements", {})
    if not gr:
        st.warning("æœªè§£æåˆ°æ¯•ä¸šè¦æ±‚ã€‚")
    else:
        # order by numeric
        keys = sorted(gr.keys(), key=lambda x: safe_int(x, 999))
        for k in keys:
            item = gr[k]
            st.markdown(f"### {k}. {item.get('title','')}")
            if item.get("text"):
                st.write(item["text"])
            subs = item.get("subs", {})
            if subs:
                for sk in sorted(subs.keys(), key=lambda x: [safe_int(p) for p in x.split(".")]):
                    st.markdown(f"- **{sk}** {subs[sk]}")

# 5) é™„è¡¨è¡¨æ ¼
with tabs[4]:
    st.subheader("é™„è¡¨è¡¨æ ¼ï¼ˆè¡¨å + æ–¹å‘å°½é‡æ¸…æ™°ï¼‰")

    tables_data = res.get("tables_data", [])
    if not tables_data:
        st.info("æœªæ£€æµ‹åˆ°è¡¨æ ¼ã€‚")
    else:
        # group by appendix
        by_app: Dict[str, List[ExtractedTable]] = {}
        for t in tables_data:
            obj = ExtractedTable(
                appendix=t.get("appendix", ""),
                appendix_title=t.get("appendix_title", ""),
                page=int(t.get("page", 0)),
                title=t.get("title", ""),
                columns=t.get("columns", []),
                rows=t.get("rows", []),
                direction=t.get("direction", ""),
            )
            key = obj.appendix or "æœªåˆ†ç±»"
            by_app.setdefault(key, []).append(obj)

        # tabs per appendix
        app_keys = list(by_app.keys())
        # order: é™„è¡¨1..é™„è¡¨5, then others
        def app_sort(k: str) -> int:
            m = re.search(r"(\d+)", k)
            if m:
                return safe_int(m.group(1), 99)
            return 99

        app_keys = sorted(app_keys, key=app_sort)
        app_tabs = st.tabs(app_keys)

        for tab_key, app_tab in zip(app_keys, app_tabs):
            with app_tab:
                lst = sorted(by_app[tab_key], key=lambda x: (x.page, x.title))
                for i, t in enumerate(lst, start=1):
                    st.markdown(f"#### {t.title}")
                    if t.direction:
                        st.caption(f"æ–¹å‘ï¼ˆæ¨æ–­ï¼‰ï¼š{t.direction}")
                    df = table_to_df(t)
                    # Streamlit uses PyArrow for rendering; some edge cases (e.g., duplicate cols / odd dtypes)
                    # may still fail. We already normalize to strings & unique cols, but keep a safe fallback.
                    try:
                        st.dataframe(safe_dataframe_for_streamlit(df), use_container_width=True, hide_index=True)
                    except Exception:
                        st.warning("è¯¥è¡¨æ ¼æ¸²æŸ“é‡åˆ°å…¼å®¹æ€§é—®é¢˜ï¼Œå·²é€€å›ä¸ºHTMLè¡¨æ ¼æ˜¾ç¤ºã€‚")
                        # pandas.DataFrame.to_markdown() needs optional dependency "tabulate".
                        # Streamlit Cloud often doesn't include it, so use HTML instead.
                        html = safe_dataframe_for_streamlit(df).to_html(index=False, escape=False)
                        st.markdown(html, unsafe_allow_html=True)

# 6) åˆ†é¡µåŸæ–‡
with tabs[5]:
    st.subheader("åˆ†é¡µåŸæ–‡ï¼ˆç”¨äºæº¯æº/è°ƒè¯•æŠ½å–ç¼ºå¤±ï¼‰")
    pages = res.get("pages_text", [])
    for i, txt in enumerate(pages, start=1):
        with st.expander(f"ç¬¬{i}é¡µæ–‡æœ¬", expanded=(i == 1)):
            st.text(txt)